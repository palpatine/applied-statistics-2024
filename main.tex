\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath} % Math
\usepackage{listings} % Required for inserting code
\usepackage{xcolor}   % Required for custom colors
\usepackage{color}
\usepackage{svg}    % Required for rendering svgs
\usepackage{float}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{outlines}

\title{Applied Statistics}
\author{Nicolas Lejeune, Peter Iatsenia}
\date{Spring 2024}

% Define colors similar to RStudio
\definecolor{codegreen}{rgb}{0,0.5,0}    % Comments in a green color
\definecolor{codegray}{rgb}{0.5,0.5,0.5} % Code-gray for numbers
\definecolor{codepurple}{rgb}{0,0.6,0} % Strings in green color
\definecolor{backcolour}{rgb}{1,1,1}     % White background color
\definecolor{codeblue}{rgb}{0,0,0}       % Blue for keywords

% R language lstset configuration
\lstset{
    language=R,                  % The language of the code
    basicstyle=\ttfamily\small,  % The style that is used for the code
    backgroundcolor=\color{backcolour}, % Set the background color for the snippet
    commentstyle=\color{codegreen},    % Comment style
    keywordstyle=\color{codeblue},     % Keyword style
    numberstyle=\tiny\color{codegray}, % The style that is used for the line numbers
    stringstyle=\color{codepurple},    % String literal style
    breakatwhitespace=false,           % Sets if automatic breaks should only happen at whitespace
    breaklines=true,                   % Sets automatic line breaking
    captionpos=b,                      % Sets the caption-position to bottom
    keepspaces=true,                   % Keeps spaces in text, useful for keeping indentation of code
    numbers=left,                      % Where to put the line numbers
    numbersep=5pt,                     % How far the line numbers are from the code
    showspaces=false,                  % Show spaces everywhere adding particular underscores
    showstringspaces=false,            % Underline spaces within strings only
    showtabs=false,                    % Show tabs within strings adding particular underscores
    tabsize=2                          % Sets default tabsize to 2 spaces
}

\begin{document}

\maketitle

\tableofcontents

\pagebreak

\section*{Preface}
This is a compressed note compendium based on the textbook: "Introduction to Statistical Data Analysis for the Life Sciences", Claus Thorn Ekstrøm \& Helle Sørensen, 2nd edition, 2015." For public use for students following the Spring 2024 Applied Statistics course.

All R Code is also available at the \href{https://github.com/palpatine/applied-statistics-2024}{github repository} in r/.

\section{Description of samples and populations}

\paragraph{}
Statistics involves using a sample, a subset of a larger population, to make inferences about the overall characteristics of that population. A population represents the complete set of subjects we want to study, while a sample is a smaller, representative group selected from this population. The key concepts are:

\begin{itemize}
    \item \textbf{Population}: The entire group of interest whose properties we want to analyze.
    \item \textbf{Sample}: A smaller, representative group drawn from the population.
    \item \textbf{Parameter}: A numerical value describing a characteristic of the entire population.
    \item \textbf{Statistic}: A numerical value describing a characteristic of the sample.
\end{itemize}

The process starts with selecting a sample from the population (sampling) and then using statistical methods to draw conclusions (statistical inference) about the population based on the sample data. For example, to estimate the average height of a population (a parameter), we measure the average height of a sample group. This sample average (a statistic) is then used to infer the population's average height. The course focuses on methods to make accurate inferences about population parameters using sample statistics. Whether a group is considered a population or a sample depends on the context and the type of inference being made.

\subsection{Data Types}
\paragraph{}
The data collected in a study dictates the statistical analysis approach, influencing the hypotheses that can be tested and the predictive models that can be used. Data can be broadly categorized into two types:

\begin{itemize}
    \item \textbf{Categorical Data}: This type includes non-numeric categories or groups.
    \item \textbf{Quantitative Data}: This type consists of numerical measurements or quantities.
\end{itemize}
The nature of the data (categorical or quantitative) is key in determining the appropriate statistical methods and models for analysis and prediction.

\subsubsection{Categorical data}

Categorical data sorts observations into groups based on qualitative traits, resulting in labels or categories. There are two subtypes:

\begin{itemize}
    \item \textbf{Nominal Data}: These categories have no natural order. Examples include hair color, gender, race, and smoking status. For instance, hair colors (brown, blonde, gray) are merely different without any inherent ranking.
    \item \textbf{Ordinal Data}: These categories have a natural order. Examples include pain levels (none, little, heavy) or income brackets (low, middle, high). While we can rank these categories (e.g., low income is less than high income), the actual difference between them isn't quantifiable. For example, the gap between low and middle income isn't necessarily the same as that between middle and high income.
\end{itemize}

The key distinction is that ordinal data has a rank order, but the magnitude of difference between categories isn't measurable or consistent.

\subsubsection{Quantitative data}

Quantitative data are numerical and fall into two categories:

\begin{itemize}
    \item \textbf{Discrete Quantitative Data}: These are countable numbers, representing finite possible values. They accurately reflect counts, like household size or number of kittens in a litter. Differences between values have a clear quantitative interpretation (e.g., the difference between 9 and 7 households is the same as between 5 and 3 households).
    \item \textbf{Continuous Quantitative Data}: Representing measurements like length, volume, time, mass, etc., these are ideally continuous and gapless. While theoretically continuous, practical limitations often lead to less detailed measurements (e.g., measuring time in days instead of seconds). Despite not being measured with infinite precision, treating these variables as continuous is generally appropriate.
\end{itemize}

Categorical data are summarized by frequencies or proportions in each category, whereas quantitative data are typically summarized using averages or means.

In the Danscher et al. (2009) study on acute laminitis in cattle, different data types were used:

\begin{itemize}
    \item \textbf{Location (Nominal Data)}: Non-ordered categories (I or II).
    \item \textbf{Weight (Continuous Quantitative Data)}: Reported in whole kilograms, with meaningful differences.
    \item \textbf{Lameness Score (Ordinal Data)}: Ranked (normal to severely lame).
    \item \textbf{Number of Swelled Joints (Discrete Quantitative Data)}: Countable numeric values.
\end{itemize}
This study exemplifies the integration of various data types in research.

\subsection{Visualizing categorical data}
\paragraph{}
Categorical data can be represented in many ways, these are listed below:

\begin{itemize}
    \item \textbf{Frequency tables}: for listing category occurrences; ideal for fewer categories.
    \item \textbf{Bar charts/graphs}: for larger categories or cross-population comparison.
    \item \textbf{Segmented bar charts}: Display relative frequencies as parts of a whole. Useful for comparing category distributions across populations.
\end{itemize}

\paragraph{}
There are also some important definitions regarding categorical data:

\begin{itemize}
    \item \textbf{Frequency}: Count of each category's occurrence.
    \item \textbf {Relative frequency}: Frequency divided by total observations; enables comparison across different-sized datasets. \text{Relative Frequency} = $\frac{\text{frequency}}{n}$
\end{itemize}






\subsection{Visualizing quantitative data}
\paragraph{}
Data visualization is crucial for understanding and interpreting categorical and quantitative data. For categorical variables or discrete quantitative data with limited values, frequency or relative frequency plots are effective. However, for continuous quantitative data, these plots become less informative due to the vast number of unique values. Instead, we use histograms, grouping data into bins, and counting observations per bin. This approach effectively represents the data's distribution, showing the center, spread, and modes.

\subsubsection*{Histograms and Relative Frequency Histograms}
\paragraph{}
Histograms, akin to bar charts for categorical data, display the count of observations in each bin. Relative frequency histograms, on the other hand, show the proportion of observations per bin, making it easier to compare different populations. The shape of a relative frequency histogram mirrors that of a standard histogram, differing only in scale. It's important to note that the histogram's accuracy depends on equal bin widths; unequal widths can distort the representation of frequencies.

\subsubsection*{Scatter Plots for Quantitative Variables}
\paragraph{}
For illustrating relationships between two quantitative variables, scatter plots are employed. These plots reveal the strength, shape (linear, curved, etc.), and direction (positive or negative) of the relationship between variables. They also help identify outliers or extreme observations. When one variable is under experimental control, it's designated as the explanatory variable and typically plotted on the x-axis, with the response variable on the y-axis. If there's no clear explanatory variable, the choice of axes is flexible, with the scatter plot highlighting correlation rather than causation.

\subsubsection*{Case Study: Tenderness of Pork}
\paragraph{}
An experiment compared two cooling methods (tunnel and rapid cooling) for pork from two groups (low and high pH). The tenderness of the pork was measured post-cooling. Data analysis included histograms and relative frequency histograms for each pH group, allowing comparison of tenderness distributions between the low- and high-pH groups. Additionally, scatter plots depicted the relationship between the tenderness scores from both cooling methods, showcasing the practical application of these visualization techniques in interpreting interactions between two quantitative variables.


\subsection{Statistical summaries}

Categorical data are effectively summarized using tables. For quantitative data, which don't fit fixed categories, binning the data like in histograms is an option, but this can lose detail and depends heavily on bin choices. Instead, summary statistics are preferred. The measure of central tendency, like an average, represents the data's "middle" value, indicating a typical observation. However, as different datasets can have the same central tendency, it's also crucial to assess the data's variability or dispersion. This shows how much data points deviate from the central value, giving a fuller understanding of the data's spread and overall distribution.

\subsubsection{Median and inter-quartile range}

In a sample with $n$ independent, quantitative observations $(y_1,...,y_n)$, these can be ordered from smallest to largest, denoted as $y_1,...,y_n$ where $y_1$ is the smallest, $y_2$ is the second smallest, and so on.

The median, a central tendency measure, is the middle value in this ordered set. For an odd number of observations $(n)$, it's $y_{(\frac{n+1}{2})}$; for an even $n$, it's $\frac{1}{2}[y_{(n/2)}+y_{(n/2+1)}]$. The median applies to both quantitative and ordinal categorical data.

The range, a basic dispersion measure, is the difference between the highest and lowest values $(y_n-y_1)$. However, the range only considers two values and can be misleading. For example, three datasets with the same range can have very different dispersions.

\begin{itemize}
    \item Dataset 1: 14, 14, 14, 14, 14, 14, 34
    \item Dataset 2: 14, 16, 19, 22, 26, 30, 34
    \item Dataset 3: 14, 14, 14, 34, 34, 34, 34
\end{itemize}
All have a range of 20, but their distributions vary significantly.

The interquartile range (IQR) is a more nuanced dispersion measure. It's calculated by removing the top and bottom 25\% of observations and then finding the range of the remaining 50\%. Denoted as $IQR=Q3-Q1$, where $Q1$ and $Q3$ are the first and third quartiles, respectively, IQR is less affected by extreme values.

Quartiles are specific examples of quantiles, which divide ordered data into equal-sized subsets. The $x$th quantile is the value below which $x\%$ of the data falls.  The 25th and 75th quantiles are the first and third quartiles, respectively, and the 50th quantile is the median, dividing the data into four equal parts. Though exact quantile calculation can vary slightly in finite datasets, the interpretation remains consistent.

\subsubsection{Boxplot}
\textbf{Boxplot Overview}
\begin{itemize}
    \item Summarizes data with five key statistics: minimum, $Q1$, median, $Q3$, and maximum.
    \item Box represents the IQR, median is shown as a central line, and whiskers extend to the minimum and maximum values.
    \item Useful for assessing distribution, including symmetry and skewness.
\end{itemize}

\textbf{Outlier Detection Using IQR}
\begin{itemize}
    \item Outliers are defined as observations beyond $Q1 - 1.5 \cdot IQR$ or $Q3 + 1.5 \cdot IQR$.
    \item Crucial for recognizing anomalous data points that may affect the analysis.
\end{itemize}

\textbf{Modified Boxplot Representation}
\begin{itemize}
    \item Shows outliers as individual points, with minimum and maximum defined within the interval $[Q1 - 1.5 \cdot IQR, Q3 + 1.5 \cdot IQR]$.
    \item Allows for clearer visualization of extreme values, aiding in comparative distribution analysis.
\end{itemize}

\subsubsection{The mean and standard deviation}
The mean and standard deviation are key measures for quantitative data:

\begin{itemize}
    \item \textbf{Mean}: The mean (denoted as $\bar{y}$) is the average of a sample's observations. It's calculated by $\bar{y}=\frac{\sum_{i=1}^{n} {y_i}}{n}$ 
    \item \textbf{Standard Deviation}:  This measures the dispersion or how much the observations typically deviate from the mean.
    It's defined as: $s=\sqrt{\frac{\sum_{i=1}^{n} ({y_i-\bar{y}})^2}{n-1}}$
    \item \textbf{Variance}: Represented as $s^2$, it's the square of the standard deviation: $s^2=\frac{\sum_{i=1}^{n} ({y_i-\bar{y}})^2}{n-1}$
\end{itemize}

Both the mean and standard deviation incorporate information from all observations, providing a more comprehensive view than the median and inter-quartile range. They are expressed in the same units as the original data, enabling direct interpretation in the context of the observed values.

\textbf{Why do we divide by n-1?}

Using the $n-1$ denominator compensates for the loss of a degree of freedom. When we know the mean $\bar x$, we loose a degree of freedom of information, because once we know $n-1$ deviations, we know the $n$'th deviation (as all the deviations should sum to 0), thus the $n$'th deviation is dependent. If we were using $n$ as a denominator, we'd be assuming that all the deviations are independent, which is not the case, causing a biased estimation.

\pagebreak

\textbf{Sample mean and standard deviation of linearly transformed data}

Let $\bar{y}$ and $s$ be the sample mean and sample standard deviation from observations $y_1,...,y_n$ and let $y_{i}'=c \cdot y_{i} + b$ be a linear transformation of the $y$'s with constraints $b$ and $c$. Then $\bar{y'} = c \cdot \bar{y}+b$ and $s'=|c|\cdot s$.

\subsubsection{Mean or median?}
The median divides the data into two parts with an equal number of observations or equal areas under the histogram, disregarding the actual distances from the center. The mean also partitions the data into two halves, but it considers the values' distance from the center, making it sensitive to extreme values. 

\textbf{Sensitivity to Extreme Values}
\begin{itemize}
    \item The median is robust against extreme values, relying only on the two middle observations.
    \item The mean is influenced by all observations, making it susceptible to extreme values.
\end{itemize} 

\textbf{Mathematical Properties and Usage}
\begin{itemize}
    \item The mean has desirable mathematical properties, aiding in proving theorems and inferential statistics.
    \item The median is more robust but mathematically more challenging to work with.
    \item The mean is preferred for symmetric data except in the presence of extreme values, where the median is more suitable.
    \item The central limit theorem supports the use of sample means as symmetric estimates, regardless of the original distribution, given a large sample size.
\end{itemize}

\subsection{What is a probability?}

Probability is the likelihood of a random event occurring and is based on the concept of relative frequency in large numbers of experiments.

When conducting random experiments, such as rolling a die or measuring daily milk production from cows, the outcomes can vary each time. This variability is intrinsic to random events. For example, the occurrence of an even number on a die roll is a basic type of random event. If we denote this event as A and perform a large number of die rolls (denoted as $n$), he relative frequency of A $(n_A/n)$, which is the number of times A occurs divided by the total number of rolls, provides an empirical estimate of the probability of A.

As the number of trials $n$ increases, the relative frequency tends to stabilize. This stabilized value, approached as $n$ becomes very large, is considered the probability of the event.

For instance, in an experiment involving throwing a thumbtack 100 times and observing whether the pin points up or down, the relative frequency of the pin pointing down stabilizes around a certain value as the number of throws increases. This stabilization point, observed empirically, is interpreted as the probability of the thumbtack landing pin-down. In the thumbtack example, the probability was found to be approximately 0.6 or 60%.

This example illustrates how probability is derived from the relative frequency of an event in a large number of trials, reflecting the likelihood of that event occurring in a given random experiment.
\pagebreak

\subsection{R Code}

\subsubsection*{Boxplots and Modified Boxplots}

\begin{lstlisting}
# Example data: A sample of quantitative observations
observations <- c(22, 26, 24, 19, 23, 27, 28, 18, 30, 40, 15)  # Replace with actual data

# Standard Boxplot
boxplot(observations, 
        main = "Standard Boxplot", 
        ylab = "Values",
        xlab = "Data",
        range=0) # range=0 extends whiskers to minimum and maximum values

# Modified Boxplot
boxplot(observations, 
        main = "Modified Boxplot", 
        ylab = "Values",
        xlab = "Data")
\end{lstlisting}

\subsubsection*{Mean, Median, Standard Deviation, and Variance}

\begin{lstlisting}
# Example data: A sample of quantitative observations
observations <- c(5, 10, 15, 20, 25)  # Replace with actual data

# Calculate the mean
mean_value <- mean(observations)
# The 'mean' function calculates the average of the observations

# Calculate the median
median_value <- median(observations)
# The 'median' function calculates the median of the observations

# Calculate the standard deviation
std_dev <- sd(observations)
# The 'sd' function calculates the standard deviation, 
# which measures the average deviation from the mean

# Calculate the variance
variance_value <- var(observations)
# The 'var' function calculates the variance (standard deviation squared)
# Variance measures how spread out the numbers are from the mean

# Print the results
print(paste("Mean:", mean_value))
print(paste("Median:", median_value))
print(paste("Standard Deviation:", std_dev))
print(paste("Variance:", variance_value))
\end{lstlisting}

\pagebreak

\subsection{Proofs}
\subsubsection*{Proof that the Sample Variance is an Unbiased Estimator of the Population Variance}
Let $X_1, X_2, \ldots, X_n$ be a random sample from a population with variance $\sigma^2$. The sample variance $S^2$ is defined as:

\[
E(S^2) = \frac{1}{n-1} E(\sum_{i=1}^{n} (X_i - \bar{X})^2)
\]

Where $\bar{X}$ is the sample mean:

\[
\bar{X} = \frac{1}{n} \sum_{i=1}^{n} X_i
\]

To prove that the sample variance is an unbiased estimator of the population variance, we need to show that:

\[
E(S^2) = \sigma^2
\]

\begin{proof}
\hfill \break
\hfill \break
Let's start by expanding $E(\sum_{i=1}^{n} (X_i - \bar{X})^2)$:
\[
E(\sum_{i=1}^{n} (X_i - \bar{X})^2) = E((\sum_{i=1}^{n} (X_i^2)) - 2\bar{X}(\sum_{i=1}^{n} (X_i)) + (\sum_{i=1}^{n} (\bar{X^2}))
\]

This is equivalent to
\[
E((\sum_{i=1}^{n} (X_i)) - 2\bar{X}(\sum_{i=1}^{n} (X_i)) + n\bar{X^2}
\]

If we consider $\bar{X} = \frac{(\sum_{i=1}^{n} (X_i)}{n}$ then,
\[
\sum_{i=1}^{n} (X_i) = n\bar{X}
E((\sum_{i=1}^{n} (X_i)) - 2\bar{X}(\sum_{i=1}^{n} (X_i)) + n\bar{X^2} = E((\sum_{i=1}^{n} (X_i)) - 2\bar{X}\bar{X}n + n\bar{X^2}
\]

\[
= E((\sum_{i=1}^{n} (X_i)) - n\bar{X^2}))
\]

From here we can rewrite this equation as
\[
\sum_{i=1}^{n} (E(X_i) - nE(\bar{X^2}))
\]

We know that
\[
E(X_i^2) = \sigma^2 + \mu^2
\]

\[
E(\bar{X^2}) = \frac{\sigma^2}{n} + \mu^2
\]

This can be subbed into the equation above to find
\[
\sum_{i=1}^{n} (\sigma^2 + \mu^2 - n(\frac{\sigma^2}{n} + \mu^2)) = n\sigma^2 + n\mu^2 - \sigma^2 - n\mu^2
\]

\[
= (n-1)\sigma^2
\]

If we sub this back into the equation for $E(S^2)$, in place of $E(\sum_{i=1}^{n} (X_i - \bar{X})^2)$ then
\[
E(S^2) = \frac{1}{n-1}(n - 1)\sigma^2 = \sigma^2
\]
\end{proof}

\section{Linear Regression}
\paragraph{}
Data analysis often seeks to express one variable as a function of another, either based on theoretical hypotheses or empirical discovery. Simple linear regression models this relationship between two quantitative variables, \( x \) and \( y \), through a linear equation \( y = \alpha + \beta \cdot x \), where \( \alpha \) is the intercept and \( \beta \) the slope. Here, \( y \) is the dependent variable, influenced by the explanatory variable, \( x \).
\paragraph{}
An example is modeling the relationship between stearic acid levels and fat digestibility, where data suggests a linear trend, enabling predictions outside the observed range. However, real-life data often deviates from the model, indicating the linear model is an approximation, capturing the general trend rather than exact values.

\subsection{Fitting a regression line}
\paragraph{}
Fitting a regression line involves determining the optimal parameters (\( \hat{\alpha}, \hat{\beta} \)) that best represent observed data pairs (\(x_i, y_i\)). This is done by minimizing residuals (\(r_i = y_i - \hat{y_i}\)), the differences between observed values and those predicted by the model \(y = \hat{\alpha} + \hat{\beta} \cdot x\). The method of least squares addresses this by squaring and summing the residuals, ensuring that deviations are treated uniformly, irrespective of their direction. It involves solving for the parameters that minimize the sum of squared residuals, leading to unique parameter estimates.
\paragraph{}
For the stearic acid and digestibility example, the best-fit line was found to be \(y = -0.9337 \cdot x + 96.5334\), allowing for predictions and insights into the relationship between stearic acid levels and digestibility. The least squares method not only ensures the minimization of squared residuals but also that the regression line passes through the mean points (\(\bar{x}, \bar{y}\)), providing a robust model for understanding and predicting the dependent variable based on the independent variable.

\subsubsection{Least squares estimation}
The least squares method is employed to optimize model parameters by minimizing the sum of squared deviations between observed data and model predictions. In the context of a linear regression, the objective is to identify the optimal parameters, \( \alpha \) (intercept) and \( \beta \) (slope), such that the model \( y = \alpha + \beta \cdot x \) best fits the observed data points \( (x_i, y_i) \).

Given \( n \) observations, the least squares criterion seeks to minimize the sum of squared differences between the observed values \( y_i \) and the values predicted by the model \( \hat{y_i} = \alpha + \beta \cdot x_i \), formally expressed as:

\[
Q(\alpha, \beta; x, y) = \sum_{i=1}^{n} (y_i - \alpha - \beta \cdot x_i)^2
\]

To find the values of \( \alpha \) and \( \beta \) that minimize \( Q \), we take partial derivatives with respect to \( \alpha \) and \( \beta \), set them to zero, and solve the resulting equations:

\[
\frac{\partial Q}{\partial \alpha} = -2 \sum_{i=1}^{n} (y_i - \alpha - \beta \cdot x_i) = 0
\]

\[
\frac{\partial Q}{\partial \beta} = -2 \sum_{i=1}^{n} x_i (y_i - \alpha - \beta \cdot x_i) = 0
\]

Solving these equations provides the least squares estimates \( \hat{\alpha} \) and \( \hat{\beta} \) for the intercept and slope, respectively:

\[
\hat{\beta} = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}
\]

\[
\hat{\alpha} = \bar{y} - \hat{\beta} \cdot \bar{x}
\]

where \( \bar{x} \) and \( \bar{y} \) are the mean values of \( x \) and \( y \) respectively. The resulting line \( y = \hat{\alpha} + \hat{\beta} \cdot x \) represents the best fit to the data in the least squares sense, ensuring the minimized sum of squared residuals and passing through the point \( (\bar{x}, \bar{y}) \).

This method is not confined to linear regression but is a generalized approach for parameter estimation in various models, underpinning the robustness of predictions and understanding of relationships between variables.

\subsection{When is linear regression appropriate?}

This sub-section discusses fitting a linear relationship between two variables, $x$ and $y$, and highlights key considerations before applying linear regression:

\begin{itemize}
    \item \textbf{Quantitative Variables}: Linear regression is suitable only for quantitative variables. Both $x$ and $y$ must be quantitative to validly model their relationship through linear regression.
    \item \textbf{Linearity of Relationship}: It's crucial to assess if the relationship between $x$ and $y$ is linear. This is typically done by graphing the data and observing the relationship. If the relationship appears curvilinear, linear regression might not be appropriate. In such cases, data transformation or alternative models should be considered.
    \item \textbf{Influential Points}: Influential points in linear regression are those data points that have a significant impact on the regression line's slope. These points are often outliers in the $x$-direction. The influence of each data point on the slope is determined by its distance from the mean value of $x$ $(\bar{x})$. If the $x$-value of a point, $x_i$, is close to $\bar{x}$, it has minimal impact on the slope since the term $(x_i - \bar{x})$ in the slope formula's numerator and denominator will be small. If $x_i$ is far from $\bar{x}$, then both numerator and denominator will be large, and the difference $y_i - \bar{y}$ can have a large impact on the slope estimate.
    \item \textbf{$x$ on $y$ or $y$ on $x$}: In regression analysis, the choice of which variable to designate as the explanatory variable $x$ and which as the response variable $y$ significantly affects the model and its interpretation. When regressing $x$ on $y$, a different model is required than when regressing $y$ on $x$.

    In the regression equation $y=\alpha+\beta x$, solving for $x$ gives $x=-\alpha/\beta+y/\beta$, implying a line with intercept $-\alpha/\beta$ and slope $1/\beta$. However, this regression does not yield the same results as a direct regression of $x$ on $y$. The reason lies in the minimization of different types of residuals: the original regression minimizes vertical errors ($y$-direction) for predicting $y$ from $x$, whereas regressing $x$ on $y$ involves minimizing horizontal errors ($x$-direction) for predicting $x$ from $y$.
    
    Deciding which variable to treat as explanatory or response is simple in experiments where one variable is controlled and the other is observed. However, in cases where this distinction is not clear, it might be more appropriate to compute the correlation coefficient, as this approach avoids the inherent bias of choosing one variable over the other as the explanatory variable.
    
    \item \textbf{Interpolation}: Interpolation is the prediction of unobserved $y$ values within the observed data range (range of $x$ values). It is generally reliable except in certain cases, such as when there are limited $x$ values with multiple responses. In such cases, it's challenging to establish a linear relationship between $x$ and $y$.

    \item \textbf{Extrapolation}: Extrapolation is predicting outside the observed data range, becoming less certain with increased distance from this range. This uncertainty arises because the linear relationship cannot be confirmed beyond the observed values. While linear regression may fit well within specific intervals, it may not be appropriate for the entire range of possible values.
    
\end{itemize}

\subsubsection{Transformation}
\paragraph{}
The text discusses how to use linear regression, a statistical method, in situations where the relationship between two variables isn't straight-line (linear). Sometimes, even if the relationship isn't linear, we can make it linear by transforming one of the variables. This process is explained through the growth example of duckweed, a plant.
Below is the mathematical explanation of the transformation process. \hfill \break

\textbf{Original Situation} \hfill \break

We have data points $(x_i, y_i)$ where $x_i$ represents the days and $y_i$ the number of leaves. The relationship between days ($x$) and leaves ($y$) isn't linear, so a straight line doesn't fit the data well. \hfill \break

\textbf{Transformation} \hfill \break

We transform the response variable (number of leaves) using a logarithmic transformation. Recall that explanatory variable is what changes, and reponse variable gets changed in response to it. Let's call the transformed variable $z$. The transformation is: 
\begin{equation}
z_i = \log(y_i)
\end{equation}
\paragraph{}
After transformation, the relationship between days ($x$) and transformed leaves ($z$) is more linear. \hfill \break

\textbf{Linear Regression Model} \hfill \break

Now, we can model $z$ as a linear function of $x$:
\begin{equation}
z = \alpha + \beta \cdot x
\end{equation}
where $\alpha$ and $\beta$ are parameters to be estimated from the data. \hfill \break

\textbf{Exponential Growth Model} \hfill \break

The exponential growth model is given by:
\begin{equation}
f(t) = c \cdot \exp(b \cdot t)
\end{equation}
where:
\begin{itemize}
    \item $f(t)$ is the population size at time $t$,
    \item $c$ is the population size at time zero,
    \item $b$ is the average population increase per time unit.
\end{itemize} \hfill \break

\textbf{Applying Logarithm to the Model} \hfill \break

Taking the natural logarithm on both sides of the exponential growth model gives:
\begin{equation}
\log(f(t)) = \log(c) + b \cdot t
\end{equation}
\paragraph{}
This is a linear relationship with $\log(f(t))$ as the response and $t$ as the explanatory variable. We can compare this with the linear model $z = \alpha + \beta \cdot x$ and identify $\log(c)$ with $\alpha$ and $b$ with $\beta$. \hfill \break

\textbf{Estimation and Back-Transformation} \hfill \break

After fitting the linear model to the transformed data, we get estimates $\hat{\alpha}$ and $\hat{\beta}$. We back-transform these estimates to the original scale:
\begin{equation}
\hat{c} = \exp(\hat{\alpha}), \quad \hat{b} = \hat{\beta}
\end{equation}
\paragraph{}
These estimates can be plugged into the exponential growth model to predict the number of leaves on any given day:
\begin{equation}
\hat{f}(t) = \hat{c} \cdot \exp(\hat{b} \cdot t)
\end{equation}
\paragraph{}
The growth rate interpretation is that for every day, the number of leaves multiplies by a factor of $\exp(\hat{b})$. \hfill \break

By transforming the response variable and applying linear regression to the transformed data, we can effectively model and understand relationships that are inherently non-linear, as demonstrated in the duckweed growth example. \hfill \break
 \hfill \break
 \hfill \break

\subsection{Correlation Coefficient}

In linear regression, we model $y$ as a function of $x$, often assuming a causal relationship where $x$ influences $y$. This assumption is valid in controlled experiments, where $x$ is determined by the investigator, as in the example of stearic acid levels. However, in other cases, while there may be an association between $x$ and $y$, it's not necessarily correct to infer causality. Examples include the relationship between systolic and diastolic blood pressure, human height and weight, steak tenderness and meat dice size, or the growth of two plants in the same pot. These instances show an association but not necessarily a direct causal link. \hfill \break

The\textbf{ sample correlation coefficient} is: 

\begin{equation*}
\hat \rho = \frac{\sum_{i=1}^{n} {(x_i-\hat{x})(y_i-\hat{y})}}
{\sqrt{(\sum_{i=1}^{n} {(x_i-\bar{x})^2})(\sum_{i=1}^{n} {(y_i-\bar{y})^2})}}
\end{equation*}

It measures the strength of the linear relationship between $x$ and $y$ and always lies between -1 and 1. A value of 1 indicates a perfect positive linear relationship, -1 a perfect negative linear relationship, and 0 means there's no linear relationship. The correlation coefficient is dimensionless, reflecting the tightness of the linear relationship, not the slope. It's important to note that a strong correlation does not imply causation and a zero correlation doesn't rule out a strong non-linear relationship.

The correlation coefficient is analogous to the regression slope when regressing $y$ on $x$ after scaling both variables to have a standard deviation of 1. For this, $x_i$ and $y_i$ are transformed into $x_i'$ and $y_i'$, respectively, by dividing by their standard deviations, $s_x$ and $s_y$: $x_i'=x_i/s_x$ and $y_i'=y_i/s_y, i=1,...,n$. The regression slope of $y'$ on $x'$ is the correlation coefficient, $\hat\rho$.

\begin{itemize}
    \item \textbf{Numerator}: The numerator of the formula involves multiplying the residual of $x$ by the residual of $y$ for each pair. A positive contribution results when $x$ and $y$ deviate in the same direction from their means, and a negative one when they deviate in opposite directions.

    \item \textbf{Denominator}: The denominator of the formula is positive, except when all $x$ or $y$ values are identical, making it undefined. 

\end{itemize}

The sign of the correlation coefficient matches the sign of the regression slope, as their numerators are the same. Notably, $x$ and $y$ are symmetric in the correlation formula, making the correlation of $x$ and $y$ identical to that of $y$ and $x$.

\subsubsection{When is the correlation coefficient relevant?}

\begin{itemize}
    \item \textbf{Quantitative Variables}: Both variables $x$ and $y$ must be quantitative for the sample correlation coefficient to apply.

    \item \textbf{Linear association}: The association between $x$ and $y$ must be linear. It is wise to graph it, to see if there is perhaps another non-linear association.

\end{itemize}

\subsubsection{Coefficient of Determination}

The coefficient of determination, $R^2$, equal to $\rho ^2$, can take on values between 0 and 1. This range indicates the \% to which the variance in the dependent variable ($y$) is explained by the independent variable ($x$). For example an $R^2$ value of 0.42 indicates that 42\% of the variance in $y$ is explained by $x$ in the model. It does not indicate if the relationship is positive or negative though, and it is symmertric with respects to $x$ and $y$, as the correlation coefficient $\rho$ is.

\subsection{Perspective}

In linear regression, x is considered the explanatory variable and is assumed to be quantitative. It can be controlled by the investigator and does not necessarily need to be continuous. The key advantage of regression analysis is its ability to model the relationship between y and the observed x values, facilitating predictions of y for unobserved x values within the sample dataset.

On the other hand, the correlation coefficient is used to measure the strength or "tightness" of the linear relationship. It is appropriate only for assessing the linear association between two continuous variables, x and y. Unlike regression analysis, it does not provide a functional relationship for prediction purposes.

\subsubsection{Modeling the residuals}

In linear regression, the sample standard deviation measures the average distance of observations from their mean. 
Residuals, measure the distance between observed and predicted values. The standard deviation of the residuals can be calculated using a similar method. 

The sum of the residuals of a linear regression equal zero:

\begin{equation*}
\sum\limits_{i} {r_i} = \sum_{i=1}^n {y_i-(\hat \alpha + \hat \beta x_i)}=0\end{equation*}

The residual standard deviation is:

\begin{equation*}
s_{y|x}=\sqrt{\frac {\sum\limits_{i} {r_i^2}} {n-2}}
\end{equation*}

The subscript $y|x$ refers to a regression model where $y$ is the dependent variable and $x$ is the independent variable. This measures the typical distance between the observed values of $y$ and the values predicted by the model, given $x$.

Note how the denominator is $n-2$, analogous to $n-1$ in the sample standard deviation formula.

\pagebreak

\subsection{R Code}

\begin{lstlisting}
# Load the dataset
data(mtcars)

# View the first few rows of the dataset, just like in jupyter
head(mtcars)

# Plotting mpg against wt
# pch is the point shape, 19 is a filled round dot
plot(mtcars$wt, mtcars$mpg, main="MPG vs Weight", xlab="Weight (1000 lbs)", ylab="Miles per Gallon", pch=19)

# Fitting a linear model
# lm is a R function for fitting linear models
model <- lm(mpg ~ wt, data=mtcars)

# Plot the linear regression line for mpg against wt using the model
abline(model, col="blue")

# Transforming hp to a logarithmic scale
mtcars$log_hp = log(mtcars$hp)

# Plotting log(hp) against mpg with a regression line
plot(mtcars$log_hp, mtcars$mpg, main="MPG vs Log(Horsepower)", xlab="Log(Horsepower)", ylab="Miles per Gallon", pch=19)
abline(lm(mpg ~ log_hp, data=mtcars), col="green")

# Calculating residuals for the mpg against wt model
residuals <- resid(model)

# Plotting residuals
plot(mtcars$wt, residuals, main="Residuals of MPG vs Weight Model", xlab="Weight (1000 lbs)", ylab="Residuals", pch=19)
abline(h=0, col="blue")

# Calculating and printing the residual standard deviation
residual_sd <- sd(residuals)
cat("Residual Standard Deviation:", residual_sd, "\n")
\end{lstlisting}

\pagebreak

\section{Comparison of groups}

One-way analysis of variance (ANOVA) is used for comparing two or more groups. 

For instance, a meat scientist investigating how three varied storage conditions affect meat tenderness would randomly allocate 24 meat pieces into three groups, with eight pieces per group. Each group's meat is stored under identical conditions, and their tenderness is measured after a period. The inquiry is whether storage conditions significantly influence tenderness, distinguishing between actual effects and random variations, and determining the magnitude of any differences. This analysis is an example of ANOVA, as the scientist will compare 3 different categorical groups.

\subsection{Graphical and simple numerical comparison}

Analyzing data typically begins with graphical inspection, such as using parallel boxplots or strip charts, to highlight key differences, data variation, and consistency across groups. Statistical summaries of each group are also compared for detailed analysis.

An example involves comparing parasite counts in two salmon stocks to determine if infection rates differ. Statistical analysis, including mean and standard deviation calculations for each stock, showed higher parasite counts in the Ätran stock compared to the Conon stock:

\begin{itemize}
    \item \textbf{Ätran}: mean = 32.23, SD = 7.28

    \item \textbf{Conon}: mean = 21.54, SD = 5.81 

\end{itemize}

This suggests higher susceptibility to parasites in Ätran salmon. However, results could vary with new samples, highlighting the need to distinguish between genuine differences and random variation. The analysis focuses on the difference between sample means, considering the natural variability of sample means to determine if observed differences are significant or due to chance.

\subsection{Between-group variation and within-group variation}

When comparing only two groups, the difference in means ($\bar{y_1}-\bar{y_2}$) serves as a practical measure of group differences. However, it's crucial to consider the natural variation within samples to determine if the observed difference might be due to chance.

With three or more groups, the analysis involves multiple pairwise differences that need to be collectively assessed. This introduces the concept of between-group and within-group variations, essential for understanding the dynamics of data analysis:

\begin{itemize}
    \item \textbf{Between-group variation} refers to the differences between the groups. Graphically, it's represented by the discrepancy between the means of each group and the overall mean. The formula is: ${SS}_{grp}=\sum_{j=1}^{k} n_j(\bar{y_i}-\bar{y})^2$

    \item \textbf{Within-group variation} refers to the variability within each group - the spread of observations around their group mean. The formula is: ${SS}_{e}=\sum_{i=1}^{n} (y_i-\bar{y}_{g(i)})^2$

\end{itemize}

For the two formulas:

\begin{itemize}
    \item \textbf{$k$} = the number of groups
    \item \textbf{$n_j$} = the number of observations in group $j$
    \item \textbf{$g(i)$} = the group for observation i
\end{itemize}


A significant between-group variation suggests differences between group means. However, if within-group variation is also high, the differences could be attributed to random variation, indicating that repeating the experiment might yield varying results. Thus, analysis must account for both types of variation.

This differentiation between variation sources is foundational to the analysis of variance (ANOVA), a method named for its focus on dissecting and analyzing these variations to discern significant differences between groups.

\subsection{Populations, samples, and expected values}

In comparing different populations, such as men's and women's average blood pressure, we aim to analyze the population means ($\alpha_m$ for men and $\alpha_f$ for women), which represent the expected values of blood pressure in the absence of additional information. To compare these averages, samples from each population are collected and their blood pressures measured to infer about the population averages.

The concept applies similarly to comparing two different salmon stocks from the earlier exammple, Ätran and Conon, where samples from each population are used to study differences in parasite counts. In experiments like the antibiotics study involving heifers, even though the groups come from a single population and are differentiated by treatment, the objective remains the same: to compare expected values ($\alpha$'s) across groups, where $\alpha_{spiramycin}$ and $\alpha_{control}$ might represent the average amounts of organic material in treated vs. untreated heifers, respectively.

Regardless of whether groups are formed from distinct populations or as a result of experimental intervention, the goal is to compare these expected values across groups. Essential to this comparison is accounting for the variation within groups, quantified by the standard deviations of each group. This within-group variation is a critical factor in assessing the significance of differences between the expected values of different groups or treatments.

\subsection{Least squares estimation and residuals}
Consider a scenario with $n$ observations distributed across $k$ groups, where each group is labeled from 1 to $k$. The objective is to estimate the expected values $\mu_1, \ldots, \mu_k$ for each group. To achieve this, we employ the least squares criterion. For any observation $i$, let $g(i)$ represent its group, implying $\mu_{g(i)}$ is the expected value for observation $i$. The sample mean $\bar{y}_j$ and sample standard deviation $s_j$ for group $j$ are defined as:
\[
\bar{y}_j = \frac{1}{n_j} \sum_{i:g(i)=j} y_i, \quad s_j = \sqrt{\frac{1}{n_j - 1} \sum_{i:g(i)=j} (y_i - \bar{y}_j)^2},
\]
for $j = 1, \ldots, k$. The least squares estimates of $\mu_1, \ldots, \mu_k$ are the values that minimize the sum of squared deviations $Q(\mu_1, \ldots, \mu_k) = \sum_{i=1}^{n} (y_i - \mu_{g(i)})^2$. These estimates are given by $\hat{\mu}_j = \bar{y}_j$ for $j = 1, \ldots, k$.

The residual for observation $i$ is $r_i = y_i - \hat{\mu}_{g(i)}$, and the residual variance $s^2$ and standard deviation $s$ are computed as:
\[
s^2 = \frac{1}{n - k} \sum_{i=1}^{n} r_i^2, \quad s = \sqrt{s^2},
\]
where $n - k$ adjusts for the estimation of $k$ parameters. The residual variance can also be expressed as a weighted average of the group variance estimates $s_j^2$, which is known as the pooled sample variance. This approach accounts for the within-group variability and assumes equal population standard deviations across groups.

\subsection{Paired and unpaired samples}
We revisit the scenario of comparing two groups, but with a distinct data structure characterized by pairs of observations naturally linked to each other, known as paired samples. This arrangement often arises when two measurements are taken under varying conditions for each subject, such as in dietary studies where subjects switch diets between periods, or in experiments involving pairs of related units like twins. The aim is to assess differences between conditions, reducing the substantial variation among subjects by focusing on paired differences.

An example involves a study on equine lameness with measurements of symmetry scores for eight horses, both in healthy and induced-lameness states. The analysis relies on the differences in symmetry scores between these states for each horse, aiming to determine if lameness significantly alters the symmetry score. Positive differences for all horses suggest a change, but statistical analysis is needed to confirm if this is beyond random variation.

It's crucial to differentiate between paired and unpaired (independent) samples since they require distinct analytical methods. In paired samples, observations within a pair share underlying characteristics, making the assumption of independence between observations inappropriate. This shared information, such as a horse's general gait pattern, can influence the results. By analyzing the differences within pairs, we eliminate issues related to independence and potentially neutralize factors like physical proportions or general patterns that could affect both measurements similarly.

Paired designs can offer practical and economical advantages by requiring fewer subjects to achieve a certain precision in estimating differences. However, it's not always feasible to conduct paired studies, especially when measurements involve irreversible processes or when pairing is impractical, as in the case with different salmon stocks. Paired studies aim to minimize random variation unrelated to the investigation's focus, essentially using the experimental units as their own controls to highlight the effect of interest.

\subsection{Perspective}

This section discusses three types of data structures relevant to comparing groups or treatments:

\begin{itemize}
    \item \textbf{Two Independent Samples}: This structure involves samples from two distinct groups or treatments, presumed independent. It's highlighted separately for its importance in distinguishing from paired samples due to the necessity of different analysis methods and because it introduces foundational concepts for understanding more complex arrangements.

    \item \textbf{Independent Samples Across Multiple Groups}: This structure extends the first to encompass samples from $k$ different groups or treatments, maintaining independence among them. This setup is suited for one-way analysis of variance (ANOVA), which compares variation sources within and across groups to assess significant differences.

    \item \textbf{Paired Samples}: This involves paired measurements, where each pair corresponds to observations from two different groups or treatments. The paired nature of the data requires distinct analytical approaches due to the inherent linkage between observations within each pair.
\end{itemize}

The first case serves as a basis for the second, emphasizing the shift in complexity when analyzing more than two groups. One-way ANOVA, used in the independent samples scenario, focuses on comparing expected values or average response levels across groups, underlining the importance of understanding the precision of these estimates to discern real differences from those occurring by chance.

A critical assumption for ANOVA is the similarity of standard deviations across groups, with a rule-of-thumb being that the ratio between the largest and smallest group standard deviations should not exceed 2. However, this guideline's applicability depends on sample sizes, with larger and equally sized samples offering more robust results.

The analysis must also account for data symmetry (since standard deviation is meaningful primarily for symmetric data) and sample size, as small samples can lead to imprecise standard deviation estimates. In cases of heterogeneous variation, data transformation (like logarithmic transformation) might be necessary to achieve homogeneity of variances across groups.

Furthermore, scientific experiments often involve more than one explanatory factor, necessitating multi-way ANOVA for simultaneous consideration of multiple factors. Two-way ANOVA, for example, analyzes the effects of two factors and their interactions, providing a more comprehensive understanding of the factors influencing the response variable.

\subsection{R Code}

\begin{lstlisting}
# Load the mtcars dataset
data(mtcars)

# Graphical comparison of mpg across different cylinder groups
boxplot(mpg ~ cyl, data = mtcars,
        main = "MPG by Number of Cylinders",
        xlab = "Number of Cylinders", ylab = "Miles Per Gallon")

# One-Way ANOVA for mpg across cylinder groups
anova_result <- aov(mpg ~ as.factor(cyl), data = mtcars)
summary(anova_result)

# Checking assumptions

# Normality check for each cylinder group for 'mpg'
shapiro.test(mtcars$mpg[mtcars$cyl == 4])
shapiro.test(mtcars$mpg[mtcars$cyl == 6])
shapiro.test(mtcars$mpg[mtcars$cyl == 8])

# Homogeneity of variances (Levene's Test is not available in base R, using Bartlett's test as an alternative)
bartlett.test(mpg ~ as.factor(cyl), data = mtcars)

# Since mtcars does not have paired data, let's simulate a scenario for demonstration purposes
# Simulating before and after data for a paired sample scenario
set.seed(123) # For reproducibility
before <- rnorm(10, 20, 5)
after <- before + rnorm(10, -2, 2) # Assume some decrease

# Paired t-test
paired_test_result <- t.test(before, after, paired = TRUE)
print(paired_test_result)
\end{lstlisting}

\section{Normal Distribution}

Statistical models capture systematic behavior and random variation. The section introduces the normal (Gaussian) distribution for random variation, extending beyond the average behavior explored in linear regression and one-way ANOVA. The normal distribution, vital for continuous variables, is effective due to the central limit theorem (CLT), which suggests averages tend toward a normal distribution regardless of the original data's distribution. This distribution is not suitable for categorical data, addressed with the binomial distribution in later chapters. Named after Carl Friedrich Gauss, the Gaussian distribution is preferred for its accurate data representation, especially in biological data, and its simple mathematical properties, enhancing statistical analysis for continuous data.

\subsection{Properties}

Continuous quantitative data, exemplified by a study on crab weights at the Royal Veterinary and Agricultural University in Denmark, show the utility of the normal distribution. Recording weights of 162 crabs, the study found a mean of 12.76 grams and a standard deviation of 2.25 grams. A histogram of crab weights aligns well with a normal distribution curve defined by these parameters, illustrating the distribution's effectiveness in modeling weight variation. The corresponding density function for this normal distribution highlights its applicability in analyzing such continuous data.

$f(y)= \frac {1} {\sqrt{2 \pi \cdot 2.25^2}} \exp (-\frac {1} {2 \cdot 2.25^2} (y-12.76)^2)$

\subsubsection{Density, mean, and standard deviation}

Histograms for continuous data divide observations into subintervals, representing frequency with rectangle heights. When normalized, these rectangles' areas sum to 1, indicating relative frequencies or probabilities for large samples. Smooth histograms can approximate smooth curves, like the normal distribution density:

$f(y) = \frac {1} {\sqrt{2 \pi \sigma ^2}} \exp (- \frac {1} {2 \sigma ^2} (y-\mu)^2 ), -\infty < y < \infty$

where $\mu$ and $\sigma$ represent the mean and standard deviation. This density suggests that the probability of an observation falling within any interval (a, b) is the area under the curve from a to b, calculated as $P(a<Y<B)= \int_a^b f(y) dy$, where $Y$ is a random observation.

For a sample drawn according to this density, the sample mean and standard deviation approach $\mu$ and $\sigma$ s sample size increases, defining the mean and standard deviation of the distribution. A variable $Y$ is normally distributed, $Y \sim N(\mu, \sigma ^2)$, if its values follow this density.

Continuing the crab weight example, using $\mu = 12.76$ and $\sigma=2.25$, the normal density closely matches the histogram, suggesting a $N(12.76, 2.25^2)$ distribution for crab weights. The probability of a crab weighing between 16 and 18 grams is computed through the integral of the density function over that range, showing a close match between calculated probabilities and observed relative frequencies when the sample well represents the normal distribution.

Normal densities are bell-shaped and symmetric around $\mu$, with properties of symmetry, centering (maximum value at $\mu$), and dispersion (width varies with $\sigma$). The interpretations of $\mu$ and $\sigma$ align with their roles in determining the distribution's center and spread. Probabilities for any interval are non-zero due to the density's positive value across its range, but single points have zero probability. The total area under the density curve equals 1, reflecting the total probability mass.

\subsubsection{Transformations of normally distributed variables}

The normal distribution's key properties include the normality of sums of normally distributed variables and the retention of normality through linear transformations. Specifically:

\begin{itemize}
    \item \textbf{Sum of Normally Distributed Variables}: If $Y_1 \sim N(\mu_1 \sigma_1^2)$ and $Y_2 \sim N(\mu_2 \sigma_2^2)$ are independent, their sum $Y_1 + Y_2$ follows a normal distribution $N(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)$, with mean $\mu_1 + \mu_2$ and standard deviation $\sqrt{\sigma_1^2 + \sigma_2^2}$
    \item \textbf{Linear Transformations}: A normally distributed variable $Y \sim N(\mu \sigma^2)$ transformed by $V=a+bY$ also follows a normal distribution, $N(a+b\mu, b^2\sigma^2)$, where $a$ and $b$ are real numbers. The mean and standard deviation of $V$ are $a+b\mu$ and $|b|\sigma$, respectively
    \item \textbf{Standardization}: Any normally distributed variable $Y$ can be standardized to $Z= \frac {Y-\mu} {\sigma} \sim N(0,1)$, a special case of linear transformation, with $a = -\mu / \sigma$ and $b = 1/ \sigma$
\end{itemize}

Independence is crucial for these properties, especially for the variance formula in sums. Even without independence, $Y_1 + Y_2$ remains normally distributed with mean $\mu_1 + \mu_2$, but variance calculations require independence. These properties extend to any number of terms, affecting sample means and standard deviations.

For crab weights modeled by $N(12.76, 2.25^2)$ applications include:

\begin{itemize}
    \item The total weight of two crabs, summing to a mean of 25.52 and a standard deviation of 3.18.
    \item Converting weights to kilograms with a specific adjustment results in a normal distribution with adjusted mean and standard deviation.
    \item Standardizing crab weights yields a $N(0,1)$ distribution
\end{itemize}

Furthermore, if $y_1,...,y_n$ are independent and identically distributed as $N(\mu, \sigma^2)$, the sum of these variables is normally distributed with mean $n \mu$ and variance $n \sigma ^2$. The sample mean, $\bar{y} = \frac {1} {n} \sum_{i=1}^{n} y_i$, follows $N(\mu, \sigma ^2 / n)$, implying an average of normally distributed variables remains normally distributed, with the original mean but standard deviation reduced by $\sqrt{n}$. This principle is vital for understanding distributions of sample means and their implications in statistical analysis.

\subsubsection{Probability calculations}
The standard normal distribution, denoted as \(N(0, 1)\), is characterized by its density function \(\phi(y) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2}y^2\right)\). This formulation is pivotal for calculating probabilities for any normal variable \(N(\mu, \sigma^2)\) through the transformation \(P(a < Y < b) = P\left(\frac{a - \mu}{\sigma} < Z < \frac{b - \mu}{\sigma}\right)\), where \(Z \sim N(0, 1)\).

The probability that \(Z\) is less than or equal to a specific value \(z\) is obtained by the integral \(\Phi(z) = \int_{-\infty}^{z} \phi(y) dy\), representing the cumulative distribution function (cdf) of \(Z\), or equivalently, of \(N(0, 1)\). The function \(\phi(y)\) is the derivative of the cdf, \(\Phi(z)\).

This cdf, \(\Phi(z)\), facilitates the calculation of probabilities for intervals within the \(N(0, 1)\) distribution and, by extension, for any \(N(\mu, \sigma^2)\) distribution, via the relation \(P(a < Y < b) = \Phi\left(\frac{b - \mu}{\sigma}\right) - \Phi\left(\frac{a - \mu}{\sigma}\right)\). Values of \(\Phi(z)\) indicate the cumulative probabilities and quantiles critical to the standard normal distribution.

\subsubsection{Central part of distribution}
Determining the central portion of a distribution, for instance, the central 90\%, involves finding an interval where 90\% of observations are expected. For the standard normal distribution \(N(0, 1)\), this interval is \([-1.645, 1.645]\) encompassing the central 90\% of the distribution, as \(P(-1.645 < Z < 1.645) = 0.90\).

This principle extends to any normal distribution \(N(\mu, \sigma^2)\), indicating that the interval \((\mu - 1.645\sigma, \mu + 1.645\sigma)\) captures the central 90\% of observations, denoted as \(\mu \pm 1.645\sigma\). The choice of 1.645, the 95\% quantile, is to accommodate the central 90\% based on the distribution's symmetry.

Further, for different central portions, intervals such as \(\mu \pm 1.96\sigma\) for 95\%, and \(\mu \pm 2.576\sigma\) for 99\% are calculated similarly. These intervals help in setting expectations regarding the location of a specific percentage of observations within a normal distribution.

For example, with crab weights assumed to follow a normal distribution with mean 12.76 grams and standard deviation 2.25 grams, 95\% of crab weights are expected within \(12.76 \pm 1.96 \times 2.25\) grams, between 8.35 and 17.17 grams. Using the cumulative distribution function, the probability for specific weight intervals can be calculated, such as a 6.5\% chance of a crab weight between 16 and 18 grams.

\subsection{One sample}

The expected value of observations $y_1, ..., y_n$ reflects the average value in a population. These observations help infer population characteristics. For example, studying crab weights aims to determine the population's average weight and what deviations are considered unusual. The sample mean, $\bar y$ estimates the population mean. However, its reliability varies with sample size—the larger the sample, the more accurate the estimate. Assuming the population follows a normal distribution and the sample is randomly drawn, it's possible to precisely estimate population characteristics like average weights and typical variations.

\subsubsection{Independence}

Observations in a sample are assumed to be independent, meaning the probability of multiple events happening simultaneously is the product of their individual probabilities:

$P(Y_1 \leq a_1, ..., Y_n \leq a_n) = P(Y_1 \leq a_1) \cdot \cdot \cdot P(Y_n \leq a_n)$

Independence implies each observation adds unique information to the dataset. For example, in studying apple tree yields, if 20 trees are randomly selected from 1000, their yields are considered independent. However, if for each of 10 randomly selected trees, a "twin" tree is also selected for its similarity, the yields are not independent since each pair of trees holds related information. Similarly, selecting 10 trees and their "opposites" (based on size or other criteria) also results in dependent observations. Furthermore, sampling two trees from each of 10 orchards introduces dependency because trees within the same orchard are influenced by common environmental factors, making it unreasonable to assume independence among these samples.

\subsubsection{Estimation}

When $y_1, ..., y_n$ are independent observations from a normal distribution with mean $\mu$ and standard deviation $\sigma$, they are considered iid $N(\mu, \sigma^2)$. The sample mean $\bar y$ and sample standard deviation $s$ serve as natural estimates for $\mu$ and $\sigma$ respectively, with $\hat \mu = \bar y$ and $\hat \sigma = s$.

The sample mean $\bar y$ itself follows a normal distribution with its expected value $E(\hat \mu) = \mu$ and standard deviation $sd(\hat \mu) = \frac {1} {\sqrt{n}} \sigma$, illustrating statistical properties:

\begin{itemize}
    \item \textbf{Unbiased Estimate}: The sample mean accurately reflects the population mean on average, indicating that across many samples, the average of the sample means equals $\mu$
    \item \textbf{Consistent Estimate}: Precision of the sample mean improves with sample size, as its standard deviation decreases, allowing for an increasingly accurate estimate of $\mu$
\end{itemize}

These properties underscore that the sample mean is both a correct and precise estimator of the population mean, with precision that can be enhanced by increasing the sample size.

\subsection{Are the data (approximately) normally distributed?}

Model validation often requires a variable's distribution to closely resemble a normal distribution. Perfect normality is rare, but statistical methods remain effective if the normality assumption is reasonably satisfied. This is usually checked using graphical methods.

\subsubsection{Histograms and QQ-plots}

For validating a variable's approximate normal distribution, comparing its histogram to a normal density curve with matching sample mean and standard deviation is useful. This method was applied to crab weight data, showing a good fit, indicating an approximate normal distribution.

Additionally, the QQ-plot (quantile-quantile plot) is crucial for this validation, comparing sample quantiles against the normal distribution's quantiles. A QQ-plot for crab weights aligned closely with a straight line defined by the sample's mean ($\bar y = 12.76$) and standard deviation ($s =2.25$), indicating no significant deviations from normality.

The QQ-plot operates on the principle that if a sample ($z_1,...,z_n$) possibly comes from a $N(0,1)$ distribution, plotting ordered observations ($z_{j}$) against corresponding $N(0,1)$ quantiles ($u_j$) should reveal a linear pattern with intercept zero and slope one for a perfectly normal distribution. For a general sample ($y_1,...,y_n$) from $N(\mu, \sigma^2)$, this plot helps estimate $\mu$ and $\sigma$ by the linearity of plotted points around a straight line, with the line's intercept and slope serving as estimates for $\mu$ and $\sigma$.

To make a QQ plot for checking if your data follows a normal distribution:

\begin{itemize}
    \item \textbf{Sort the data}: Arrange the data points in ascending order
    \item \textbf{Calculate theoretical quantiles}: Determine the expected positions of the data points if they were perfectly normally distributed. This involves figuring out where each data point would lie on a normal distribution curve
    \item \textbf{Plot the data}: On a graph, plot each of the sorted data points against the corresponding theoretical quantile. The x-axis will represent the theoretical (expected) quantiles from a normal distribution, and the y-axis will show the actual data's quantiles
\end{itemize}

If the data is normally distributed, the points on the QQ plot will roughly follow a straight line.

\subsubsection{Transformations}

Histograms or QQ-plots sometimes reveal that data do not fit a normal distribution, often due to subgroup differences (e.g., men vs. women) resulting in bimodal distributions or skewness in data with only positive values. For bimodal distributions, modeling each group with its own normal distribution may be more appropriate. Skewed data may benefit from log-transformation, making the log-transformed data more normally distributed, as shown with vitamin A intake among Danish men. This transformation aligns histograms and QQ-plots closer to normality. However, any analysis or probability computations should then be conducted on the transformed scale. The study on Danish food intake, including vitamin A and basal metabolic rate (BMR), demonstrates these points. BMR data, when analyzed separately for men and women, fit normal distributions reasonably well but combined data show bimodality, indicating the importance of considering subgroup differences for normal approximation.

To log-transform data:

\begin{outline}
    \1 \textbf{Choose the Log Base}: Select the base for the logarithm, typically $e$ (natural logarithm) or 10.
    \1 \textbf{Decide on Log Transformation}: Determine whether you need to log-transform $y$, $x$, or both, based on the distribution of your data or the specific analysis requirements.
        \2 If transforming $y$: Apply the log function to each $y_i$ to get $log(y_1), ..., log(y_n)$
        \2 If transforming $x$: Apply the log function to each $x_i$ to get $log(x_1), ..., log(x_n)$
        \2 If transforming both: Apply the log function to each pair of $x_i$ and $y_i$ to get pairs of $log(x_1),log(y_1);...;log(x_n),log(y_n)$
    \1 \textbf{Transform}: Transform the data, then plot/store the resulting values
\end{outline}

\subsubsection{The exponential distribution}
The exponential distribution, utilized for data measuring time until an event, is defined by a rate parameter \(\lambda > 0\) with density function \(g(y) = \lambda e^{-\lambda y}\) for \(y > 0\). This distribution is suitable for strictly positive variables, with the probability for a variable \(Y\) to fall within an interval \((a, b)\) given by \(P(a < Y < b) = e^{-\lambda a} - e^{-\lambda b}\).

The mean of this distribution is \(\frac{1}{\lambda}\), hence \(\lambda\) can be estimated as \(\hat{\lambda} = \frac{1}{\bar{y}}\) from a sample with data points \(y_1, y_2, \ldots, y_n\).

For example, in a study of interspike intervals from neurons in guinea pigs, with an average interval of 0.872, the rate \(\lambda\) was estimated as \(\hat{\lambda} = 1.147\). This estimation appeared to match the data well, as shown in the histograms juxtaposed with the exponential distribution's density. However, a detailed examination indicated a mismatch, particularly fewer observations near zero than the exponential model would predict, highlighting the critical need for careful data evaluation against the chosen model.

\subsection{The central limit theorem}
The Central Limit Theorem (CLT) is a pivotal statistical principle stating that the distribution of the sample mean of independent, identically distributed (iid) variables approximates a normal distribution as the sample size, \(n\), grows, regardless of the original distribution's form. Formally, if \(y_1, y_2, \ldots, y_n\) are iid variables with mean \(\mu\) and standard deviation \(\sigma\), then the sample mean \(\bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i\) approaches a normal distribution, \(N\left(\mu, \frac{\sigma^2}{n}\right)\), for sufficiently large \(n\).

This theorem highlights the ubiquity of the normal distribution, suggesting that for large samples, averages or means of data can be approximated through the normal distribution, regardless of the original data's distribution.

Illustrations of the CLT include:

1. **Binary Variables**: For binary variables taking values 0 or 1 with probabilities \(1-p\) and \(p\), respectively, the distribution of the sample mean converges to a normal distribution with mean \(p\) and standard deviation \(\sqrt{\frac{p(1-p)}{n}}\) for large sample sizes.

2. **Bimodal Distribution**: In the case of a bimodal distribution, such as the concentration of a substance in cow milk differing between healthy and diseased cows, the sample mean across several observations trends towards a normal distribution with increasing \(n\), despite the original distribution's distinct bimodality, as demonstrated through simulation.

These examples underscore the CLT's broad applicability, affirming the normal distribution's central role in statistical analysis and probability theory.

\subsection{R Code}

\section{Law of Large Numbers and Multivariate Gaussian Distribution}

\subsection{Law of Large Numbers}

The Law of Large Numbers (LLN) is a fundamental theorem in probability and statistics that describes the result of performing the same experiment a large number of times. According to the LLN, the average of the results obtained from a large number of trials will converge to the expected value as the number of trials approaches infinity. This theorem provides a solid foundation for the concept of long-term stability in probabilities and is crucial for understanding the behavior of averages in large samples.

The LLN is divided into two versions: the Weak Law of Large Numbers (WLLN) and the Strong Law of Large Numbers (SLLN). Both versions assert that with a sufficient number of observations, the sample mean will be close to the population mean, but they differ in their mathematical rigor and conditions for convergence.

\subsubsection{Weak Law of Large Numbers (WLLN)}

The Weak Law of Large Numbers states that for a sequence of independent and identically distributed (iid) random variables with a finite expected value, the sample average converges in probability towards the expected value as the sample size increases. Mathematically, if $X_1,X_2,...,X_n$ are iid random variables with expected value $E[X]=\mu$ and variance $Var(X)=\sigma^2$, then for any $\epsilon > 0$,

$P(|\frac {1} {n} \sum_{i=1}^{n} X_i- \mu| \geq \epsilon) \rightarrow 0 \text{ as } n \rightarrow \infty$

This means that the probability of the sample mean deviating from the population mean by more than $\epsilon$ tends to zero as the sample size $n$ becomes very large.

\subsubsection{Strong Law of Large Numbers (SLLN)}

The Strong Law of Large Numbers strengthens the WLLN by stating that the sample average almost surely converges to the expected value. In other words, the probability that the sample mean converges to the population mean as the sample size approaches infinity is equal to one. For the same sequence of iid random variables as above, the SLLN asserts that:

$P(\lim\limits_{n\rightarrow \infty} (\frac {1} {n} \sum_{i=1}^{n} X_i)=\mu) =1$

The SLLN implies not just that large deviations become improbable as the sample size increases, but that they essentially do not occur.

\subsubsection{Implications and Applications}

The Law of Large Numbers has profound implications in both theoretical and applied statistics:

\begin{itemize}
    \item \textbf{Estimation of Probabilities}: It underlies the principle that probabilities can be estimated by the relative frequency of events in large numbers of trials.
    \item \textbf{Statistical Stability}: It justifies the expectation that empirical averages of large samples are stable and reliable estimates of theoretical averages.
    \item \textbf{Insurance and Gambling}: In insurance, premiums are priced on the expectation that, over a large number of policyholders, actual losses will average out to the expected losses. In gambling, the LLN explains why casinos always win in the long run.
\end{itemize}

\subsubsection{Conditions and Limitations}

\begin{itemize}
    \item \textbf{Independence and Identical Distribution}: The classical form of the LLN assumes that the random variables are independent and identically distributed. This condition can be relaxed in some versions of the theorem.
    \item \textbf{Finite Expected Value}: The random variables must have a finite expected value. If the expected value is infinite, the LLN does not apply.
    \item \textbf{Convergence}: The LLN speaks about convergence as the sample size goes to infinity. In practice, "large enough" sample sizes are often sufficient for the LLN to hold, but what constitutes "large enough" can vary depending on the distribution of the data.
\end{itemize}

\subsection{Multivariate Gaussian Distribution}

The Multivariate Gaussian Distribution, also known as the Multivariate Normal Distribution, extends the concept of the normal distribution to multiple dimensions, looking at several things at once (for example: height and weight).

\subsubsection{Mean Vector and Covariance Matrix}

\paragraph{Mean Vector} Vector of means for each variable; in our example, if we denote height by $X_1$ and weight by $X_2$, the mean vector $\boldsymbol \mu$ would look like: $\boldsymbol \mu = \begin{bmatrix} \mu_1 \\ \mu_2 \end{bmatrix}$ where $\mu_1$ and $\mu_2$ are the means for $X_1$ and $X_2$ respectively.

\paragraph{Covariance Matrix} The covariance matrix indicates how much each variable varies on its own and how they vary together.

\begin{itemize}
    \item \textbf{On its own (Diagonal)}: The diagonal tells us about the variance of each variable; in our example, how spread out the heights and weights are.
    \item \textbf{Together (Off-diagonal)}: The off-diagonal part tells us about the covariance; in our example, whether taller people tend to be heavier (a positive relationships) or lighter (a negative relationship). 
\end{itemize}

In our example, for height ($X_1$) and weight $X_2$, the covariance matrix $\boldsymbol \Sigma$ is: 

\begin{align*}
\boldsymbol{\Sigma} = \begin{bmatrix} {\sigma_{X_1}^2} & {\sigma_{X_1X_2}} \\  {\sigma_{X_1X_2}} & {\sigma_{X_2}^2} \end{bmatrix}
\end{align*}

Where:

\begin{itemize}
    \item $\sigma_{X_1}^2$ is the variance in height
    \item $\sigma_{X_2}^2$ is the variance in weight
    \item $\sigma_{X_1 X_2}$ is the covariance between height and weight
    
\end{itemize}

\subsubsection{The Probability Density Function (PDF)}

The PDF of the Multivariate Gaussian tells us how likely we are to see a particular combination of variables; in our example, height and weight.

The PDF for a $D$-dimensional vector $\boldsymbol{x} = (x_1, x_2), \boldsymbol{x} \in \mathbb{R}^D$, given mean vector $\boldsymbol{\mu}$ and covariance matrix $\boldsymbol{\Sigma}$, is:

\begin{align*}
p(\boldsymbol{x})= \frac {1} {(2\pi)^{D/2} |\boldsymbol{\Sigma}|^{1/2}} \exp{[-\frac 1 2 (\boldsymbol{x} - \boldsymbol{u})^T \boldsymbol{\Sigma}^{-1} (\boldsymbol{x} - \boldsymbol{u})]}
\end{align*}

When we plot the distribution, the tilt indicates the relationship (covariance) between the variables, while the shape indicates the variance of the variables. Circular plots indicate that the variables have the same variance.

\pagebreak

\subsection{R Code}

\subsubsection*{Law of Large Numbers}

\begin{lstlisting}
# Set the true parameters for the normal distribution
true_mean <- 0
true_sd <- 1

# Set the number of simulations
n_sim <- 25000

# Generate random samples from the normal distribution
samples <- rnorm(n_sim, mean = true_mean, sd = true_sd)

# Calculate the cumulative mean
cumulative_means <- cumsum(samples) / (1:n_sim)

# Plot the cumulative mean
plot(cumulative_means, type = "l", col = "blue", ylim = c(true_mean - 0.25, true_mean + 0.25),
     xlab = "Number of Samples", ylab = "Cumulative Average",
     main = "Demonstration of the Law of Large Numbers")

# Add a line for the true mean
abline(h = true_mean, col = "red", lwd = 2)

# Annotate the true mean
legend("bottomright", legend = c("Cumulative Mean", "True Mean"), col = c("blue", "red"), lty = 1)
\end{lstlisting}

\subsubsection*{Multivariate Gaussian Distribution}

\begin{lstlisting}
# Parameters
mean_vector <- c(170, 65)
mu_height <- 170
mu_weight <- 65
var_height <- 25
var_weight <- 36
cov_hw <- 18

# Generating grid points for height and weight
heights <- seq(mu_height - 3*sqrt(var_height), mu_height + 3*sqrt(var_height), length.out = 100)
weights <- seq(mu_weight - 3*sqrt(var_weight), mu_weight + 3*sqrt(var_weight), length.out = 100)

# Function to calculate the density of the Multivariate Gaussian
mvn_density <- function(x, y, mu, Sigma) {
  exp(-0.5 * t(c(x - mu[1], y - mu[2])) %*% solve(Sigma) %*% c(x - mu[1], y - mu[2])) /
    (2 * pi * sqrt(det(Sigma)))
}

# Calculate density values
Sigma <- matrix(c(var_height, cov_hw, cov_hw, var_weight), nrow = 2)
density_values <- outer(heights, weights, Vectorize(function(x, y) mvn_density(x, y, mu = c(mu_height, mu_weight), Sigma = Sigma)))

# Plot with filled contours
filled.contour(heights, weights, density_values,
               xlab = "Height (cm)", ylab = "Weight (kg)",
               main = "Height vs Weight Distribution with Color Shading",
               color.palette = colorRampPalette(c("blue", "green", "yellow", "red")),
               key.title = title(main = "Density Levels", cex.main = 1),
               key.axes = axis(4, las = 1))
\end{lstlisting}

\pagebreak

\section{Statistical models, estimation}

Statistical inference enables drawing broad conclusions from data. Statistical models are central for discussing estimation, confidence intervals, hypothesis testing, and prediction. The focus is on normal linear models where the response variable linearly depends on explanatory variables, with independent, normally distributed random deviations. These include linear regression, one-way ANOVA, and single sample models, expanding to more complex models in later chapters.

While linear models suit continuous variables, they don't apply to categorical data due to the inappropriate normal distribution. However, the underlying statistical principles, like estimation and hypothesis testing, remain crucial across all types of statistical analyses.

\subsection{Statistical models}

Formally, a statistical model is a set pf probability distributions $\mathcal{P}(\Omega)$ on the sample space $\Omega$. 

It links a response variable to explanatory variables and random variation. In linear models, the response is quantitative, and explanatory variables, also known as covariates or predictors, can be quantitative or categorical (factors). The model comprises a fixed part, describing the response's systematic change with explanatory variables, and a random part, accounting for biological variation unexplained by these variables.

For example, with digestibility percent as $y$ and stearic acid level as $x$, the fixed part follows $y=\alpha x + \beta$, where $\alpha$ and $\beta$ are constants. The random part describes variations around the predicted linear relationship.

In another case, the amount of organic material (response) varies by antibiotic type (categorical explanatory variable or factor). The fixed part describes differences in response among antibiotic types, and the random part captures variations within the same type.

Models can include multiple explanatory variables of different types, expanding in complexity. However, some datasets, like crab weight, may lack explanatory variables, focusing solely on the response with no linking factors.

\subsubsection{Model assumptions}

In linear regression, the relationship between the response variable $y_i$ and the explanatory variable $x_i$ is expressed as:

\begin{align*}
y_i = \alpha + \beta x_i + e_i, i=1,...,n
\end{align*}

Here, $\alpha$ (intercept) and $\beta$ (slope) are unknown parameters estimated from the data, and $e_i$ represents random variation.

\pagebreak

In one-way ANOVA, with $k$ groups $\alpha_1,...,\alpha_k$, the model is:

\begin{align*}
y_i = \alpha_{g(i)} + e_i, i=1,...,n
\end{align*}

where $\alpha_{g(i)}$ denotes the mean of the group to which $y_i$ belongs, identified by $g(i)=x_i$, and $e_i$ is the random variation.

In general, the model is written as:

\begin{align*}
y_i=\mu_i + e_i, i=1,...,n
\end{align*}

where $\mu_i$ encapsulates the contribution from explanitory variables, linear in parameters. Assuming one explanatory variable $x$, and defining $\theta_1,...,\theta_p$ as parameters, the function $f$ represents how $u_i$ depends on $x_i$ and parameters:

\begin{align*}
u_i=f(x_i;\theta_1,...,\theta_p)
\end{align*}

Here, $\theta_1,...,\theta_p$ are termed mean parameters or fixed effect parameters. Although this notation suggests one explanatory variable $x$, the function $f$ can incorporate multiple variables, exemplified as $u_i=f(x_{i1},x_{i2};\theta_1,...,\theta_p)$.

Comparing the linear regression model to the one-way ANOVA model:

\begin{itemize}
    \item \textbf{Linear Regression}: For $p=2$ parameters, ($\theta_1$,$\theta_2$) equate to ($\alpha$,$\beta$), with the function defined as $f(x_i;\alpha,\beta)=\alpha+\beta \cdot x_i$. The function is continuous, allowing interpolation between points, and is defined for all $x_i$, forming a straight line.
    \item \textbf{One-Way ANOVA}: For $p=k$ groups, $\theta_j$ aligns with $a_j$, and $f(x_i;\alpha_1,...,\alpha_k)=\alpha_{g(i)}=\alpha_{x_i}$. Here, $x_i$ values denote different groups; thus, interpolation doesn't apply, and $x_i$ may be non-quantitative. Unlike linear regression, ANOVA doesn't constrain function values to a line.
\end{itemize}

Remainder terms $e_1,...,e_n$ are independent and follow a normal distribution $N(0,\sigma^2)$, meaning they have zero mean and contribute to the variability around the expected value, $\mu_i$, of $y_i$. These standard terms ensure $y_i \sim N(\mu_i, \sigma^2)$, where $\mu_i$ is the expected value and $\sigma$ the common standard deviation, indicating typical deviation from the expected value.

The linear model's key characteristics include:

\begin{itemize}
    \item \textbf{Mean}: $\mu_i$ depends linearly on parameters and explanatory variables.
    \item \textbf{Variance Homogeneity}: All $y_i$ share the same standard deviation $\sigma$.
    \item \textbf{Normality}: $y_i$ are normally distributed.
    \item \textbf{Independence}: All $y_i$ are independent.
\end{itemize}

While the dependence on explanatory variables varies between models (e.g., linear regression vs. one-way ANOVA), the last three assumptions remain consistent across all linear models. In non-linear models, $\mu_i$ may depend on variables in a non-linear manner, e.g., $u_i=\alpha \cdot \exp (\beta \cdot x_i)$, which will be further explored later. Model assumptions must be validated using data, typically through residual analysis.

\subsubsection{Model formulas}

Statistical analyses are typically conducted using software programs, where a model formula, such as \verb|y = x|, is used to specify the relationship between the response variable \verb|y| and the explanatory variable \verb|x|. For example, \verb|digest = acid| represents a model where digestibility is influenced by acid levels, and \verb|organic =| \verb|type| indicates organic material's dependence on antibiotic types.

The interpretation of the model formula changes based on the explanatory variable's nature: it represents linear regression if \verb|x| is quantitative, and one-way ANOVA if \verb|x| is categorical (a factor). Correct coding of the explanatory variable is crucial to ensure the software performs the correct analysis.

The model formula \verb|y = x - 1| is used for fitting models without an intercept, affecting the analysis differently depending on whether the model is linear regression or one-way ANOVA.

Model formulas simplify statistical model specification, especially in complex scenarios involving multiple explanatory variables. While assumptions like variance homogeneity, normality, and independence are vital for describing random variation, they are usually treated as technical and constant across analyses, hence not typically included in the model formula.

\subsubsection{Linear regression}

In the context where the explanatory variable $x$ is quantitative, the linear regression model is formulated as:

\begin{align*}
    y_i=\alpha + \beta \cdot x_i + e_i, i=1,...,n
\end{align*}

where $e_1,...,e_n$ are independent and normally distributed with mean 0 and variance $\sigma^2$, symbolized as $N(0,\sigma^2)$. This setup implies that each $y_i$ is also independent and follows a normal distribution $N(\alpha + \beta \cdot x_i,\sigma^2)$. The model parameters are the intercept $\alpha$, the slope $\beta$, and the standard deviation $\sigma$.

The terms $e_1,...,e_n$ denote the residuals or vertical deviations from the fitted line, with the assumption of variance homogeneity indicating that these deviations are consistently distributed across all $x$ values. This implies that approximately 95\% of the observations lie within $2\sigma$ of the regression line, regardless of the $x$ values.

The model formula \verb|y = x| translates to equation at the beginning by default, including the intercept. If a model without an intercept is required, indicating a line through the origin (0,0), it should be explicitly defined in the model formula as \verb|y = x - 1|.

The linear regression model strictly analyzes the influence of $x$ on $y$, treating $x$ values as fixed and known, making the model conditional on $x$. If the variation of $x$ were also of interest, a more complex, two-dimensional model would be necessary.

\subsubsection{Comparison of groups}

In scenarios involving $k$ distinct groups, the one-way ANOVA model is employed, defined as:

\begin{align*}
    y_i=\alpha_{g(i)}+e_i, i=1,...,n
\end{align*}

Here, $x_i=g(i)$ specifies the $i$-th observation. The terms $e_1,...,e_n$ are assumed to be independent and normally distributed with mean zero and variance $\sigma^2$, leading to the independent distributions of $y_1,...,y_n$ as:

\begin{align*}
    y_i \sim N(\alpha_{g(i)}, \sigma^2), i=1,...,n
\end{align*}

This assumes normal distributions within each group with different means ($\alpha_1,...,\alpha_k$) but identical standard deviation $\sigma$ across all groups, indicating within-group variation.

The parameters of this model are $\alpha_1,...,\alpha_k$ (mean or population average of each group) and $\sigma$. Differences between group means, such as $\alpha_j - \alpha_l$, are often of interest for comparing the $j$-th and $l$-th groups.

In cases where $x$ represents a categorical variable or factor, the model formula \verb|y = x| reflects the one-way ANOVA structure. The clarity of the model depends on how groups are designated; numeric labels should be treated as categorical group identifiers to avoid misinterpretation as quantitative variables, which would inadvertently lead to a linear regression analysis instead.

For two unpaired samples, representing two groups, the situation simplifies to a special case of one-way ANOVA with $k=2$, under the assumption of equal standard deviations across both groups, thus:

$y_i \sim N(\alpha_1,\sigma^2)$ if in group 1, and $y_i \sim N(\alpha_2, \sigma^2)$ if in group 2.

\subsubsection{One sample}

For the crab weight data without an explanatory variable, the analysis assumes all weights ($y_1,...,y_n$) are independent samples from the same normal distribution, expressed as

\begin{align*}
    y_i \sim N(\mu,\sigma^2), i=1,...,n
\end{align*}

This scenario, known as the one-sample case, presumes all observations are from a population with a single mean $\mu$ and variance $\sigma^2$. The model can also be written as:

$y_i=\mu + e_i, i=1,...,n$

where $e_1,...,e_n$ are independent, normally distributed with mean zero and variance $\sigma^2$. Here, $\mu$ represents the population mean, and $\sigma$ represents the standard deviation.

The model formula for this case is written as \verb|y=1|, symbolizing a constant mean for all observations, applicable in the analysis of single sample scenarios.

In the context of paired samples, such as measurements from the same subjects under different conditions, the difference $d_i=y_{i2}-y_{i1}$ or the ratio $f_i=y_{i2}/y_{i1}$ between paired measurements ($y_{i1},y_{i2}$) is treated as a single sample response. This approach effectively converts paired observations into individual data points for analysis, assuming these differences or ratios represent independent samples from a normal distribution.

\subsection{Estimation}
We explore statistical models expressed as:
\begin{equation}
y_i = \mu_i + e_i, \quad i = 1, \ldots, n,
\end{equation}
where the error terms \(e_1, \ldots, e_n\) are independent and distributed according to \(N(0, \sigma^2)\), and the means \(\mu_i\) are functions of explanatory variables through unknown parameters, represented as:
\begin{equation}
\mu_i = f(x_i; \theta_1, \ldots, \theta_p).
\end{equation}
Our goal is to estimate the parameters \(\theta_1, \ldots, \theta_p\) using sample data, acknowledging that different samples may lead to varying estimates. This necessitates a focus on the variability and precision of these estimates, including the calculation of standard errors and confidence intervals. These intervals provide ranges that, to a certain extent, align with the observed data. Although \(\sigma^2\) is not the primary interest, its role is critical in determining the precision of the estimates for \(\theta\)s.

\subsubsection{Least squares estimation of the mean parameters}
Estimation in statistical modeling is aimed at identifying parameter values (\(\theta_1, \ldots, \theta_p\)) that best align the model with observed data. This concept of fit is operationalized via squared residuals, leading to the method of least squares estimation. This method, already applied in contexts such as linear regression and one-way ANOVA, is grounded in the minimization of the sum of squared deviations:
\begin{equation}
Q(\theta_1, \ldots, \theta_p) = \sum_{i=1}^{n} \left(y_i - f(x_i; \theta_1, \ldots, \theta_p)\right)^2.
\end{equation}
Here, \(Q\) measures the fit of the model, with the goal being to minimize these squared differences to achieve the closest match between observed values \(y_i\) and their expected counterparts from the model \(f(x_i; \theta_1, \ldots, \theta_p)\).

The least squares estimates (\(\hat{\theta}_1, \ldots, \hat{\theta}_p\)) are identified by finding the parameter values that minimize \(Q\). This involves computing the partial derivatives of \(Q\) with respect to each parameter \(\theta_j\) and solving the set of equations \(\frac{\partial Q}{\partial \theta_j} = 0\) for \(j = 1, \ldots, p\). The solution to these equations corresponds to a minimum of \(Q\), ensuring the model is as closely fitted to the data as possible under the least squares criterion.

\subsubsection{Estimation of the standard deviation $\sigma$}

Upon inserting parameter estimates into our model, we calculate the predicted (fitted) values, residuals, and the residual sum of squares as follows:
\begin{align}
\hat{y}_i &= \hat{\mu}_i = f(x_i; \hat{\theta}_1, \ldots, \hat{\theta}_p), \\
r_i &= y_i - \hat{y}_i, \\
SS_e &= \sum_{i=1}^{n} (y_i - \hat{y}_i)^2. \tag{5.8}
\end{align}
The fitted value \(\hat{y}_i\) approximates the expected outcome for observation \(i\), and the residual \(r_i\) serves as our estimate for the error term \(e_i\), given that \(e_i = y_i - \mu_i\). To estimate the variance \(\sigma^2\), we use the residuals:
\begin{equation}
s^2 = \hat{\sigma}^2 = \frac{SS_e}{n - p} = \frac{1}{n - p} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2. \tag{5.9}
\end{equation}
Here, \(n - p\) represents the residual degrees of freedom (denoted \(df_e\)), making \(s^2\) (or MSe) a critical measure of residual variance. This standardization ensures that \(s^2\)'s expected value is \(\sigma^2\).

The estimated standard deviation, denoted \(\hat{\sigma}\), is the square root of \(s^2\):
\begin{equation}
\hat{\sigma} = s = \sqrt{s^2} = \sqrt{\frac{SS_e}{df_e}}. \tag{5.10}
\end{equation}
This formulation aligns with estimations encountered in various models, confirming the least squares estimates' efficiency in minimizing variance estimates.


\subsubsection{Standard errors and distribution of least square estimates}

The process of estimating parameters from observed data yields questions about the reliability and variability of these estimates. Specifically, in linear models, the distribution of least squares estimates provides a basis for answering such questions. In the one-sample scenario, where observations \(y_1, \ldots, y_n\) are independent and \(N(\mu, \sigma^2)\) distributed, the sample mean \(\bar{y}\) serves as an estimator for \(\mu\), characterized by:
\begin{equation}
\hat{\mu} = \bar{y} \sim N(\mu, \frac{\sigma^2}{n}). \tag{5.11}
\end{equation}
This demonstrates that \(\hat{\mu}\) is both unbiased and consistent, improving in precision with increasing sample size.

Extending to all linear models:
\begin{equation}
y_i = \mu_i + e_i, \quad i = 1, \ldots, n, \tag{5.12}
\end{equation}
the least squares estimates \(\hat{\theta}_1, \ldots, \hat{\theta}_p\) are unbiased, consistent, and normally distributed. The variance associated with each estimate, denoted \(k_j \cdot \sigma^2\), reduces as the sample size increases:
\begin{equation}
\hat{\theta}_j \sim N(\theta, k_j \cdot \sigma^2). \tag{5.13}
\end{equation}
The standard error of these estimates, indicative of their variability, is given by:
\begin{equation}
SE(\hat{\theta}_j) = s\sqrt{k_j}. \tag{5.14}
\end{equation}
While each estimate is considered individually here, it's important to note that estimates may be correlated, influencing the interpretation of variability among them. This complexity, though beyond our current exploration, underscores the nuanced understanding required for comprehensive statistical analysis.


\subsubsection{Linear regression}
The linear regression model is given by:
\begin{equation}
y_i = \alpha + \beta \cdot x_i + e_i, \quad i = 1, \ldots, n, \tag{5.15}
\end{equation}
where \(e_1, \ldots, e_n\) are independent and \(N(0, \sigma^2)\) distributed. The least squares estimates for \(\beta\) and \(\alpha\) are:
\begin{align}
\hat{\beta} &= \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^2}, \\
\hat{\alpha} &= \bar{y} - \hat{\beta} \cdot \bar{x}.
\end{align}
Defining \(SS_x = \sum_{i=1}^{n}(x_i - \bar{x})^2\), the distributions of \(\hat{\beta}\) and \(\hat{\alpha}\) are:
\begin{align}
\hat{\beta} &\sim N\left(\beta, \frac{\sigma^2}{SS_x}\right), \\
\hat{\alpha} &\sim N\left(\alpha, \sigma^2\left(\frac{1}{n} + \frac{\bar{x}^2}{SS_x}\right)\right).
\end{align}
As sample size increases, \(SS_x\) increases, decreasing the variances of \(\hat{\beta}\) and \(\hat{\alpha}\), thus enhancing precision.

The standard errors are calculated as:
\begin{align}
SE(\hat{\beta}) &= \frac{s}{\sqrt{SS_x}}, \\
SE(\hat{\alpha}) &= s \sqrt{\frac{1}{n} + \frac{\bar{x}^2}{SS_x}},
\end{align}
where \(s = \sqrt{\frac{SS_e}{dfe}} = \sqrt{\frac{1}{n - 2} \sum_{i=1}^{n}(y_i - \hat{\alpha} - \hat{\beta} \cdot x_i)^2}\).

For a new value \(x_0\), the expected response \(\hat{\mu}_0 = \hat{\alpha} + \hat{\beta} \cdot x_0\) is normally distributed, with:
\begin{equation}
\hat{\mu}_0 \sim N\left(\alpha + \beta \cdot x_0, \sigma^2\left(\frac{1}{n} + \frac{(x_0 - \bar{x})^2}{SS_x}\right)\right),
\end{equation}
and its standard error:
\begin{equation}
SE(\hat{\mu}_0) = s \sqrt{\frac{1}{n} + \frac{(x_0 - \bar{x})^2}{SS_x}}. \tag{5.16}
\end{equation}
This model ensures that \(\hat{\mu}_0\) is an unbiased and consistent estimate for the expected value at \(x_0\).

\textit{Example 5.3 (Stearic acid and digestibility of fat):} Given estimates \(\hat{\alpha} = 96.5334\), \(\hat{\beta} = -0.9337\), \(SS_e = 61.7645\), \(s^2 = 8.8234\), \(s = 2.970\), \(\bar{x} = 14.5889\), and \(SS_x = 1028.549\), for a stearic acid level \(x_0 = 20\%\), the digestibility is expected to decrease by 0.93 percentage points for a 1 percentage point increase in stearic acid, with a predicted digestibility of \(77.859\) and a standard error of \(1.1096\) for the prediction.

\subsubsection{Comparison of groups}
In a one-way ANOVA framework, the model is formulated as:
\begin{equation}
y_i = \mu_{g(i)} + e_i, \quad i = 1, \ldots, n,
\end{equation}
where \(g(i)\) denotes the group of the \(i\)th observation, and \(e_1, \ldots, e_n\) are independent \(N(0, \sigma^2)\) distributed errors. The least squares estimates for the group means \(\mu_1, \ldots, \mu_k\) are the group averages:
\begin{equation}
\hat{\mu}_j = \bar{y}_j = \frac{1}{n_j} \sum_{\{i | g(i) = j\}} y_i \sim N\left(\mu_j, \frac{\sigma^2}{n_j}\right).
\end{equation}
The standard errors for these estimates are given by:
\begin{equation}
SE(\hat{\mu}_j) = s \sqrt{\frac{1}{n_j}},
\end{equation}
where \(s\) is the square root of the pooled variance estimate \(s^2 = \frac{SS_e}{dfe}\).

Differences between group means, or contrasts, are of particular interest. For two groups \(j\) and \(l\), the estimate of the difference is:
\begin{equation}
\hat{\mu}_j - \hat{\mu}_l = \bar{y}_j - \bar{y}_l,
\end{equation}
with the standard error:
\begin{equation}
SE(\hat{\mu}_j - \hat{\mu}_l) = s \sqrt{\frac{1}{n_j} + \frac{1}{n_l}}. \tag{5.17}
\end{equation}
\textbf{Example 5.4 (Antibiotics and dung decomposition):} For the antibiotics data, estimates for group means and comparisons to the control group are calculated, demonstrating the application of the above formulas. The standard error calculations illustrate the impact of group size on precision, with smaller groups leading to larger standard errors.

This section showcases the methodology for estimating and comparing group means in one-way ANOVA, emphasizing the statistical principles and computations involved.


\subsubsection{One sample}

In cases involving a single sample, where observations \(y_1, \ldots, y_n\) are independent and \(N(\mu, \sigma^2)\) distributed, the estimation formulas adapt to this new terminology as follows:
\begin{align}
\hat{\mu} &= \bar{y} \sim N\left(\mu, \frac{\sigma^2}{n}\right), \\
\hat{\sigma}^2 &= s^2 = \frac{1}{n - 1} \sum_{i=1}^{n} (y_i - \bar{y})^2,
\end{align}
leading to the standard error of \(\hat{\mu}\) being:
\begin{equation}
SE(\hat{\mu}) = \frac{s}{\sqrt{n}}.
\end{equation}
\textbf{Example 5.5 (Crab weights):} For a dataset concerning crab weights, we find that:
\begin{itemize}
\item Estimated mean weight, \(\hat{\mu} = \bar{y} = 12.76\),
\item Sample standard deviation, \(s = 2.25\),
\item Standard error of the mean, \(SE(\hat{\mu}) = 2.25 \cdot \sqrt{\frac{1}{162}} = 0.177\).
\end{itemize}
This example showcases the estimation of the average weight of crabs to be 12.76, with the standard error providing insight into the precision of this estimate.


\subsection{Maximum likelihood}

When estimating parameters without clear sample equivalents, the maximum likelihood principle is useful. For instance, estimating the probability ($p$) of becoming pregnant within one cycle can be complex. Traditional methods might not use all data, like only considering first-cycle pregnancies, which gives incomplete estimates (e.g., 0.29 for smokers, 0.41 for nonsmokers). Also, methods ignoring cycles beyond a certain count can overestimate $p$. The maximum likelihood principle, however, uses all available data to provide more accurate and comprehensive estimators, solving these issues.

\subsubsection{The maximum likelihood principle}

The maximum likelihood principle estimates model parameters by maximizing the probability of observed data. This involves constructing a likelihood function, $L(\theta)$, reflecting how likely the observed data are given certain parameter values. The goal is to find parameter values that make this function reach its maximum, known as maximum likelihood estimates (MLEs).

For instance, consider testing 10 chips from a batch with unknown defect rates. If one chip is defective, and the batches could have 10\% or 50\% defective rates, the likelihoods of this outcome under both scenarios can be calculated. The MLE approach would favor the batch with the higher likelihood for the observed data, in this case the 10\% defect rate, guiding a decision between two uncertain options based on which scenario is statistically more probable.

\subsubsection{Likelihood and loglikelihood}

The likelihood function $L(\theta)$ represents the probability of observing a specific dataset given certain parameter values, $\theta$, for the underlying distribution.

\begin{itemize}
    \item For discrete distributions, the likelihood is the product of the probabilities of each data point $x_i$, given $\theta$, represented as $p_{\theta}(x_i)$. 
    \item For continuous distributions, where direct probabilities at a specific point are zero, likelihood is defined through the PDF $f_{\theta}(x)$, leading to the likelihood function $L(\theta)=f_{\theta}(x_1) \cdot f_{\theta}(x_2) \cdot ... \cdot f_{\theta}(x_n)$.
\end{itemize}

The maximal likelihood estimates (MLEs) are values of $\theta$ that maximize the value of $L(\theta)$, or the loglikelihood function $\ell()(\theta)=\text{ln}(L(\theta))$, which simplifies the differentiation process due to the logarithm transforming products into sums.

As an example, for a dataset modeled as a sample from an Exponential distribution with parameter $\lambda$, the likelihood function is $L(\lambda)=\lambda^n \cdot e^{-\lambda \sum_{i=0}^{n} x_i}$, and the loglikelihood function is $\ell (\lambda)=n \text{ln}(\lambda)-\lambda \sum_{i=0}^{n} x_i$. To find the MLE of $\lambda$ we differentiate $\ell (\lambda)$ with respect to $\lambda$ and set it to zero, yielding $\lambda = \frac {1} {\bar{x}_n}$, where $\bar{x}_n$ represents the sample mean.

\subsubsection{Properties of maximum likelihood estimators}

Maximum likelihood estimators (MLEs) are not only universally applicable due to the maximum likelihood principle, but they also possess several desirable statistical properties.

\begin{itemize}
    \item \textbf{Invariance Principle}: The invariance principle of MLEs dictates that if \(T\) is the MLE for a parameter \(\theta\), then for any invertible function \(g\), \(g(T)\) becomes the MLE for \(g(\theta)\). For instance, if 
\[
D_{n} = \sqrt{\frac{1}{n} \sum_{i=1}^{n}(X_{i} - \bar{X}_{n})^2}
\]
is the MLE for \(\sigma\) in a normal distribution, then \(D_{n}^2\) is the MLE for \(\sigma^2\).

    \item \textbf{Asymptotic Unbiasedness}: While MLEs may exhibit bias for finite sample sizes, they are asymptotically unbiased. This means that the expected value of an MLE \(T_n\) approaches the true parameter value \(\theta\) as the sample size \(n\) increases infinitely, i.e.,
\[
\lim_{n \rightarrow \infty} \mathrm{E}[T_{n}] = \theta.
\]

    \item \textbf{Asymptotic Minimum Variance}: MLEs attain the minimum possible variance asymptotically among all unbiased estimators, meeting the Cramér-Rao lower bound. This property ensures that MLEs are the most efficient estimators as the sample size becomes large.
\end{itemize}

\subsection{Bayesian Inference}

Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available. It is an important technique in statistical analysis that differs from classical inference by incorporating prior knowledge into the model.

The foundation of Bayesian inference is Bayes' theorem, which is expressed as:

$$
\text{Posterior} = \frac{\text{Likelihood} \times \text{Prior}}{\text{Evidence}}
$$

where:
\begin{itemize}
    \item \textbf{Posterior} is the probability of the parameters given the data.
    \item \textbf{Likelihood} is the probability of the data given the parameters.
    \item \textbf{Prior} is the initial belief about the parameters, before seeing the data.
    \item \textbf{Evidence} is the total probability of the data under all possible parameter values.
\end{itemize}

\subsubsection{Posterior Distribution of Parameters}

The posterior distribution combines the prior distribution and the likelihood of observed data to give a updated distribution reflecting the current state of knowledge. In mathematical terms:

$$
p(\theta | D) = \frac{p(D | \theta) \cdot p(\theta)}{p(D)}
$$

where \( \theta \) represents the parameters, and \( D \) represents observed the data.

\subsubsection{Posterior Summaries}

The posterior distribution's summaries include the Conditional Mean (CM) estimate and the Maximum A Posteriori (MAP) estimate.

\begin{itemize}
    \item \textbf{CM estimate}: The CM estimate is the expected value of the posterior distribution:
\[
\text{CM} = E[\theta | \text{D}] = \int \theta \times p(\theta | \text{D}) \, d\theta
\]
\textbf{Pros:} Represents the average parameter value considering the posterior distribution; includes all possible parameter values weighted by their posterior probabilities.

\textbf{Cons:} Computationally intensive for complex models; may not represent the most probable parameter value if the distribution is skewed.

    \item \textbf{MAP Estimate}: The MAP estimate identifies the parameter value that maximizes the posterior distribution:
\[
\text{MAP} = \underset{\theta}{\text{argmax}} \, p(\theta | \text{D})
\]
\textbf{Pros:} Provides the most probable parameter value; often computationally simpler than finding the CM.

\textbf{Cons:} May be sensitive to the choice of prior; does not consider the full distribution of parameter values.

\end{itemize}

\section{Confidence intervals}

Confidence intervals provide a range of plausible values for an unknown parameter, allowing statements about the level of confidence that the true parameter value is within this range. This approach differs from giving a single point estimate and allows the selection of a confidence level.

\subsection{General Principle}
Sample statistics, like the sample mean and variance, are used as estimators for the true values ($\mu$ and $\sigma^2$) of a distribution. These estimators are evaluated for their bias and mean squared error (MSE) to judge their quality. Given an estimator $T$ for a parameter $\theta$, its realization $t$ serves as the point estimate. However, since results can vary with different samples, an interval estimate is more informative than a point estimate.

By using the sampling distribution of an estimator, we can create confidence intervals for unknown parameters. If $T$ is an unbiased estimator for $\theta$ with a standard deviation of $\sigma_T$, then from Chebyshev's inequality, it follows that:

\begin{equation*}
P\left(|T-\theta| < 2\sigma_T\right) \geq \frac{3}{4}.
\end{equation*}

In an example, such as Michelson's speed of light measurements, this implies that, with at least 75\% probability, $T$ is within $200 \text{ km/sec}$ of the true parameter $\theta$. Equivalently, we can express the uncertainty about $\theta$ as:

\begin{equation*}
\theta \in (T-200, T+200) \text{ with probability at least } 75\%.
\end{equation*}

When applying this to specific data, such as these measurements, we derive a realized interval estimate. However, once a specific interval is calculated, the probability statement changes to a confidence statement:

\begin{equation*}
\theta \in (299652.4, 300052.4) \text{ with confidence at least } 75\%.
\end{equation*}

This reflects the methodological shift from a probabilistic statement about the estimator to a confidence statement about the parameter, based on the actual data obtained and the properties of the estimator.

\subsubsection{General defenition}

Confidence intervals are typically expressed in the form $(t-c \cdot \sigma_{T}, t+c \cdot \sigma_{T})$, where $c$ is usually close to 2 or 3, indicating a higher confidence level than more conservative intervals. 

A $100\gamma\%$ confidence interval for a parameter $\theta$ from a dataset $x_1, \ldots, x_n$, modeled as realizations of random variables $X_1, \ldots, X_n$, is defined as follows: if there exist sample statistics $L_n=g(X_1, \ldots, X_n)$ and $U_n=h(X_1, \ldots, X_n)$ such that $P(L_n < \theta < U_n) = \gamma$ for every $\theta$, then $(l_n, u_n)$, where $l_n=g(x_1, \ldots, x_n)$ and $u_n=h(x_1, \ldots, x_n)$, constitutes a $100\gamma\%$ confidence interval for $\theta$. $\gamma$ here is the confidence level, a number between 0 and 1.

In cases where exact sample statistics $L_n$ and $U_n$ do not exist, a conservative confidence interval may be used, which satisfies $P(L_n < \theta < U_n) \geq \gamma$. Although such intervals ensure a minimum confidence level, they might be more inclusive than necessary, making them conservative.

An illustrative example uses simulation to construct 90\% confidence intervals for the mean $\mu$ of a normal distribution based on sample data. In practice, multiple confidence intervals generated from different datasets give a visual representation of their effectiveness; for instance, in 50 generated intervals for $\mu=0$, 46 might contain the true $\mu$, showcasing the operational concept of confidence intervals.

In the following sections, we explore confidence intervals for the mean, starting with known variance cases and progressing to unknown variance and non-normal distributions, including the use of bootstrapping and central limit theorem applications for large sample sizes.

\subsection{Normal Data and Confidence Intervals}

Consider a sample $X_{1}, \ldots, X_{n}$ from a normal distribution $N(\mu, \sigma^{2})$, where $\mu$ is the parameter of interest and $\sigma^{2}$ is known. Confidence intervals for $\mu$ can be derived using the critical values $z_{p}$ from the standard normal distribution, satisfying $\mathrm{P}(Z \geq z_{p})=p$.

The sample mean from a normal distribution $N(\mu, \sigma^{2}/n)$ is denoted by $\bar{X}_{n}$, and the standardized form $\frac{\bar{X}_{n}-\mu}{\sigma / \sqrt{n}}$ follows an $N(0,1)$ distribution. For a confidence level $\gamma$, choose $c_{l}$ and $c_{u}$ such that $\mathrm{P}(c_{l}<Z<c_{u})=\gamma$. Then, a $100 \gamma \%$ confidence interval for $\mu$ is:
\[
\left(\bar{x}_{n}-c_{u} \frac{\sigma}{\sqrt{n}}, \bar{x}_{n}-c_{l} \frac{\sigma}{\sqrt{n}}\right).
\]
Typically, $\alpha=1-\gamma$ is split equally between the tails, yielding:
\[
\left(\bar{x}_{n}-z_{\alpha / 2} \frac{\sigma}{\sqrt{n}}, \bar{x}_{n}+z_{\alpha / 2} \frac{\sigma}{\sqrt{n}}\right).
\]

For unknown variance, the formula changes as the standard normal distribution is no longer applicable. Instead, with $S_{n}$ as the sample standard deviation, the formula $\frac{\bar{X}_{n}-\mu}{S_{n} / \sqrt{n}}$ follows a t-distribution with $n-1$ degrees of freedom, resulting in a $100(1-\alpha)\%$ confidence interval for $\mu$:
\[
\left(\bar{x}_{n}-t_{n-1, \alpha / 2} \frac{S_{n}}{\sqrt{n}}, \bar{x}_{n}+t_{n-1, \alpha / 2} \frac{S_{n}}{\sqrt{n}}\right).
\]

Examples include determining confidence intervals for the gross calorific value of coal, with the interval's width and critical values depending on whether $\sigma$ is known or estimated.

\subsection{Bootstrap Confidence Intervals}

When the normal distribution does not adequately model the data, especially with small samples, bootstrapping is recommended for constructing confidence intervals for an unknown mean $\mu$ from a dataset $x_{1}, \ldots, x_{n}$ from some distribution $F$. Normally, we find $c_{l}$ and $c_{u}$ for the confidence interval based on the studentized mean $T=\frac{\bar{X}_{n}-\mu}{S_{n} / \sqrt{n}}$. In bootstrapping, we estimate the distribution $F$ with $\hat{F}$, sample $X_{1}^{*}, \ldots, X_{n}^{*}$ from $\hat{F}$, and use $T^{*}=\frac{\bar{X}_{n}^{*}-\mu^{*}}{S_{n}^{*} / \sqrt{n}}$ to approximate $T$'s distribution. 

We generate many bootstrap samples and calculate corresponding $t^{*}$ values, from which we derive the critical values $c_{l}^{*}$ and $c_{u}^{*}$ that approximate $c_{l}$ and $c_{u}$. Thus, a $100(1-\alpha)\%$ bootstrap confidence interval for $\mu$ is:

\[
\left(\bar{x}_{n}-c_{u}^{*} \frac{s_{n}}{\sqrt{n}}, \bar{x}_{n}-c_{l}^{*} \frac{s_{n}}{\sqrt{n}}\right).
\]

For instance, with software data exhibiting right skewness, a $90\%$ bootstrap confidence interval was found using the $5$th and $95$th percentiles of the $t^{*}$ values. The bootstrap method provides a more accurate reflection of the actual data distribution, particularly for skewed distributions, compared to the standard t-distribution based intervals. This results in more accurate confidence intervals, particularly for non-normal data.

\subsection*{Large Samples and Confidence Intervals}

The central limit theorem suggests that for large sample sizes $n$, the distribution of the studentized mean $\frac{\bar{X}_{n}-\mu}{S_{n} / \sqrt{n}}$ approaches the standard normal distribution. This foundation supports the construction of large sample confidence intervals. For a large enough sample size from any distribution $F$ with expectation $\mu$, the approximate $100(1-\alpha) \%$ confidence interval for $\mu$ is:

\[
\left(\bar{x}_{n}-z_{\alpha / 2} \frac{s_{n}}{\sqrt{n}}, \bar{x}_{n}+z_{\alpha / 2} \frac{s_{n}}{\sqrt{n}}\right).
\]

However, determining "how large is large enough" depends on the distribution characteristics and the desired accuracy. Simulation results for different distributions and sample sizes provide empirical coverage probabilities, aiding in assessing the adequacy of sample size for accurate confidence intervals. For skewed distributions, larger sample sizes might be needed to achieve desired coverage levels.

For instance, Rutherford and Geiger's 1910 study on alpha-particle emission employed a significantly large dataset to construct a 98\% confidence interval for the expected number of particles, using the large sample method, which resulted in:

\[
\left(3.8715-2.33 \frac{1.9225}{\sqrt{2608}}, 3.8715+2.33 \frac{1.9225}{\sqrt{2608}}\right)=(3.784,3.959).
\]

\section{More on Confidence Intervals}

This sections extends the discussion of confidence intervals beyond the expectation-focused cases in last section. Initially, we explore confidence intervals for the binomial distribution parameter $p$, then generalize the approach to construct confidence intervals. Following this, one-sided confidence intervals and determinations of sample size for specific interval widths are discussed.

\subsection{The Probability of Success}
When estimating the proportion $p$ in a binomial distribution $\operatorname{Bin}(n, p)$, typically the sample proportion $X/n$ is used as an estimator. The Wilson method provides approximate $100(1-\alpha)\%$ confidence intervals for large $n$, despite its confidence level potentially deviating for $p$ near 0 and 1.

The normal approximation of the binomial leads to the inequality for constructing confidence intervals:
\[
-z_{\alpha / 2}<\frac{\frac{X}{n}-p}{\sqrt{\frac{p(1-p)}{n}}}<z_{\alpha / 2}
\]
which translates to a quadratic inequality after solving for $p$:
\[
\left(\frac{X}{n}-p\right)^{2}-\left(z_{\alpha / 2}\right)^{2} \frac{p(1-p)}{n}<0.
\]
Upon substitution of actual sample values and solving for $p$, we obtain the confidence interval.

For instance, for a sample where 78 out of 125 voters support a candidate, with a $95\%$ confidence level, the confidence interval for the population proportion $p$ would be $(0.54,0.70)$, derived from the quadratic inequality.

\subsubsection*{Coverage probabilities and alternative methods}
An alternative method for constructing confidence intervals for the binomial parameter $p$, especially for $p$ near 0 or 1, is the Agresti-Coull method. The method adjusts the original count $X$ and the sample size $n$ as follows:
\[
\tilde{X} = X + \frac{z_{\alpha/2}^2}{2}, \quad \tilde{n} = n + z_{\alpha/2}^2,
\]
and calculates the adjusted proportion:
\[
\tilde{p} = \frac{\tilde{X}}{\tilde{n}}.
\]
The approximate $100(1-\alpha)\%$ confidence interval for $p$ is then given by:
\[
\left(\tilde{p} - z_{\alpha / 2} \sqrt{\frac{\tilde{p}(1-\tilde{p})}{\tilde{n}}}, \tilde{p} + z_{\alpha / 2} \sqrt{\frac{\tilde{p}(1-\tilde{p})}{\tilde{n}}}\right).
\]

\subsection{Is there a general method}

To construct confidence intervals in new situations, consider a random sample $X_{1}, \ldots, X_{n}$ from a shifted exponential distribution $X_{i} = \delta + Y_{i}$ where $Y_{1}, \ldots, Y_{n}$ are from an $\operatorname{Exp}(1)$ distribution. The minimum value $T = \min \{X_{1}, \ldots, X_{n}\}$ serves as the maximum likelihood estimator for the minimum lifetime $\delta$. Given that $M = \min \{Y_{1}, \ldots, Y_{n}\}$ follows an $\operatorname{Exp}(n)$ distribution, the distribution function of $T$ is defined as:

\begin{align*}
F_{T}(a) = \mathrm{P}(T \leq a) = \begin{cases}
0, & \text{for } a < \delta \\
1 - \mathrm{e}^{-n(a-\delta)}, & \text{for } a \geq \delta
\end{cases}
\end{align*}

For constructing a confidence interval for $\delta$, we solve $\mathrm{P}(c_{l} < T < c_{u}) = 1 - \alpha$ by setting $\mathrm{P}(T \leq c_{l}) = \mathrm{P}(T \geq c_{u}) = \frac{\alpha}{2}$. This leads to:

\[
c_{l} = \delta - \frac{1}{n} \ln \left(1-\frac{\alpha}{2}\right), \quad c_{u} = \delta - \frac{1}{n} \ln \left(\frac{\alpha}{2}\right).
\]

A $100(1-\alpha)\%$ confidence interval for $\delta$ is therefore:

\[
\left(t + \frac{1}{n} \ln \left(\frac{\alpha}{2}\right), t + \frac{1}{n} \ln \left(1 - \frac{\alpha}{2}\right)\right).
\]

For $\alpha = 0.05$, the confidence interval becomes:

\[
\left(t - \frac{3.69}{n}, t - \frac{0.0253}{n}\right).
\]


\subsubsection*{Remark about a general method}

If a sample statistic $T$ and unknown parameter $\theta$ from a random sample $X_{1}, \ldots, X_{n}$ can be linked through functions $g(\theta)$ and $h(\theta)$ such that $\mathrm{P}(g(\theta) < T < h(\theta)) = 1-\alpha$ for all $\theta$, then $\left(h^{-1}(t), g^{-1}(t)\right)$ forms a $100(1-\alpha)\%$ confidence interval for $\theta$. This approach can be applied to derive confidence statements about $\theta$ when $g$ and $h$ are strictly increasing functions.

\subsection{One-sided Confidence Intervals}

In scenarios such as evaluating a shipment's gross calorific content, one-sided confidence intervals can be particularly useful. For instance, when ensuring that the calorific content exceeds a certain threshold, say $31.00 \mathrm{MJ}/\mathrm{kg}$. From earlier data, a two-sided $95\%$ confidence interval for the gross calorific content was $(30.946,31.067)$. In situations requiring only a lower bound, a one-sided confidence interval becomes relevant:

\[
\mathrm{P}\left(\bar{X}_{n}-t_{n-1, \alpha} \frac{S_{n}}{\sqrt{n}} < \mu \right) = 1 - \alpha.
\]

This suggests a one-sided $100(1-\alpha)\%$ confidence interval for $\mu$ is:

\[
\left(\bar{x}_{n}-t_{n-1, \alpha} \frac{s_{n}}{\sqrt{n}}, \infty\right).
\]

For another example to do with coal, adopting $\alpha = 0.05$ with $t_{21,0.05} = 1.721$, the interval is:

\[
\left(31.012-1.721 \frac{0.1294}{\sqrt{22}}, \infty\right) = (30.964, \infty).
\]

This one-sided interval has a lower bound higher than the two-sided one, reflecting that all uncertainty is placed on one side, although it is still below 31.00.

The formal definition of confidence intervals includes one-sided versions. For a statistic $L_{n}$ such that $\mathrm{P}(L_{n} < \theta) = \gamma$ for every $\theta$, $\left(l_{n}, \infty\right)$ is a $100\gamma\%$ one-sided confidence interval for $\theta$, termed a lower confidence bound. Conversely, for $U_{n}$ where $\mathrm{P}(\theta < U_{n}) = \gamma$, the one-sided interval is $\left(-\infty, u_{n}\right)$, with $u_{n}$ being an upper confidence bound.

\subsection{Determining the Sample Size}

The goal is often to achieve as narrow a confidence interval as possible for greater accuracy and more definitive conclusions. To pre-specify the accuracy of a confidence interval, determining the necessary sample size becomes essential. This varies with the type of confidence interval but is guided by common principles.

For instance, consider determining the necessary sample size for testing the calorific content of coal to achieve a $95\%$ confidence interval narrower than $0.05 \mathrm{MJ}/\mathrm{kg}$. Assuming measurements follow a normal distribution with standard deviation $\sigma = 0.1$ under ISO method 1928, the desired confidence level $1-\alpha$ entails a confidence interval width:

\[
2 \cdot z_{\alpha / 2} \frac{\sigma}{\sqrt{n}}
\]

To ensure the width does not exceed a certain value $w$, the minimum sample size $n$ should satisfy:

\[
n \geq \left(\frac{2 z_{\alpha / 2} \sigma}{w}\right)^2.
\]

In our calorific content example, with $w = 0.05$, $\sigma = 0.1$, and $z_{0.025} = 1.96$, the calculation yields:

\[
n \geq \left(\frac{2 \cdot 1.96 \cdot 0.1}{0.05}\right)^2 = 61.4,
\]

indicating a requirement of at least 62 measurements to meet the width criterion.

If $\sigma$ is unknown, an estimate must be used to approximate the required sample size, recognizing that the actual standard deviation determined post-data collection might differ, influencing the ultimate confidence interval size.

\section{Hypothesis Tests}

Hypothesis tests evaluate whether the observed data deviate significantly from what is expected under specific assumptions, forming a critical aspect of statistical inference. They are integral to understanding if differences in data (such as changes in biological measures) are due to chance or actual effects.

Consider an example from a biological study where we assess the impact of a new feed on the hormone levels of cattle. We observe changes in hormone levels before and after administering the feed to nine cows. The data exhibits variations, with most cows showing an increase in hormone levels, hinting at the feed's potential effect.

In hypothesis testing, we initially establish a null hypothesis ($H_0$), which in our case is that the feed has no effect on hormone levels, hence $\mu = 0$, where $\mu$ is the mean difference in hormone concentrations. To assess this, we compute the mean ($\bar{d}$) of the observed differences in hormone levels. For the cattle study, $\bar{d} = 13.78\, \mu \mathrm{g/ml}$.

To determine if this mean difference is statistically significant, we calculate a confidence interval. For our example, the 95\% confidence interval for the mean difference does not include zero (2.06, 25.49), suggesting that the null hypothesis might be false and the feed does indeed have an effect.

Further analysis involves a t-test, where the test statistic $T_{\text{obs}}$ is calculated as:

\[
T_{\text{obs}} = \frac{\bar{d} - \mu}{\text{SE}(\bar{d})} = \frac{\bar{d} - 0}{\text{SE}(\bar{d})} = \frac{13.78}{5.08} = 2.71,
\]

where SE($\bar{d}$) is the standard error of the mean difference. The t-test compares $T_{\text{obs}}$ against a t-distribution with $n-1$ degrees of freedom (here, 8). The resulting p-value, the probability of observing a test statistic as extreme as $T_{\text{obs}}$ under $H_0$, is 0.026. Since this p-value is below the conventional alpha level of 0.05, we reject $H_0$, indicating the feed likely affects hormone levels.

However, a hypothesis test's outcome is never absolute. Rejecting $H_0$ doesn't confirm the alternative hypothesis unequivocally; it merely indicates that under the null hypothesis, the observed data are unlikely. The test's significance level, typically set at 5\%, is the threshold for deciding whether the observed data are sufficiently improbable under $H_0$ to warrant this rejection. Importantly, decisions in hypothesis testing are susceptible to errors: Type I errors (false positives) occur if we reject a true $H_0$, while Type II errors (false negatives) occur if we fail to reject a false $H_0$.

In sum, hypothesis testing in research, such as in the cattle feed study, involves setting up a null hypothesis, determining the appropriate test statistic, and calculating a p-value to decide whether the observed data significantly contradict $H_0$. Through this systematic approach, scientists can assess the likelihood of observed effects arising by chance, thus informing more confident conclusions about biological phenomena.

\subsection{Null Hypotheses}

In scientific research, data collection aims to answer questions like "Does the treatment work?" or "Is there a relationship between variables such as age and blood substance concentration?" This leads to the formulation of statistical hypotheses, which are simplifying assumptions about our statistical model.

For example, consider the linear model:

\[
y_{i} = \mu_{i} + e_{i}, \quad i = 1, \ldots, n,
\]

where $\mu_{i}$ are means influenced by parameters and variables, and $e_{i}$ are independent, normally distributed error terms with mean 0 and variance $\sigma^{2}$. A null hypothesis ($H_0$) simplifies the statistical model by making specific assertions about these parameters.

A single sample hypothesis might propose a specific mean value $\mu_0$, asserting that all $y_i$ are drawn from $N(\mu_0, \sigma^2)$. In linear regression, such as $y_i = \alpha + \beta \cdot x_i + e_i$, the hypothesis $H_0: \beta = 0$ suggests no linear relationship between $x$ and $y$, leading to all $y_i$ having the distribution $N(\alpha, \sigma^2)$ under $H_0$.

In one-way ANOVA, where observations are from different groups, the null hypothesis can assert no difference among group means, $H_0: \alpha_1 = \alpha_2 = \cdots = \alpha_k$. If true, all observations are from $N(\alpha, \sigma^2)$ distributions, with $\alpha$ being the shared group mean.

These examples demonstrate how hypotheses are deeply integrated with the underlying statistical models, reinforcing the necessity of selecting appropriate models for data analysis. The null model, or model under $H_0$, imposes additional constraints to simplify the original model.

Each null hypothesis has an alternative hypothesis ($H_A$), which represents what is true if $H_0$ is false. Often, $H_A$ is simply the logical opposite of $H_0$, such as $H_A: \beta \neq 0$ for linear regression. However, directional alternatives like $H_A: \beta > 0$ or $H_A: \beta < 0$ can also be appropriate depending on the research question. It is crucial to establish $H_A$ before data analysis to avoid biases.

The null hypothesis typically corresponds to "no effect" or "no association", which can be precisely defined in the model. A statistical test then examines if the observed data contradict $H_0$. Rejecting $H_0$ suggests believing in $H_A$, which claims an effect exists. However, failing to reject $H_0$ only indicates the data does not provide sufficient evidence against it, not that $H_0$ is definitely true.

Consider a study on stearic acid's effect on fat digestibility in animals. Here, the null hypothesis $H_0: \beta = 0$ would assert no relationship between stearic acid and digestibility, tested against $H_A: \beta \neq 0$. If evidence suggests digestibility is 75\% for a 20\% stearic acid level, we might test $H_0: \alpha + \beta \cdot 20 = 75$ against $H_A: \alpha + \beta \cdot 20 \neq 75$.

Similarly, in a study investigating the association between the length of gestation and lifespan in horses, the hypothesis $H_0: \beta = 0$ indicates no relationship, with a possible one-sided alternative $H_A: \beta > 0$ if it's hypothesized that longer gestation leads to longer life.

Finally, in studies like antibiotic effects on dung decomposition, $H_0$ might state no difference between treatment averages, against the alternative that at least two treatments differ. Importantly, the choice of hypothesis significantly affects the interpretation of data and results, underscoring the need for careful hypothesis formulation in scientific research.

\subsection{$t$-tests}

We explore hypothesis testing within a linear model:

\[
y_{i} = \mu_{i} + e_{i}, \quad i = 1, \ldots, n,
\]

where $\mu_{i}$ are modeled through parameters $\theta_{1}, \ldots, \theta_{p}$ and explanatory variables, and $e_{i}$ are independent and identically distributed following a normal distribution $N(0, \sigma^2)$. We focus on testing hypotheses about a specific parameter $\theta_j$ in the model, formulated as:

\[
H_0: \theta_{j} = \theta_{0},
\]

for some fixed value $\theta_0$. This form of hypothesis testing can be applied broadly, including cases where $\theta_j$ represents a linear combination of other parameters.

Data supporting $H_0$ yield estimates $\hat{\theta}_j$ close to $\theta_0$. Conversely, large deviations suggest contradictions. We employ a t-test, leveraging the statistic:

\[
T_{\text{obs}} = \frac{\hat{\theta}_{j} - \theta_{0}}{\text{SE}(\hat{\theta}_{j})},
\]

which follows a t-distribution with $n-p$ degrees of freedom under $H_0$. The extremeness of $T_{\text{obs}}$ informs us about the alignment between our data and $H_0$.

The p-value, depending on the nature of the alternative hypothesis ($H_A$), guides us in determining the statistical significance of the observed deviations:

- For a two-sided alternative ($H_A: \theta_{j} \neq \theta_{0}$), both large positive and negative values of $T_{\text{obs}}$ are critical:

  \[
  p\text{-value} = P(|T| \geq |T_{\text{obs}}|) = 2 \cdot P(T \geq |T_{\text{obs}}|),
  \]

- For a one-sided alternative ($H_A: \theta_{j} > \theta_{0}$), large values of $T_{\text{obs}}$ are critical, leading to:

  \[
  p\text{-value} = P(T \geq T_{\text{obs}}).
  \]

- Similarly, for $H_A: \theta_{j} < \theta_{0}$, only small values of $T_{\text{obs}}$ matter:

  \[
  p\text{-value} = P(T \leq T_{\text{obs}}).
  \]

A hypothesis is rejected if the p-value is less than the chosen significance level $\alpha$ (typically 0.05, 0.01, or 0.10).

For two-sided hypotheses, $H_0$ is rejected at the 5\% significance level if $|T_{\text{obs}}|$ exceeds the 97.5th percentile of the $t_{n-p}$ distribution. For one-sided tests, the critical region changes accordingly.

Furthermore, a connection exists between t-tests and confidence intervals: $H_0$ is rejected if the hypothesized value $\theta_0$ does not fall within the constructed confidence interval for $\theta_j$. This relationship underscores how both t-tests and confidence intervals provide complementary information about parameter estimates within linear models.

Practical examples, like evaluating mean differences between groups or the association between variables in regression contexts, demonstrate the application of these principles, aiding in decisions about hypotheses concerning biological, industrial, or environmental processes. By comparing observed test statistics against theoretical distributions, researchers can infer whether the observed data contradict their hypotheses about the population parameters, guiding scientific and decision-making processes.

\subsection{Tests in a One-Way ANOVA}

The one-way ANOVA method is applied to determine if there are statistically significant differences among several group means. This is particularly crucial when dealing with more than two groups, making the simple $t$-test inadequate.

\subsubsection{The F-test for Comparison of Groups}

In the context of one-way ANOVA, observations are modeled as:

\[
y_{i} = \alpha_{g(i)} + e_{i}, \quad i = 1, \ldots, n,
\]

where $g(i)$ identifies the group belonging of the $i$-th observation, and $e_i$ are independently and normally distributed with mean 0 and variance $\sigma^2$. The hypothesis aims to assess the equality of group means, formulated as:

\[
H_0: \alpha_{1} = \alpha_{2} = \cdots = \alpha_{k},
\]

with the alternative hypothesis suggesting at least one pair of group means is different.

In analyzing group differences, we segregate total variation into within-group variation ($\text{SS}_{e}$) and between-group variation ($\text{SS}_{\text{grp}}$). The within-group or residual sum of squares is:

\[
\text{SS}_{e} = \sum_{i=1}^{n} (y_i - \bar{y}_{g(i)})^2,
\]

capturing the variability of observations within each group. Here, $\bar{y}_{g(i)}$ is the mean of the group to which observation $i$ belongs. The corresponding mean square error is calculated by dividing $\text{SS}_{e}$ by the residual degrees of freedom ($n - k$):

\[
\text{MS}_{e} = \frac{\text{SS}_{e}}{n - k}.
\]

For between-group variation, which reflects differences among group means, we use:

\[
\text{SS}_{\text{grp}} = \sum_{j=1}^{k} n_j (\bar{y}_j - \bar{y})^2,
\]

where $n_j$ is the number of observations in the $j$-th group, $\bar{y}_j$ is the mean of the $j$-th group, and $\bar{y}$ is the overall mean of all observations. This sum of squares is divided by the degrees of freedom for group ($k - 1$) to obtain:

\[
\text{MS}_{\text{grp}} = \frac{\text{SS}_{\text{grp}}}{k - 1}.
\]

The F-test evaluates the ratio of these mean squares:

\[
F_{\text{obs}} = \frac{\text{MS}_{\text{grp}}}{\text{MS}_{e}}.
\]

Large values of $F_{\text{obs}}$ suggest significant differences among group means, challenging the null hypothesis of equal means.

For practical application, consider an analysis to test if different antibiotics lead to varying rates of dung decomposition. The computation involves determining $\text{SS}_{e}$, $\text{SS}_{\text{grp}}$, and subsequently $\text{MS}_{e}$ and $\text{MS}_{\text{grp}}$. These calculations help to construct the $F_{\text{obs}}$ statistic, which is then compared against the critical value from the F distribution with $(k - 1, n - k)$ degrees of freedom. A statistically significant $F_{\text{obs}}$ (typically when the p-value is less than 0.05) leads to the rejection of the null hypothesis, implying that the antibiotic types have different effects on dung decomposition rates.

Hence, the F-test in one-way ANOVA offers a systematic approach to determine if there are significant differences among the means of multiple groups, guiding researchers in understanding the impact of different treatments or conditions.

\subsubsection{Pairwise Comparisons and LSD-values}

In cases where the interest is in specific group comparisons within an ANOVA framework, pairwise comparisons are often conducted. While these analyses may focus on just two groups at a time, they leverage the full dataset to enhance the precision of the standard deviation estimate, effectively "borrowing information" from all groups.

The hypothesis for comparing two specific groups, $j$ and $l$, is expressed as:

\[
H_{0}: \alpha_{j} = \alpha_{l} \quad \text{or equivalently} \quad H_{0}: \alpha_{j} - \alpha_{l} = 0,
\]

with the two-sided alternative $H_{A}: \alpha_{j} - \alpha_{l} \neq 0$. The comparison employs a t-test given by:

\[
T_{\text{obs}} = \frac{\hat{\alpha}_{j} - \hat{\alpha}_{l}}{\mathrm{SE}(\hat{\alpha}_{j} - \hat{\alpha}_{l})} = \frac{\hat{\alpha}_{j} - \hat{\alpha}_{l}}{s \sqrt{\left(\frac{1}{n_{j}} + \frac{1}{n_{l}}\right)}},
\]

where $\hat{\alpha}_{j}$ and $\hat{\alpha}_{l}$ are the sample means of the two groups, $s$ is the pooled standard deviation, and $n_j$ and $n_l$ are the sample sizes of the two groups. This test statistic follows the $t_{n-k}$ distribution.

The significant difference is declared if the absolute observed value $|T_{\text{obs}}|$ exceeds the critical value $t_{0.975, n-k}$, correlating to the 95\% confidence level. Mathematically, this condition is:

\[
|\hat{\alpha}_{j} - \hat{\alpha}_{l}| \geq t_{0.975, n-k} \cdot s \sqrt{\left(\frac{1}{n_{j}} + \frac{1}{n_{l}}\right)}.
\]

The threshold on the right is known as the least significant difference (LSD), specifically, the 95\% LSD-value for the difference between $\alpha_j$ and $\alpha_l$. When all groups have the same number of observations, denoted $n'$, the LSD simplifies to:

\[
\text{LSD}_{0.95} = t_{0.975, n-k} \cdot s \sqrt{\frac{2}{n'}}.
\]

This uniform LSD-value allows for straightforward comparisons between any two group means.

Consider an example where the effects of different antibiotics on dung decomposition are examined. With means computed for each antibiotic group and the pooled standard deviation established, pairwise comparisons can be systematically conducted. For instance, if the LSD-value calculated is 0.144 for groups with six observations each, and the observed mean difference between any two specific groups exceeds this value, the groups are considered significantly different at the 95\% confidence level.

However, when conducting multiple pairwise comparisons, one must be cautious due to the risk of the multiple testing problem, which can inflate the overall type I error rate.

In practice, the analysis might reveal significant differences among specific antibiotic types regarding their effect on dung decomposition. This can lead to actionable insights in agricultural or pharmaceutical contexts. Yet, researchers must apply these analyses judiciously, acknowledging the potential for increased false positive rates due to multiple comparisons and employing techniques such as the LSD-value cautiously to mitigate these risks.

\subsection{Hypothesis Tests as Comparison of Nested Models}

Hypothesis testing can be viewed as a comparison between nested models. In the one-way ANOVA context, this perspective involves evaluating the fit between a full model, which allows group means to differ, and a null model, which assumes all group means are equal.

The $F$-statistic, used to compare group effects, is derived as:

\[
F_{\mathrm{obs}} = \frac{\text{MS}_{\text{grp}}}{\text{MS}_{e}} = \frac{\text{SS}_{\text{grp}} / (k - 1)}{\text{SS}_{e} / (n - k)}.
\]

This ratio reflects the relative magnitude of between-group variation to within-group variation, effectively comparing model fits under varying hypotheses. A low $F_{\mathrm{obs}}$ value suggests the null model (equal means) is adequate, while a high value indicates the full model provides a significantly better fit, thus rejecting the null hypothesis.

The residual sum of squares under the null hypothesis ($\text{SS}_{\text{total}}$) always exceeds or equals that under the full model ($\text{SS}_{e}$). The difference, $\text{SS}_{\text{total}} - \text{SS}_{e}$, quantifies the improvement in fit provided by allowing group means to differ. Large values of this difference suggest significant between-group variability, leading to rejection of the null hypothesis. When standardized by the respective degrees of freedom, this difference yields the $F$-statistic:

\[
F_{\mathrm{obs}} = \frac{(\text{SS}_{\text{total}} - \text{SS}_{e}) / (k - 1)}{\text{SS}_{e} / (n - k)}.
\]

In general linear models, we extend this logic by comparing a full model (without hypothesis-imposed restrictions) against a null model (with restrictions). This involves calculating the residual sum of squares for both models ($\text{SS}_{\text{full}}$ and $\text{SS}_{0}$) and their respective degrees of freedom ($\text{df}_{\text{full}}$ and $\text{df}_{0}$). The $F$-statistic is then:

\[
F_{\mathrm{obs}} = \frac{(\text{SS}_{0} - \text{SS}_{\text{full}}) / (\text{df}_{0} - \text{df}_{\text{full}})}{\text{SS}_{\text{full}} / \text{df}_{\text{full}}}.
\]

This statistic follows an $F$-distribution under the null hypothesis, allowing calculation of a p-value for hypothesis testing. The degrees of freedom reflect the additional parameters estimated in the full model compared to the null model. The $F_{\mathrm{obs}}$ value is considered large if it results in a small p-value, indicating the full model provides a significantly better explanation of the data compared to the null model.

As demonstrated with stearic acid's effect on fat digestibility, testing hypotheses about single parameters can be conducted using either $t$-tests or $F$-tests. In these tests, the $F$-statistic equates to the square of the $t$-statistic, thus providing identical p-values and conclusions. This connection highlights the consistency and versatility of linear model-based hypothesis testing, whether comparing group means in ANOVA or evaluating parameter significance in regression models.
\subsection{Hypothesis Tests as Comparison of Nested Models}

Hypothesis testing, particularly using the $F$-statistic in a one-way ANOVA, compares between-group and within-group variations to evaluate model fits. The $F$-statistic, defined as

\begin{equation*}
F_{\mathrm{obs}} = \frac{\mathrm{MS}_{\mathrm{grp}}}{\mathrm{MS}_{e}} = \frac{\mathrm{SS}_{\mathrm{grp}} /(k-1)}{\mathrm{SS}_{e} /(n-k)},
\end{equation*}

facilitates comparing the residual sums of squares ($\mathrm{SS}_{e}$) under two models: the full model allowing different group means and the null model assuming a common mean across groups. The difference $\mathrm{SS}_{\text{total}} - \mathrm{SS}_{e}$, representing the improvement in model fit from the null model to the full model, when standardized by degrees of freedom, yields the $F_{\mathrm{obs}}$.

This approach extends to all linear models, comparing a full model against a null model restricted by the hypothesis. The comparison is quantified by the $F$-test statistic

\begin{equation*}
F_{\mathrm{obs}} = \frac{(\mathrm{SS}_{0} - \mathrm{SS}_{\mathrm{full}}) /(\mathrm{df}_{0} - \mathrm{df}_{\mathrm{full}})}{\mathrm{SS}_{\mathrm{full}} / \mathrm{df}_{\mathrm{full}}},
\end{equation*}

where $\mathrm{SS}_{\text{full}} \leq \mathrm{SS}_{0}$ reflects the reduction in the residual sum of squares due to the hypothesis, adjusted for model flexibility and complexity through degrees of freedom.

Example analyses illustrate that $F_{\text{obs}}$ for testing a hypothesis about a single parameter in linear regression models yields the same $p$-value as a $t$-test, demonstrating their equivalence. This alignment confirms that the $F$-test can effectively compare nested models, evaluating the impact of hypotheses on model fit.

\subsection{Type I and Type II Errors}

In hypothesis testing, Type I (false positive) and Type II (false negative) errors reflect incorrect rejections of true hypotheses and failures to reject false hypotheses, respectively. The significance level ($\alpha$) controls the probability of Type I errors, with a common choice being $0.05$. Adjusting $\alpha$ impacts the balance between Type I and Type II errors, highlighting a trade-off between detecting true effects and avoiding false alarms.

Multiple testing increases the risk of Type I errors, demonstrated through the concept of Bonferroni correction in pairwise comparisons within ANOVA. This correction, by adjusting $p$-values for the number of tests, addresses the heightened risk of false positives but increases the chance of Type II errors, underscoring the complexity of drawing conclusions from multiple tests. Strategies to mitigate these risks include conducting overall group tests, prioritizing comparisons based on prior knowledge, and transparently reporting $p$-values.

\subsection*{General Procedure for Hypothesis Tests}

The general procedure for conducting hypothesis tests involves:

\begin{enumerate}
    \item Choosing a significance level ($\alpha$), typically $0.05$.
    \item Defining the null and alternative hypotheses to address the scientific question.
    \item Selecting an appropriate test statistic, determining critical values, and identifying the statistic's distribution under the null hypothesis.
    \item Computing the $p$-value to decide on rejecting or not rejecting the hypothesis based on $\alpha$.
    \item Translating the test's outcome to a conclusion relevant to the original question,including quantification of significant effects if applicable.
\end{enumerate}

This framework underscores the systematic approach required to effectively test hypotheses, encompassing selection of significance levels, hypothesis formulation, test statistic determination, $p$-value computation, and interpretation of results.

\section{Prediction and Confidence Intervals}

Confidence intervals are statistical ranges, typically based on sample data, which are likely to contain an unknown population parameter; conversely, prediction intervals provide a range within which we expect a new observation to fall. While confidence intervals are focused on the uncertainty of the mean predicted values, prediction intervals encompass the variability expected in future individual observations.

\subsection{Formulation of Intervals}
The prediction interval in the context of linear regression is formulated as:
\[
\hat{\alpha} + \hat{\beta} \cdot x_0 \pm t_{0.975, n-2} \cdot s \cdot \sqrt{1 + \frac{1}{n} + \frac{(x_0 - \bar{x})^2}{SS_x}},
\]
where $\hat{\alpha}$ and $\hat{\beta}$ are estimated coefficients from the regression, $x_0$ is the point at which the prediction is being made, $n$ is the sample size, $s$ is the standard deviation of errors, and $SS_x$ is the sum of squares of the explanatory variable. This interval combines the uncertainty in the estimate of the mean response (confidence interval) with the variability of individual observations around that mean.

The confidence interval for the mean response at $x_0$ in linear regression is narrower and is given by:
\[
\hat{\alpha} + \hat{\beta} \cdot x_0 \pm t_{0.975, n-2} \cdot s \cdot \sqrt{\frac{1}{n} + \frac{(x_0 - \bar{x})^2}{SS_x}}.
\]
It differs from the prediction interval by excluding the term $1$ inside the square root, reflecting that it addresses the uncertainty of the mean rather than individual observations.

The essential difference between prediction and confidence intervals lies in their width: prediction intervals are inherently wider due to the additional variability of individual outcomes, while confidence intervals pertain strictly to the uncertainty of estimating the mean. Importantly, while increasing the sample size reduces the width of confidence intervals, prediction intervals retain a component of irreducible variability, reflected in their formula.

\subsection{Practical Applications}
Prediction intervals are essential in forecasting, allowing for an appreciation of the uncertainty surrounding individual predictions, such as in weather forecasting, stock price movements, or patient treatment responses. Confidence intervals, meanwhile, help understand the precision of sample-based estimates of population parameters.

\section{Statistical Model Validation}

Model validation in statistics ensures that the assumptions which underpin statistical models, particularly linear regression and ANOVA, are sound. This is critical as these assumptions impact the model's conclusions.

\subsection{Linear Regression and ANOVA}
Linear regression analyses the linear relationship between variables, encapsulated by the T-statistic formula:

\[
T = \frac{\hat{\beta} - \beta_0}{s/\sqrt{SS_x}}
\]

Here, $\hat{\beta}$ is the estimated regression coefficient, $\beta_0$ is the hypothesized value under the null hypothesis, $s$ denotes the standard error of the estimate, and $SS_x$ is the sum of squares attributable to the independent variable. This T-statistic follows a t-distribution with $n-2$ degrees of freedom, which is foundational for hypothesis testing in regression.

For ANOVA, the significance of differences among group means is evaluated using an F-statistic ($F_{obs}$), which follows an F-distribution under the null hypothesis.

\[
F_{\text{obs}} = \frac{\text{MS}_{\text{between}}}{\text{MS}_{\text{within}}}
\]

where:
\begin{itemize}
    \item $\text{MS}_{\text{between}}$ (Mean Square Between Groups) is the variance estimate based on the group means and reflects between-group variability.
    \item $\text{MS}_{\text{within}}$ (Mean Square Within Groups) is an estimate of the population variance based on the within-group variability.
\end{itemize}

In mathematical terms, these components are calculated as follows:

\[
\text{MS}_{\text{between}} = \frac{\sum_{i=1}^{k} n_i (\bar{y}_i - \bar{y})^2}{k - 1}
\]

\[
\text{MS}_{\text{within}} = \frac{\sum_{i=1}^{k} \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_i)^2}{N - k}
\]

Here, $n_i$ is the number of observations in group $i$, $\bar{y}_i$ is the mean of group $i$, $\bar{y}$ is the overall mean, $y_{ij}$ is the $j$th observation from the $i$th group, $k$ is the number of groups, and $N$ is the total number of observations.

The resulting $F_{\text{obs}}$ statistic follows an F-distribution with $(k - 1)$ degrees of freedom for the numerator and $(N - k)$ degrees of freedom for the denominator under the null hypothesis. If the calculated F-value exceeds the critical value from the F-distribution for a given significance level (commonly 0.05), the null hypothesis of equal means is rejected, suggesting that there is a statistically significant difference among the group means.

ANOVA assumptions include:
\begin{itemize}
    \item Independence of observations.
    \item Normal distribution of residuals within each group.
    \item Homogeneity of variances across groups (homoscedasticity).
\end{itemize}

\subsection{Linear Model Assumptions}
Consider a linear model expressed as $y_i = \mu_i + e_i$. The error terms $e_i$ are assumed to satisfy the following conditions for all observations $i = 1, ... , n$:

\begin{enumerate}
    \item Expected value of zero: $E[e_i] = 0$.
    \item Constant standard deviation (homoscedasticity): Var($e_i$) = $\sigma^2$.
    \item Normal distribution: $e_i \sim N(0, \sigma^2)$.
    \item Independence: Cov($e_i, e_j$) = 0 for $i \neq j$.
\end{enumerate}

Violating these assumptions can result in inaccurate statistical inferences.

\subsection{Verifying Assumptions: Normality and Homoscedasticity}
To validate model assumptions, we examine the residuals, $r_i = y_i - \hat{y}_i$, where $\hat{y}_i$ are the predicted values from the model. The normality assumption can be checked using Quantile-Quantile (Q-Q) plots, where deviations from a straight line indicate departure from normality. Homoscedasticity is assessed through plots of residuals against predicted values or independent variables; patterns such as funnels suggest heteroscedasticity.

\subsection{Diagnostics for Residuals}
Investigation of residuals' behavior is central to validating model assumptions. Specifically, residual plots should not exhibit discernible patterns that would indicate systemic errors in the model such as non-linear relationships, heteroscedasticity, or autocorrelation.

\subsection{Methodology}
The approach to performing a detailed statistical analysis encompasses:

\subsubsection*{Step 1: Data Preparation and Exploration}
Before any statistical analysis, it is crucial to understand the dataset's structure. This involves reviewing the data to identify any potential issues such as missing values, outliers, or incorrect data formats. Preparation typically involves:
\begin{itemize}
    \item Cleaning the data by addressing missing values and errors.
    \item Summarizing the data with descriptive statistics to capture its central tendency, dispersion, and shape.
    \item Visualizing the data with plots to understand its distribution and identify potential outliers.
\end{itemize}

\subsubsection*{Step 2: Summary Statistics and Distribution Analysis}
After preparing the data, perform a thorough analysis by calculating summary statistics such as mean, median, and standard deviation. Additionally, assess the distribution of your data using histograms or density plots to check for normality, skewness, or kurtosis, which will inform the appropriateness of statistical tests or models.

\subsubsection*{Step 3: Correlation Analysis}
Identify and quantify potential linear relationships between variables using the Pearson correlation coefficient. Visual tools like scatter plots or heatmaps can help visualize these correlations and identify potential nonlinear relationships or clusters.

\subsubsection*{Step 4: Regression Analysis}
Apply regression analysis to model the relationship between an independent variable and a dependent variable. This includes:
\begin{itemize}
    \item Fitting a simple linear regression model to understand the linear relationship between variables.
    \item Assessing the impact of outliers or influential points on the model using diagnostics such as leverage scores and Cook's distance.
    \item Validating the model assumptions such as linearity, normality of residuals, and homoscedasticity through residual analysis.
\end{itemize}

\subsubsection*{Step 5: Model Evaluation and Validation}
Evaluate the fit of the regression model using goodness-of-fit metrics like R-squared and adjusted R-squared. These metrics will help you understand how well your model explains the variability of the response data. Additionally, conduct a thorough residual analysis to ensure that there are no patterns indicating issues with the model fit, such as non-linearity or autocorrelation.

\subsubsection*{Step 6: Outlier Analysis and Model Refinement}
Identify and address outliers and influential data points. This step involves:
\begin{itemize}
    \item Conducting a sensitivity analysis to understand how the removal or adjustment of these points affects the model.
    \item Deciding whether to exclude outliers based on statistical criteria and domain knowledge.
    \item Refining the model and reevaluating its performance after adjustments.
\end{itemize}

\subsubsection*{Step 7: Further Analysis}
If necessary, extend your analysis to accommodate more complex relationships:
\begin{itemize}
    \item Use multiple regression if there are additional relevant variables.
    \item Explore polynomial regression or transformation of variables if preliminary analysis suggests a nonlinear relationship.
\end{itemize}

\subsubsection*{Step 8: Interpretation and Reporting}
Finally, interpret the results of your statistical analysis within the context of your study. This includes:
\begin{itemize}
    \item Explaining the meaning and practical significance of regression coefficients.
    \item Discussing any limitations of your analysis and the implications of the assumptions made during the process.
    \item Presenting your findings in a clear, understandable manner, supported by visualizations and summary statistics.
\end{itemize}
This process is iterative, allowing for refinement and deeper insights at each step based on the outcomes of the preceding steps, leading to a robust and insightful statistical analysis.

\section{Linear normal models}

Experiments often include multiple variables affecting the response, requiring their inclusion in the statistical model. Previously, models like linear regression and one-way analysis of variance have addressed single explanatory variables—quantitative and categorical, respectively. Both models are types of linear models, assuming normally distributed residuals. This section expands on these models and introduces a general framework for linear models.

\subsection{Multiple linear regression}

Multiple linear regression generalizes simple linear regression to include multiple continuous explanatory variables, modeling the relationship as:
\[
y_i = \alpha + \beta_1 x_{i1} + \cdots + \beta_d x_{id} + e_i, \quad i = 1, \ldots, n
\]
with $e_i \sim N(0, \sigma^2)$, representing the residuals. The $\beta_j$ parameters quantify the impact of each variable, assuming other variables are constant.

The estimates for $\alpha, \beta_1, \ldots, \beta_d$ are obtained using least squares. This model can accommodate hypotheses testing for individual parameters, typically testing if they are zero:
\[
T_{\text{obs}} = \frac{\hat{\beta}_j - 0}{\operatorname{SE}(\hat{\beta}_j)},
\]
following a t-distribution with $n-d-1$ degrees of freedom. Calculations involving these parameters often require software due to the complexity of matrix operations involved.

As an example, data collected from 31 cherry trees includes tree diameter and height, modeled to predict tree volume without harvesting:
\[
v_i = \alpha + \beta_1 h_i + \beta_2 d_i + e_i, \quad i = 1, \ldots, 31
\]
with $e_i \sim N(0, \sigma^2)$. An exploratory analysis reveals potential heterogeneity in residuals, suggesting transformations or different models might be necessary.



\textbf{Statistical inference}: Hypotheses such as $H_0: \beta_j = 0$ test the contribution of variables. Multicollinearity among variables can affect the reliability of parameter estimates, a common challenge in multiple regression.

\textbf{Extension to other models}: Including polynomial terms allows for modeling non-linear relationships, such as quadratic regression:
\[
y_i = \alpha + \beta_1 x_i + \beta_2 x_i^2 + e_i, \quad i = 1, \ldots, n.
\]
This model can test for curvature in the data relationship, improving fit over a simple linear model.


\subsection{Additive two-way analysis of variance}
Previously, we expanded the one-way ANOVA to include multiple explanatory variables in a manner analogous to extending simple linear regression. Here, we focus on the two-way ANOVA, applicable to cases with two categorical explanatory variables, which are a subset of the general multi-way ANOVA framework.

For the two-way ANOVA, observation $i$ is modeled as:
\begin{equation*}
y_{i}=\alpha_{g(i)}+\beta_{h(i)}+e_{i}, \quad i=1, \ldots, n
\end{equation*}
where $e_i \sim N(0, \sigma^2)$ are independent residuals, and $g(i)$ and $h(i)$ map $i$ to the levels of two categorical variables.

Parameter estimation employs least squares, deriving variance from:
\begin{equation*}
s^{2} =\frac{\mathrm{SS}_{e}}{\mathrm{df}_{e}}=\frac{1}{n-\left(k_{1}+k_{2}-1\right)} \sum_{i}\left(y_{i}-\hat{y}_{i}\right)^{2}
\end{equation*}
This formula incorporates the effective number of parameters, $k_{1}+k_{2}-1$, due to one redundant parameter among $k_{1}+k_{2}$. This parameter count arises because one cell serves as a reference, leaving $k_{1}-1$ and $k_{2}-1$ contrast parameters for the two variables, respectively.

Furthermore, in the two-way ANOVA, each observation fits precisely into a cell defined by the intersections of categories from each variable, simplifying the prediction of responses at each category combination.

Testing for differences among categories (e.g., no variation across categories of a variable) follows the logic of one-way ANOVA but includes both variables in the model. The mean squared deviations for each explanatory variable against the residual mean square lead to $F$-statistics compared to appropriate $F$-distributions.

\subsubsection{The additive multi-way analysis of variance}
Consider a scenario with $d$ categorical variables $x_{1}, \ldots, x_{d}$ influencing an observed response $y$ in a dataset. Each categorical variable $x_j$ may have a different number of categories $k_j$. The model for the multi-way analysis of variance is expressed as:

\begin{equation*}
y_{i} = \alpha_{g(i)} + \beta_{h(i)} + \cdots + \gamma_{l(i)} + e_{i}, \quad i = 1, \ldots, n
\end{equation*}

where $e_i \sim N(0, \sigma^2)$ represents the residuals assumed to be independent and normally distributed. Functions $g, \ldots, l$ map each observation to a specific combination of category levels. The statistical methodologies, including estimation, hypothesis testing, and contrast analysis used in the two-way analysis can be seamlessly extended to this more general framework.


\subsubsection{Analysis of variance as linear regression}
The general analysis of variance model described as
\begin{equation*}
y_{i}=\alpha_{g(i)}+\beta_{h(i)}+\cdots+\gamma_{l(i)}+e_{i}, \quad i=1, \ldots, n
\end{equation*}
can be expressed using dummy variables for its categorical explanatory variables. Each variable, such as one with $k_1$ categories, is represented by $k_1$ dummy variables $x_{i1}^1, \ldots, x_{ik_1}^1$, defined as:
\[
x_{ij}^1= 
\begin{cases}
1 & \text{if observation } i \text{ belongs to category } j\\
0 & \text{otherwise}
\end{cases}
\]
This allows the model to be rewritten in a linear regression format:
\begin{equation*}
y_{i} = \alpha_1 x_{i1}^1 + \alpha_2 x_{i2}^1 + \cdots + \alpha_{k_1} x_{ik_1}^1 + \beta_1 x_{i1}^2 + \cdots + \gamma_{k_d} x_{ik_d}^d + e_{i}
\end{equation*}
Here, $x$'s are indicator variables representing categorical levels in $d$ variables, each with $k_1, k_2, \ldots, k_d$ categories respectively.

An example with a pork quality dataset utilizes two variables, pig and day, with 10 and 3 categories respectively. Dummy variables $x_{ij}^{\text{pig}}$ and $x_{ij}^{\text{day}}$ define membership in pig and day categories. For instance, $x_{ij}^{\text{pig}} = 1$ if observation $i$ is from pig $j$, and $x_{ij}^{\text{day}} = 1$ if observation $i$ is from day $j$, otherwise 0.

The model for pork quality is then:
\begin{align*}
y_{i} & = \alpha_1 x_{i1}^{\text{pig}} + \cdots + \alpha_{10} x_{i10}^{\text{pig}} + \beta_1 x_{i1}^{\text{day}} + \beta_4 x_{i4}^{\text{day}} + \beta_6 x_{i6}^{\text{day}} + e_{i} 
\end{align*}

This representation shows that ANOVA models can be understood as linear regression models by reformulating categorical variables into dummy variables. The term linear model arises because the parameters ($\alpha, \beta, \gamma$) relate linearly to the response variable, even though the predictors are categorical. The generalization of this approach allows for complex models that integrate both continuous and categorical data, structured as linear combinations of parameters and functions of predictor variables.

\subsection{Linear models}
Linear models represent observations $y_i$ as combinations of predictor variables and an error term, formalized as:
\begin{equation*}
y_{i}=\sum_{j=1}^{d} \beta_{j} \cdot x_{i j}+e_{i}, \quad i=1, \ldots, n
\end{equation*}
where $e_i \sim N(0, \sigma^2)$ are iid error terms. This structure resembles regression models, but it also integrates categorical variables by using dummy variables as demonstrated earlier, allowing for a blend of categorical and quantitative variables.

For models with numerous explanatory variables, including those with many categories, simplifications include representing continuous variables by their regression coefficients $\beta_j \cdot x_{ij}$ and categorical variables by parameters such as $\alpha_{g(i)}$ and $\gamma_{h(i)}$ that denote category-specific intercepts. Thus, a model combining both types of variables could be expressed as:
\begin{equation*}
y_{i} = \alpha_{g(i)} + \gamma_{h(i)} + \beta_1 \cdot x_{i1} + \beta_2 \cdot x_{i2} + e_i 
\end{equation*}
This form separates the effects of categorical groupings and continuous predictors more clearly and is easier to interpret than a single summation of predictors and parameters.

Moreover, statistical model formulas offer a simplified standard for expressing models, improving clarity and interpretability, particularly when dealing with models that incorporate a mix of variable types. These formulas enable researchers to succinctly describe complex interactions between predictors within a coherent framework of linear assumptions.

\subsubsection{Model formulas}
Model formulas provide a streamlined approach to express relationships in statistical models, particularly when integrating multiple explanatory variables. Their utility becomes evident with complex models. For instance, the relationship between tree volume and its dimensions was modeled as:
\[
\verb|volume = diameter + height|
\]
while pork color influenced by storage conditions was represented by:
\[
\verb|colour score = day + pig|
\]

In these formulas, the left-hand side specifies the response variable, with explanatory variables on the right-hand side. This format, however, does not distinguish between the nature of variables (categorical vs. continuous) or the number of parameters each variable contributes; such details must be inferred from the context or additional information.

Continuous variables typically contribute a single parameter each, reflecting their direct linear effect on the response. In contrast, categorical variables like "day" and "pig" introduce multiple parameters (three for day, ten for pig), corresponding to the number of categories minus one, each representing a deviation from a reference category.

Model formulas simplify the representation of the relationships being modeled, enhancing interpretability. A fundamental aspect of model formulas is the implicit inclusion of an intercept, which represents the grand mean across the data. In models involving categorical variables, the intercept serves as the reference level. If a model needs to exclude the intercept, indicating a scenario where the regression line should pass through the origin, it must be explicitly stated by appending "-1" to the model formula:
\[
\verb|volume = diameter + height -1|
\]
This notation explicitly eliminates the intercept, fitting the model directly through the origin, adjusting the interpretation and application of the regression parameters accordingly.

\subsubsection{Estimation and parameterization}
Mean parameters in linear models are estimated using least squares, a method applicable to models with multiple categorical variables, despite potential overparameterization issues. Overparameterization occurs when the number of parameters exceeds the capacity for estimation based on available data.

We revisit the parameterization of one-way ANOVA models to illustrate approaches to handling multiple group comparisons. Specifically, parameterization can either directly represent each group's mean or utilize a reference group mean along with differences from this mean for other groups. This dual approach allows flexible model adjustment by adding or subtracting parameters as needed.\footnote{Different software parameterize models variably, affecting intercept interpretation. R and SAS typically use a reference level for categorical variables, parameterizing other categories as contrasts relative to this level.}

Consider a two-variable categorical model:
\begin{equation*}
y_{i}=\alpha_{g(i)}+\beta_{h(i)}+e_{i}, \quad i=1, \ldots, n
\end{equation*}
An example might be an animal feeding experiment testing different supplements. If $\mu$ represents the average weight for a reference feeding strategy, and $\alpha$ and $\beta$ represent the average weight changes due to two different substances, the group means under various combinations of these substances are:
\begin{center}
\begin{tabular}{lcc}
\hline\hline
 & \multicolumn{2}{c}{Substance 1} \\
\cline { 2 - 3 }
Substance 2 & Not added & Added \\
\hline
Not added & $\mu$ & $\mu+\alpha$ \\
Added & $\mu+\beta$ & $\mu+\alpha+\beta$ \\
\hline
\end{tabular}
\end{center}
This design assumes independent substance effects, an assumption explored further in subsequent sections.

Statistical software handles overparameterization by defaulting one categorical level as the reference and treating others as contrasts, simplifying interpretation and analysis. It is critical, however, to accurately interpret these parameters: quantitative variable estimates represent partial regression slopes; categorical variable estimates indicate contrasts.

The residual variance is estimated as usual:
\begin{equation*}
s^{2}=\frac{\mathrm{SS}_{e}}{\mathrm{df}_{e}}=\frac{1}{n-p} \sum_{i}\left(y_{i}-\hat{y}_{i}\right)^{2}
\end{equation*}
where $p$ is the number of parameters actively estimated. Most statistical packages automatically compute $p$, accommodating complex model structures effectively.

\subsubsection{Hypothesis testing in linear models}
In linear models, hypothesis testing follows the methods previously described. Single parameter hypotheses, such as 
\[
H_{0}: \beta_{j}=0,
\]
are tested using a $t$-test with the statistic 
\[
T_{\text{obs}} = \frac{\hat{\beta}_{j}}{\operatorname{SE}(\hat{\beta}_{j})}
\]
compared to a $t$ distribution with $n-p$ degrees of freedom. For compound hypotheses involving multiple parameters, such as 
\[
H_{1}: \beta_{A} = \cdots = \beta_{D},
\]
an $F$-test is employed. This test compares a reduced model under $H_{1}$ to the full model using the statistic
\[
F_{\mathrm{obs}} = \frac{(\mathrm{SS}_{0} - \mathrm{SS}_{\text{full}}) / (\mathrm{df}_{0} - \mathrm{df}_{\mathrm{full}})}{\mathrm{SS}_{\text{full}} / \mathrm{df}_{\text{full}}}
\]
where $\mathrm{SS}_{0}$ and $\mathrm{SS}_{\text{full}}$ are the sums of squares from the reduced and full models, respectively.\footnote{The effective number of parameters is determined by the rank of the design matrix $\boldsymbol{X}$ in the matrix form of the model $\boldsymbol{Y} = \boldsymbol{X} \boldsymbol{\beta} + \boldsymbol{e}$.}

Models with multiple explanatory variables, like the two-way additive model 
\[
y = A + B,
\]
permit the testing of hypotheses such as 
\[
H_{0}: \alpha_{1} = \cdots = \alpha_{k_{1}}
\]
or 
\[
H_{1}: \beta_{1} = \cdots = \beta_{k_{2}}.
\]
If $H_{0}$ is not rejected, the model reduces to 
\[
y = B,
\]
removing the insignificant terms associated with $A$. Similarly, rejecting $H_{1}$ simplifies the model to 
\[
y = A.
\]
Each hypothesis test necessitates refitting the model to ensure the estimates are based on the correct model structure.

For example, reducing the model from $y = A + B$ to $y = B$ requires recalculating the residual standard error with $n - k_{1}$ degrees of freedom rather than $n - k_{1} - k_{2} + 1$. This recalibration is crucial as it provides a more accurate estimate based on fewer parameters.

\paragraph{Model reduction steps}
\begin{itemize}
  \item Refitting the model after removing explanatory variables is essential for accurate parameter estimation and conclusions.
  \item Sequentially removing variables and refitting ensures that each model reduction is properly evaluated.
  \item The process starts with a full model and systematically simplifies to focus on significant variables.
\end{itemize}

This structured approach to model reduction and hypothesis testing ensures that the conclusions drawn from statistical analysis are robust and supported by the data.

\subsection{Interactions between variables}
Additive models in analysis of variance assume that contrasts between levels of one variable do not vary with the levels of other variables. However, there are situations where it is plausible that the effect of one variable may be influenced by another variable, necessitating consideration of interaction terms.

In one-way ANOVA, accurate estimation of within-group variation requires multiple observations per level. Similarly, for models incorporating interactions, it is essential to have at least two observations per level of the interaction term to estimate the interaction effects accurately. Without sufficient data per interaction level, only an additive model can be appropriately fitted, limiting the ability to discern whether interactions between variables significantly influence the response.

\subsubsection{Interactions between categorical variables}
In modeling situations where the effect of one variable may depend on another, such as blood pressure differences by gender across various ages, an interaction term is crucial. The simple additive model,

\[
\verb|blood pressure = gender + age|,
\]

assumes uniform differences across age groups, which might not be realistic. Incorporating an interaction term allows for varying effects across groups, making the model more flexible and representative of the underlying dynamics.

\begin{equation*}
y_{i} = \alpha_{g(i)} + \beta_{h(i)} + \gamma_{g(i), h(i)} + e_{i}, \quad i = 1, \ldots, n 
\end{equation*}
This model introduces $\gamma_{g(i), h(i)}$, representing the interaction between levels $g(i)$ of gender and $h(i)$ of age, with $k_{1} \times k_{2}$ interaction parameters possible.

For the example of gender (2 categories) and age (4 categories), the model integrates 8 specific interaction terms, reflecting each gender-age combination. The inclusion of $\gamma$ parameters effectively absorbs the main effects due to the overparameterization, allowing the model to adapt to each specific combination without constraints.

Model formulas incorporate interactions with an asterisk,
\[
\verb|blood pressure = gender + age + gender*age|,
\]
indicating that the effect of gender is contingent on age, and vice versa. For successful model fitting, each interaction combination must have sufficient replication to estimate the interaction effects; otherwise, the model may only fit the observed values without capturing the underlying relationships effectively.

\subsubsection{Hypothesis tests}
For models involving interactions, the hypothesis test for interaction terms can be stated as
\[
H_{0}: \gamma_{11} = \gamma_{12} = \cdots = \gamma_{k_{1} k_{2}} = 0,
\]
implying no interaction effect, thus reverting the model to its additive form. The statistical test used is the $F$-test, calculated as:
\[
F_{\mathrm{obs}} = \frac{(\mathrm{SS}_{\text{no interaction}} - \mathrm{SS}_{\text{with interaction}}) / (\mathrm{df}_{\text{no interaction}} - \mathrm{df}_{\text{with interaction}})}{\mathrm{SS}_{\text{with interaction}} / \mathrm{df}_{\text{with interaction}}},
\]
following an $F$ distribution. This setup compares a null model (without interaction) to a full model (with interaction).

\paragraph{Hierarchical principle}

If an interaction term is included in a model, lower order interactions and main effects of the variables involved in the interaction must also be included. This ensures the integrity of the model's structure and the validity of the statistical tests.

For example, the model for blood pressure involving age and gender is:
\[
\verb|blood pressure = age + gender + age*gender|.
\]
Testing for the effect of age alone is not permissible while the interaction term is present, as this would contradict the inclusion of the interaction term, which asserts that the effect of age is conditional on gender.

\subsubsection{Interactions between categorical and quantitative variables}
Interactions between categorical and quantitative variables are modeled to determine if the effect of a quantitative variable differs across levels of a categorical variable. For instance, considering blood pressure affected by age (quantitative) and gender (categorical), the model can be represented as:
\[
\verb|blood pressure = age + gender + age * gender|
\]
This model implies two regression lines—one for each gender—with potentially different intercepts and slopes, thus allowing the effect of age on blood pressure to vary by gender.

When testing for interaction, the null hypothesis posits no difference in the slopes for the quantitative variable across the groups of the categorical variable:
\[
H_0: \alpha_{\text{men}} = \alpha_{\text{women}}
\]
Rejecting this hypothesis indicates differing effects of the quantitative variable across categories, suggesting a significant interaction. This model's setup allows for comprehensive analysis by describing two dependent behaviors within a single framework.

\section{Statistical inference in the binomial distribution}

For $n$ trials, each having outcomes 0 or 1, denote $Z_{i}$ as the outcome of the $i$-th trial, with 1 being success and 0 failure. The outcomes follow:
\[
\mathrm{P}(Z_{i} = 1) = p, \quad \mathrm{P}(Z_{i} = 0) = 1-p
\]
All trials share the same distribution and are mutually independent.

The sum of successes $Y = \sum_{i=1}^{n} Z_{i}$ in $n$ independent trials, follows a binomial distribution given by:
\[
\mathrm{P}(Y = y) = \binom{n}{y} p^{y} (1-p)^{n-y}, \quad y \in \{0,1, \ldots, n\}
\]

Expected value and variance for $Z_i$ are:
\[
\mathbb{E}(Z_{i}) = p, \quad \operatorname{Var}(Z_{i}) = p(1-p)
\]
For $Y$, these extend to:
\[
\mathbb{E}(Y) = np, \quad \operatorname{Var}(Y) = np(1-p)
\]
If $Y_1 \sim \operatorname{bin}(n_1, p)$ and $Y_2 \sim \operatorname{bin}(n_2, p)$, then $Y_1 + Y_2 \sim \operatorname{bin}(n_1 + n_2, p)$.

For large $n$, the binomial distribution approaches a Gaussian distribution, particularly when both $np$ and $n(1-p)$ are at least 5, making the approximation suitable.

\subsection{MLE for binomial proportion}

The likelihood function for a binomial distribution with sample size $n=20$ is represented in a graph, illustrating the MLE under the alternative hypothesis $H_1$:
\[
\hat{p} = \frac{y}{n}
\]
For $H_0$, the MLE of $p$ is simply $p_0$.

\paragraph{Distribution of MLE:}
\[
\hat{p} = \frac{Y}{n} \in \left\{0, \frac{1}{n}, \frac{2}{n}, \ldots, 1\right\}
\]
Probability of observing a specific $\hat{p}$:
\[
\mathrm{P}\left(\hat{p} = \frac{y}{n}\right) = \binom{n}{y} p^y (1-p)^{n-y}
\]
Thus, $n\hat{p} \sim \operatorname{Bin}(n, p)$, showing that the MLE $\hat{p}$ is the sample proportion.

\paragraph{Properties of MLE:}
\[
\mathbb{E}(\hat{p}) = p, \quad \operatorname{Var}(\hat{p}) = \frac{p(1-p)}{n}
\]
These properties confirm that the MLE is unbiased and consistent.

\subsection{Confidence intervals}

A confidence interval (CI) for a parameter estimates a range where the true parameter value is likely to be found, with a given level of confidence, typically 95\%. The formal definition for a $95\%$ CI, $(l(Y), r(Y))$, is that it satisfies:
\[
P_{p}(l(Y) < p < r(Y)) = 0.95
\]
This implies that repeating the experiment multiple times would yield a CI containing the true parameter $p$ 95\% of the time. A CI can be constructed by inverting a test for a simple hypothesis, including all parameter values that would not be rejected by the test.

\subsubsection{Wald Confidence Interval}
The Wald CI is based on the asymptotic normality of the maximum likelihood estimator (MLE), $\hat{p}$. This interval is calculated using the z-score bounds:
\[
z_{\alpha / 2} < \frac{\hat{p} - p}{\sqrt{\widehat{\operatorname{Var}}(\hat{p})}} < z_{1 - \alpha / 2}
\]
which translates to:
\[
\hat{p} \pm 1.96 \times \text{SE}(\hat{p})
\]
where SE is the standard error of $\hat{p}$. However, this method has limitations:
\begin{itemize}
  \item It can produce values outside the interval $[0, 1]$.
  \item It fails to provide an interval when $\hat{p} = 0$ or $\hat{p} = 1$.
\end{itemize}

\subsubsection{True Coverage of the CI}
The actual coverage probability of a CI can be verified by summing binomial probabilities for different $Y$ values, each providing a CI. True coverage is assessed by:
\begin{enumerate}
  \item Calculating the CI for each observed $Y$.
  \item Summing the probabilities of $Y$ values that yield a CI containing the true $p$.
\end{enumerate}
Due to the discrete nature of data, achieving exactly 95\% coverage is challenging, often resulting in conservative intervals.

\subsubsection{Agresti-Coull Confidence Interval}
The Agresti-Coull CI modifies the Wald CI by adjusting $\hat{p}$ to $\tilde{p}$:
\[
\tilde{p} = \frac{y + \frac{z_{1-\alpha/2}^2}{2}}{n + z_{1-\alpha/2}^2}
\]
This adjustment adds a pseudo-count to both success and failure categories, stabilizing variance estimates. The modified interval is:
\[
\tilde{p} \pm z_{1-\alpha/2} \sqrt{\frac{\tilde{p}(1 - \tilde{p})}{n + z_{1-\alpha/2}^2}}
\]
For a 95\% CI, this approach considers adding approximately 2 pseudo-observations to each count.

\subsection{Hypothesis Testing: Simple vs Composite Hypotheses}
In hypothesis testing, $H_0: p = p_0$ is a simple hypothesis because it specifies only one value for the parameter. The alternative hypothesis $H_1: 0 < p < 1$ is composite, covering a range of parameter values. In the context of the binomial distribution, to evaluate how extreme the observed data is under $H_0$, we calculate:
\[
\sum_{y: \mathrm{P}(Y=y) \leq \mathrm{P}(Y=y_{\text{obs}})} \mathrm{P}(Y=y)
\]
This calculation sums the probabilities of all outcomes as extreme or more extreme than the observed, and is utilized to determine the $p$-value in exact tests like the binom.test in R.

\subsection{Likelihood Ratio Tests and Their Applications}
Likelihood ratio tests assess how extreme data is by comparing the maximum likelihood estimates (MLEs) under the null hypothesis $H_0$ and the alternative hypothesis $H_1$. The likelihood ratio $Q(Y)$ is defined as:
\[
Q(Y) = \frac{L(\hat{p}_{0})}{L(\hat{p}_{1})}
\]
where $\hat{p}_{0}$ is the MLE under $H_0$ and $\hat{p}_{1}$ is the MLE under $H_1$. Values of $Q(Y)$ range between 0 and 1, with lower values indicating less support for $H_0$.

To compute the test probability for data being at least as extreme as observed, we use:
\[
\mathrm{P}\left(Q(Y) \leq Q(y_{\text{obs}})\right) = \sum_{y: Q(y) \leq Q(y_{\text{obs}})} \mathrm{P}(Y=y)
\]

Alternatively, the likelihood ratio can be represented using the statistic:
\[
-2 \log \left(\frac{L(\hat{p}_{0})}{L(\hat{p}_{1})}\right)
\]
which ranges from 0 to $\infty$, with higher values indicating less support for $H_0$.

Under $H_0$, this statistic approximately follows a $\chi^2$ distribution with degrees of freedom equal to the difference in parameters between $H_1$ and $H_0$ (Wilks' theorem). For the simple case where $H_0: p = p_0$ and $H_1: 0 < p < 1$, the degrees of freedom are 1.

The chi-squared approximation is valid if the expected numbers of successes and failures are both at least 5.

Likelihood-based confidence intervals can also be derived from these tests. Solving for $p_0$ where:
\[
-2 \log \left(\frac{L(\hat{p}_{0})}{L(\hat{p}_{1})}\right) > 3.84
\]
gives a 95\% confidence interval based on the chi-square distribution critical value for 1 degree of freedom (3.84), suggesting that the true proportion lies within specific bounds.

\subsection{Confidence Interval for the Difference of Proportions}

We consider two binomial distributions, \( Y_{1} \sim \operatorname{bin}(26, p_{1}) \) and \( Y_{2} \sim \operatorname{bin}(25, p_{2}) \), to analyze the difference in proportions \( p_{1} - p_{2} \). The estimated difference between the proportions is calculated as:
\[
\hat{p}_{1} - \hat{p}_{2} = 0.88 - 0.52 = 0.36
\]
This measure quantifies the discrepancy in success rates between the two scenarios represented by the binomial distributions.

The confidence interval (CI) for the difference between two binomial proportions, $\hat{p}_{1}$ and $\hat{p}_{2}$, is given by:

\[
\hat{p}_{1} - \hat{p}_{2} \pm z_{\alpha / 2} \sqrt{\frac{\hat{p}_{1}(1-\hat{p}_{1})}{n_{1}} + \frac{\hat{p}_{2}(1-\hat{p}_{2})}{n_{2}}}
\]

This expression assumes the asymptotic normality of $\hat{p}_{1} - \hat{p}_{2}$, supported by the Central Limit Theorem (CLT) or the asymptotic normality of the maximum likelihood estimator (MLE). The sample proportions are modeled as normally distributed:

\[
\begin{aligned}
\hat{p}_{1} &\approx N\left(p_{1}, \frac{p_{1}(1-p_{1})}{n_{1}}\right), \\
\hat{p}_{2} &\approx N\left(p_{2}, \frac{p_{2}(1-p_{2})}{n_{2}}\right).
\end{aligned}
\]

Given the independence of the samples, the sample proportions are jointly asymptotically normal, and their difference, as a linear transformation, also follows a normal distribution (delta method). This CI reflects the range within which the difference between the two proportions is expected to lie, with a specified level of confidence.

\section{Comparing binomal distributions and analysis of count data}

\subsection{Difference in proportions}

Considering the ability to identify pets by smell, we have data for dog and cat owners. The binomial distributions are given by $Y_{1} \sim \operatorname{bin}\left(26, p_{1}\right)$ and $Y_{2} \sim \operatorname{bin}\left(25, p_{2}\right)$ for dog and cat owners, respectively, representing correct identifications.

\begin{center}
\begin{tabular}{lcc}
\hline
 & Identified Correctly & Misidentified \\
\hline
Dog Owners & 23 & 3 \\
Cat Owners & 13 & 12 \\
\hline
\end{tabular}
\end{center}

The estimated difference in probabilities of correct identification is calculated as:
$$
\hat{p}_{1} - \hat{p}_{2} = \frac{23}{26} - \frac{13}{25} = 0.88 - 0.52 = 0.36
$$

\subsubsection{Contextual Importance of Proportion Differences}
In scenarios where the difference $p_{1} - p_{2}$ is the same, the relative significance can vary. For instance, a difference in adverse reaction probabilities between a drug and a placebo might be minimal numerically but substantial in practical implications.

\subsubsection{Confidence Interval for the Difference of Proportions}
The confidence interval for $\hat{p}_{1} - \hat{p}_{2}$, considering the binomial distributions approach normality with large sample sizes, is given by:

$$
\hat{p}_{1} - \hat{p}_{2} \pm z_{\alpha / 2} \sqrt{\frac{\hat{p}_{1} (1 - \hat{p}_{1})}{n_{1}} + \frac{\hat{p}_{2} (1 - \hat{p}_{2})}{n_{2}}}
$$

where $z_{\alpha / 2}$ is the critical value from the standard normal distribution for a chosen $\alpha$ level.

Assuming $\hat{p}_{1}$ and $\hat{p}_{2}$ are approximations of normal distributions, they are represented as:
$\begin{aligned}
& \hat{p}_{1} \sim N\left(p_{1}, \frac{p_{1}(1-p_{1})}{n_{1}}\right), \\
& \hat{p}_{2} \sim N\left(p_{2}, \frac{p_{2}(1-p_{2})}{n_{2}}\right).
\end{aligned}$

Since the samples are independent, the difference $\hat{p}_{1} - \hat{p}_{2}$ is also normally distributed, validated by the Central Limit Theorem and the properties of linear transformations.

\subsection{The multinomial distribution}

Consider $n$ trials, each with $c$ possible outcomes. Each trial has a probability $p_j$ of resulting in outcome $j$. If $Y_j$ counts the occurrences of outcome $j$, then $(Y_1, \ldots, Y_c)$ follows a multinomial distribution, expressed as:
$$
p(y_1, \ldots, y_c) = \frac{n!}{y_1! \cdots y_c!} \prod_{j=1}^{c} p_j^{y_j},
$$
with $\sum_{j=1}^{c} y_j = n$ and $\sum_{j=1}^{c} p_j = 1$.

For two categories, $(Y_1, Y_2)$ simplifies to a binomial distribution for $Y_1$.

Each trial outcome $Z_i$ is represented as:
$$
Z_i = (Z_{i1}, \ldots, Z_{ic}), \quad Z_{ij} = \begin{cases} 1, & \text{if outcome is } j, \\ 0, & \text{otherwise.} \end{cases}
$$
The random variable $Y_j = \sum_{i=1}^{n} Z_{ij}$ counts occurrences of category $j$. The expected count for category $j$ is $\mathbb{E}Y_j = n p_j$.

\subsubsection{Properties of the Multinomial Distribution}

\begin{itemize}
    \item The sum of multinomially distributed variables is multinomial if the probability vectors are identical.
    \item Merging categories also yields a multinomial distribution; probabilities of merged categories are summed.
    \item Counts for a single category follow a binomial distribution.
\end{itemize}

\subsubsection{Multinomial Likelihood Function}
The MLE for proportions $p_1, \ldots, p_c$ is $\hat{p}_j = \frac{n_j}{n}$. Maximization of the log-likelihood function
$$
\ell(p) = \sum_{i=1}^{c} n_i \log(p_i)
$$
is subject to the constraint $\sum p_j = 1$.

\subsubsection{Pearson Chi-Squared Test}
The Pearson chi-squared statistic for testing $H_0: p = p^0$ is:
$$
X^2 = \sum_{j=1}^{c} \frac{(O_j - E_j)^2}{E_j} = \sum_{j=1}^{c} \frac{(Y_j - n p_j^0)^2}{n p_j^0},
$$
where $O_j$ is the observed count and $E_j$ is the expected count under $H_0$. Small $X^2$ values indicate a good fit with $H_0$.

\subsubsection{Example: Mendelian Inheritance}
Mendel's data and theoretical proportions under $H_0$:
$$
H_0: p = \left(\frac{9}{16}, \frac{3}{16}, \frac{3}{16}, \frac{1}{16}\right),
$$
yield a Pearson statistic $X^2 = 0.470$ with a high $p$-value, suggesting a good fit with Mendelian predictions.

\section{Two-way contingency tables}

Subjects are classified based on two criteria into categories $\{1, \ldots, R\}$ and $\{1, \ldots, C\}$. The resulting contingency table records frequency counts as shown below:
\begin{center}
\begin{tabular}{c|cccc|c}
 & Column 1 & Column 2 & $\cdots$ & Column C & Total \\
\hline
Row 1 & $y_{11}$ & $y_{12}$ & $\cdots$ & $y_{1 C}$ & $n_{1}$ \\
Row 2 & $y_{21}$ & $y_{22}$ & $\cdots$ & $y_{2 C}$ & $n_{2}$ \\
$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
Row R & $y_{R 1}$ & $y_{R 2}$ & $\cdots$ & $y_{R C}$ & $n_{R}$ \\
\hline
Total &  &  &  &  & $n$ \\
\hline
\end{tabular}
\end{center}

\subsection{Sampling schemes for contingency tables}
For example, studying the connection between seat-belt use and accident severity yields a contingency table summarized as follows:
\begin{center}
\begin{tabular}{lll}
Seat-belt use & Fatality & Nonfatality \\
\hline
Yes &  &  \\
No &  &  \\
\hline
\end{tabular}
\end{center}

\subsubsection{Multinomial Sampling with a Fixed Overall Total}
Sample 200 police records, classifying each by fatality and seat-belt usage. The counts $(Y_{11}, Y_{12}, Y_{21}, Y_{22})$ are distributed as follows:
\[
(Y_{11}, Y_{12}, Y_{21}, Y_{22}) \sim \operatorname{Multinom}(200, p_{11}, p_{12}, p_{21}, p_{22})
\]
\[
p(y_{11}, y_{12}, y_{21}, y_{22})=\binom{200}{y_{11}, \ldots, y_{22}} \prod_{r=1}^{2} \prod_{j=1}^{2} p_{r j}^{y_{r j}}
\]

\subsubsection{Independent Multinomial Sampling with Fixed Row Totals}
Assign 100 subjects to wear seat-belts and 100 not to, and record fatality outcomes. Sampling within each row is given by:
\[
(Y_{r 1}, Y_{r 2}) \sim \operatorname{Multinom}(100, p_{r 1}, p_{r 2})
\]
\[
p(y_{11}, y_{12}, y_{21}, y_{22})=\prod_{r=1}^{2}\left\{\binom{100}{y_{r 1}, y_{r 2}} \prod_{j=1}^{2} p_{r j}^{y_{r j}}\right\}
\]

\subsubsection{Independent Multinomial Sampling with Fixed Column Totals}
Sample 100 records from fatal and 100 from non-fatal accidents to check seat-belt usage. This setup also involves independent multinomial distributions based on fixed column totals.

\subsection{Tests in a Two-Way Table}
Two primary types of tests are conducted on two-way contingency tables:

\begin{enumerate}
    \item \textbf{Test for Homogeneity:} This test is used when there are several populations, each classified according to the same categorical variable. The question it addresses is whether the $R$ different multinomial distributions share a common probability vector across these classifications.
    
    \item \textbf{Test for Independence:} This test applies when a single sample is classified according to two criteria (rows and columns). It investigates whether the probabilities of the cells (joint probabilities) are products of their corresponding row and column probabilities.
\end{enumerate}

Both tests typically employ the chi-squared statistic to determine if observed frequencies differ significantly from expected frequencies under the hypothesis of homogeneity or independence.

\subsubsection{Independence of Categorical Variables}
The principle of independence in a contingency table can be summarized as follows:
\[
\mathrm{P}(\{\text{Row } r\} \cap \{\text{Column } j\}) = \mathrm{P}(\{\text{Row } r\}) \mathrm{P}(\{\text{Column } j\})
\]
This equation means that the joint probability of an observation falling into a specific row and column is the product of the marginal probabilities of that row and column, assuming independence.

Alternatively, independence can be expressed in terms of conditional probabilities:
\[
\mathrm{P}(\{\text{Column } j\} \mid \{\text{Row } r\}) \text{ is consistent across all rows}
\]
This implies that the probability of falling into a specific column, given any row, is the same, which would signify independence between the row and column categorizations.

In cases of independent multinomial sampling, where each row of the contingency table represents a different multinomial distribution (with the row total fixed by design), the tests for comparing binomial proportions are applicable. This scenario generally does not lend itself to discussing row probabilities directly due to their predetermined nature.

\subsection{Pearson Chi-Squared Test in a Two-Way Table}
The Pearson Chi-Squared test is designed to measure the discrepancy between observed counts and expected counts in a contingency table under the null hypothesis $H_0$, which typically posits independence or homogeneity across groups. The test statistic is defined as:
\[
X^{2}=\sum_{r=1}^{R} \sum_{j=1}^{C} \frac{(O_{r j}-E_{r j})^2}{E_{r j}}
\]

where $O_{r j}$ represents the observed count in cell $(r, j)$, and $E_{r j}$ is the expected count in cell $(r, j)$ assuming $H_0$ is true. Smaller values of $X^2$ suggest a closer fit between the observed data and the expectations under $H_0$.

The chi-squared approximation is valid if each expected cell count is at least 5.

\subsubsection{Asymptotic Distribution of the Pearson Statistic}
Under the null hypothesis $H_0$, the test statistic $X^2$ approximately follows a chi-squared distribution $\chi_{\mathrm{df}}^2$, where the degrees of freedom, $\mathrm{df}$, are calculated as:
\[
\mathrm{df} = (R-1)(C-1)
\]
This degrees of freedom formula accounts for the reduction in parameters when moving from the alternative hypothesis $H_1$ (no restrictions on cell probabilities) to the null hypothesis $H_0$ (independent categories).

\section{Logistic regression}

\subsection{Comparing Probabilities, Odds, and Risks}

Two common methods for comparing probabilities, $p_{1}$ and $p_{2}$, are:

\begin{itemize}
  \item \textbf{Difference in Probability}: $p_{1}-p_{2}$
  \begin{itemize}
    \item A difference of zero indicates equal probabilities.
  \end{itemize}
  
  \item \textbf{Relative Risk}: $p_{1} / p_{2}$
  \begin{itemize}
    \item A relative risk of 1 implies equal probabilities.
  \end{itemize}
\end{itemize}

An alternative measure of risk is the odds of an event $A$, defined as:
$$
\text{Odds of event } A = \frac{\mathrm{P}(A)}{1 - \mathrm{P}(A)} \in [0, \infty]
$$
When $\mathrm{P}(A) = 1$, the odds are defined as $\infty$. 

For instance, if the probability of boys born in every 100 births is 51, then:
$$
\text{Odds of a boy} = \frac{0.51}{0.49} = 1.04
$$

\subsubsection{Comparison of Risks: Odds Ratio}
The odds ratio ($\theta$) compares two group-specific odds of an event:
$$
\theta = \frac{\frac{p_{1}}{1-p_{1}}}{\frac{p_{2}}{1-p_{2}}} = \frac{p_{1}\left(1-p_{2}\right)}{p_{2}\left(1-p_{1}\right)}
$$
$\hat{\theta}$ can be estimated directly from $\hat{p}_{1}$ and $\hat{p}_{2}$.

\subsubsection{Confidence Interval for Log Odds Ratio}
A confidence interval for the log odds ratio in a $2 \times 2$-table is given by:
$$
\log \left(\frac{y_{11} y_{22}}{y_{12} y_{21}}\right) \pm z_{\alpha / 2} \sqrt{\frac{1}{y_{11}}+\frac{1}{y_{12}}+\frac{1}{y_{21}}+\frac{1}{y_{22}}}
$$
This is assuming multinomial or independent multinomial sampling. A confidence interval for the odds ratio can be obtained by backtransformation:
$$
\log \theta \in (l, r) \Leftrightarrow \theta \in (\exp (l), \exp (r))
$$
If there are any zero counts ($n_{ij}=0$), the Wald interval does not exist. An ad-hoc approach is to add a small number (usually 0.5) to all cells.

\subsection{Logistic regression models}

Let $Y_{1}, \ldots, Y_{n}$ be independent random variables with
$$
Y_{i} \sim \operatorname{bin}\left(1, p_{i}\right)
$$
where each $Y_i$ represents a binary outcome and $p_{i}$ denotes the success probability associated with "trial $i$". These trials are independent but not necessarily identically distributed.

The success probability $p_{i}$ is modeled as a function of the explanatory variables associated with "trial $i$". Probabilities are modeled indirectly by assuming that the log-odds are linear in the parameters:
$$
\log \left(\text { Odds of success }_{i}\right) = \alpha+\beta_{1} x_{i 1}+\cdots+\beta_{d} x_{i d}
$$

The logit function, denoted as $\operatorname{logit}\left(p_{i}\right)$, transforms probabilities to log-odds:
$$
\operatorname{logit}\left(p_{i}\right) = \log \left(\frac{p_{i}}{1-p_{i}}\right)
$$
A linear model for the log-odds corresponds to probabilities modeled as:
$$
p_{i} = \operatorname{logit}^{-1}\left(\alpha+\beta_{1} x_{i 1}+\cdots+\beta_{d} x_{i d}\right)=\frac{\exp \left(\alpha+\beta_{1} x_{i 1}+\cdots+\beta_{d} x_{i d}\right)}{1+\exp \left(\alpha+\beta_{1} x_{i 1}+\cdots+\beta_{d} x_{i d}\right)}
$$

\subsubsection{Example: Male Moths}

In an experiment, 120 male moths were subjected to an insecticide. The logit model for the probability of death is:
$$
\log \left(\frac{p_{i}}{1-p_{i}}\right) = \alpha + \beta \cdot \operatorname{dose}_{i}
$$

The estimated log-odds that moth $i$ dies are given by:
$$
\log \left(\frac{p_{i}}{1-p_{i}}\right) = -1.9277 + 0.2972 \cdot \operatorname{dose}_{i}
$$

$\hat{\alpha} = -1.9277$ represents the log-odds that a moth with a dose of 0 dies. The odds of a moth dying, given no insecticide, are $\exp (-1.9277) = 0.1455$, corresponding to a probability of 0.127.

$\hat{\beta} = 0.2972$ is the log-odds ratio comparing the odds of dying for moths with doses differing by one unit.

In conclusion, for every unit increase in dose, the odds of dying increase by approximately $35\%$, with an odds ratio of $\exp (\hat{\beta}) = 1.346$.

\subsection{Logistic Likelihood Function and MLE}

The $n$ observations are independent and binomially distributed with $p_{i}$ depending on the unknown parameters and the given values of the explanatory variables:
$$
\begin{aligned}
L\left(\alpha, \beta_{1}, \ldots, \beta_{d}\right) & =\prod_{i=1}^{n} p_{i}^{y_{i}}\left(1-p_{i}\right)^{1-y_{i}} \\
& =\prod_{i=1}^{n}\left(\frac{\exp (\alpha+\boldsymbol{\beta} \boldsymbol{x})}{1+\exp (\alpha+\boldsymbol{\beta} \boldsymbol{x})}\right)^{y_{i}}\left(1-\frac{\exp (\alpha+\boldsymbol{\beta} \boldsymbol{x})}{1+\exp (\alpha+\boldsymbol{\beta} \boldsymbol{x})}\right)^{1-y_{i}}
\end{aligned}
$$
The Maximum Likelihood Estimation (MLE) is found by an iterative algorithm; there is no closed-form solution as in Gaussian models.

Confidence intervals for parameters can be based on the asymptotic normality of the MLE. Parameterization can often be cleverly done to express a quantity of interest directly as a parameter.

The estimated variance matrix for the MLE (either observed information matrix or Fisher information) is typically available in R. It can be used to get the standard errors of complex functions of the parameters.

In some situations, the MLE does not exist, such as complete separation. It's challenging to reason about separation, but some red flags include oddly extreme parameter estimates and extremely large standard errors.

\subsection{Likelihood Ratio Tests}

Likelihood-ratio tests are more general than Wald tests and usually more reliable. They can compare any two nested models:
$$
-2 \log \left(\frac{L_{0}}{L_{\text {full }}}\right)=2\left(\log \left(L_{\text {full }}\right)-\log \left(L_{0}\right)\right) \approx \chi_{\left|H_{\text {full }}\right|-\left|H_{0}\right|}^{2}
$$
Here, the likelihood has been maximized under both models. Degrees of freedom are computed as the difference in the number of parameters between the two models.

For logistic regression models, typically a shifted version of the log-likelihood, the deviance, is used. The drop in deviance between two models is the same as the drop in log-likelihood.

\section{Monte Carlo}

Simulation serves as a powerful probabilistic tool, often used for approximating probabilities and statistical moments when analytical solutions are complex or unavailable. For example, the Monty Hall problem's optimal strategy—switching doors—can be validated by simulating thousands of games, demonstrating a success rate of about $2/3$. Similarly, if the mean and variance of a random variable $X$ are unknown, but sampling from $X$ is possible, they can be estimated by:
\[
E(X) \approx \bar{X}_n = \frac{1}{n}(X_1 + \cdots + X_n),
\]
\[
\operatorname{Var}(X) \approx \frac{1}{n-1} \sum_{j=1}^{n} (X_j - \bar{X}_n)^2.
\]
These approximations improve with larger sample sizes $n$, facilitated by the law of large numbers. This approach, called the Monte Carlo method, relies on the ability to generate random samples efficiently.

A challenge arises when the target distribution, such as a Beta distribution with density $f(x) \propto x^{3.1}(1-x)^{4.2}$ for $0 < x < 1$, does not suggest an easy sampling strategy. Though theoretically feasible using transformations from a $\operatorname{Unif}(0,1)$ random variable, practical implementation, like computing or inverting the cumulative distribution function (CDF), can be infeasible.

Many scientific applications involve even more complex distributions, where normalizing constants of the density functions are unknown. Markov chain Monte Carlo (MCMC) techniques, however, allow simulation from these distributions using Markov chains. By designing a Markov chain with a desired stationary distribution $\mathbf{s}$, one can approximate $\mathbf{s}$ by simulating the chain over a long period.

Two primary MCMC methods are the Metropolis-Hastings algorithm and Gibbs sampling.

\subsection{Metropolis-Hastings Algorithm}
The Metropolis-Hastings algorithm modifies any irreducible Markov chain to achieve a desired stationary distribution by selectively accepting or rejecting the proposed transitions based on specific probabilities. This ensures that the resultant chain retains the target distribution as its stationary distribution.

\subsubsection{Algorithm Outline}
Given:
\begin{itemize}
    \item A desired stationary distribution $\mathbf{s} = (s_1, \ldots, s_M)$ on state space $\{1, \ldots, M\}$,
    \item An original Markov chain transition matrix $P = (p_{ij})$,
\end{itemize}

the Metropolis-Hastings algorithm operates as follows:
\begin{enumerate}
    \item Start from any state $X_0$.
    \item At each step $n$, from current state $X_n = i$:
    \begin{enumerate}
        \item Propose a new state $j$ using the transition probabilities from $P$.
        \item Calculate the acceptance probability:
        \[
        a_{ij} = \min \left(\frac{s_j p_{ji}}{s_i p_{ij}}, 1\right).
        \]
        \item Accept the move to state $j$ with probability $a_{ij}$, otherwise remain in state $i$.
    \end{enumerate}
    \item Set $X_{n+1}$ based on the acceptance step.
\end{enumerate}

This procedure modifies $P$ to a new transition matrix $Q$ of the Metropolis-Hastings chain, where:
\[
q_{ij} = p_{ij} a_{ij} \quad \text{and} \quad q_{ii} = 1 - \sum_{j \neq i} q_{ij}.
\]
The algorithm assures that $q_{ij}$ and $q_{ji}$ are either both zero or both non-zero, maintaining balance and ensuring reversibility under the stationary distribution $\mathbf{s}$.

\subsubsection{Properties}
\begin{itemize}
    \item \textbf{Reversibility}: This chain satisfies the detailed balance condition, $s_i q_{ij} = s_j q_{ji}$, confirming $\mathbf{s}$ as the stationary distribution.
    \item \textbf{Flexibility}: Works for any given $P$ on the same state space without needing the normalizing constant for $\mathbf{s}$.
    \item \textbf{Practical Considerations}: The choice of $P$ (proposal distribution) critically affects the convergence speed of the chain. Effective proposal distributions balance between high acceptance rates and the ability to explore the state space efficiently.
\end{itemize}

\subsubsection{Implementation}
For effective implementation:
\begin{enumerate}
    \item Choose an initial state that either represents a typical state or diversely explores potential states.
    \item Ensure the proposal distribution $P$ is well-designed to suggest feasible transitions.
    \item Run the chain for a sufficient number of steps to approximate convergence to the stationary distribution.
\end{enumerate}

This algorithm extends beyond discrete state spaces to continuous distributions, accommodating complex models in various scientific fields.

\subsection{Gibbs Sampling}
Gibbs sampling is an MCMC method particularly useful for sampling from a joint distribution by iteratively updating each variable from its conditional distribution while holding others fixed. This method is efficient when conditional distributions are straightforward to sample from.

\subsubsection{Systematic Scan Gibbs Sampler}
For discrete random variables $X$ and $Y$ with a joint probability mass function (PMF) $p_{X,Y}(x,y)$, the systematic scan Gibbs sampler updates variables in a fixed order:
\begin{enumerate}
    \item Draw $x_{n+1}$ from the conditional distribution $P(X \mid Y = y_n)$.
    \item Draw $y_{n+1}$ from the conditional distribution $P(Y \mid X = x_{n+1})$.
\end{enumerate}
The chain $(X_0, Y_0), (X_1, Y_1), \ldots$ converges to the stationary distribution $p_{X,Y}$.

\subsubsection{Random Scan Gibbs Sampler}
Similar to the systematic approach but selects the variable to update at random:
\begin{enumerate}
    \item Randomly choose a variable.
    \item Update the chosen variable from its conditional distribution based on the current state of the other variable.
\end{enumerate}
This also ensures convergence to the joint distribution $p_{X, Y}$.

\subsubsection{Generalization to Higher Dimensions}
Gibbs sampling extends to higher dimensions where each variable of a $d$-dimensional vector is updated in sequence from its conditional distribution given the current values of all other variables. Both systematic and random update orders are applicable.

\subsubsection{Comparison with Metropolis-Hastings}
Gibbs sampling differs from Metropolis-Hastings by focusing on conditional distributions without requiring acceptance probabilities. It is a special case of Metropolis-Hastings with a 100\% acceptance rate, simplifying implementation by eliminating the need for proposal distributions.

\subsubsection{Proof of Convergence}
Gibbs sampling is proved to converge under the framework of Metropolis-Hastings. For example, in a two-variable setup, transitions between states that involve updating one variable while keeping the other fixed always have an acceptance probability of 1, satisfying the Metropolis-Hastings condition for acceptance and ensuring the sampler converges to the correct stationary distribution.

\subsubsection{Applications}
Gibbs sampling is suited for complex models where direct sampling from the joint distribution is challenging but sampling from conditional distributions is feasible. It is commonly used in Bayesian statistics for updating posterior distributions of model parameters.

\subsubsection{Practical Implementation}
\begin{itemize}
    \item Initialize all variables.
    \item Iteratively update each variable from its conditional distribution.
    \item After a burn-in period, collect samples to approximate the joint distribution or compute statistics such as means or variances.
\end{itemize}

This algorithm is particularly effective in scenarios where the joint distribution is known only up to a normalizing constant, as each step involves conditioning on known quantities.

\end{document}