\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath} % Math
\usepackage{listings} % Required for inserting code
\usepackage{xcolor}   % Required for custom colors
\usepackage{color}
\usepackage{svg}    % Required for rendering svgs
\usepackage{float}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{outlines}

\title{Applied Statistics}
\author{Nicolas Lejeune, Peter Iatsenia}
\date{Spring 2024}

% Define colors similar to RStudio
\definecolor{codegreen}{rgb}{0,0.5,0}    % Comments in a green color
\definecolor{codegray}{rgb}{0.5,0.5,0.5} % Code-gray for numbers
\definecolor{codepurple}{rgb}{0,0.6,0} % Strings in green color
\definecolor{backcolour}{rgb}{1,1,1}     % White background color
\definecolor{codeblue}{rgb}{0,0,0}       % Blue for keywords

% R language lstset configuration
\lstset{
    language=R,                  % The language of the code
    basicstyle=\ttfamily\small,  % The style that is used for the code
    backgroundcolor=\color{backcolour}, % Set the background color for the snippet
    commentstyle=\color{codegreen},    % Comment style
    keywordstyle=\color{codeblue},     % Keyword style
    numberstyle=\tiny\color{codegray}, % The style that is used for the line numbers
    stringstyle=\color{codepurple},    % String literal style
    breakatwhitespace=false,           % Sets if automatic breaks should only happen at whitespace
    breaklines=true,                   % Sets automatic line breaking
    captionpos=b,                      % Sets the caption-position to bottom
    keepspaces=true,                   % Keeps spaces in text, useful for keeping indentation of code
    numbers=left,                      % Where to put the line numbers
    numbersep=5pt,                     % How far the line numbers are from the code
    showspaces=false,                  % Show spaces everywhere adding particular underscores
    showstringspaces=false,            % Underline spaces within strings only
    showtabs=false,                    % Show tabs within strings adding particular underscores
    tabsize=2                          % Sets default tabsize to 2 spaces
}

\begin{document}

\maketitle

\tableofcontents

\pagebreak

\section*{Preface}
This is a compressed note compendium based on the textbook: "Introduction to Statistical Data Analysis for the Life Sciences", Claus Thorn Ekstrøm \& Helle Sørensen, 2nd edition, 2015." For public use for students following the Spring 2024 Applied Statistics course.

All R Code is also available at the \href{https://github.com/palpatine/applied-statistics-2024}{github repository} in r/.

\section{Description of samples and populations}

\paragraph{}
Statistics involves using a sample, a subset of a larger population, to make inferences about the overall characteristics of that population. A population represents the complete set of subjects we want to study, while a sample is a smaller, representative group selected from this population. The key concepts are:

\begin{itemize}
    \item \textbf{Population}: The entire group of interest whose properties we want to analyze.
    \item \textbf{Sample}: A smaller, representative group drawn from the population.
    \item \textbf{Parameter}: A numerical value describing a characteristic of the entire population.
    \item \textbf{Statistic}: A numerical value describing a characteristic of the sample.
\end{itemize}

The process starts with selecting a sample from the population (sampling) and then using statistical methods to draw conclusions (statistical inference) about the population based on the sample data. For example, to estimate the average height of a population (a parameter), we measure the average height of a sample group. This sample average (a statistic) is then used to infer the population's average height. The course focuses on methods to make accurate inferences about population parameters using sample statistics. Whether a group is considered a population or a sample depends on the context and the type of inference being made.

\subsection{Data Types}
\paragraph{}
The data collected in a study dictates the statistical analysis approach, influencing the hypotheses that can be tested and the predictive models that can be used. Data can be broadly categorized into two types:

\begin{itemize}
    \item \textbf{Categorical Data}: This type includes non-numeric categories or groups.
    \item \textbf{Quantitative Data}: This type consists of numerical measurements or quantities.
\end{itemize}
The nature of the data (categorical or quantitative) is key in determining the appropriate statistical methods and models for analysis and prediction.

\subsubsection{Categorical data}

Categorical data sorts observations into groups based on qualitative traits, resulting in labels or categories. There are two subtypes:

\begin{itemize}
    \item \textbf{Nominal Data}: These categories have no natural order. Examples include hair color, gender, race, and smoking status. For instance, hair colors (brown, blonde, gray) are merely different without any inherent ranking.
    \item \textbf{Ordinal Data}: These categories have a natural order. Examples include pain levels (none, little, heavy) or income brackets (low, middle, high). While we can rank these categories (e.g., low income is less than high income), the actual difference between them isn't quantifiable. For example, the gap between low and middle income isn't necessarily the same as that between middle and high income.
\end{itemize}

The key distinction is that ordinal data has a rank order, but the magnitude of difference between categories isn't measurable or consistent.

\subsubsection{Quantitative data}

Quantitative data are numerical and fall into two categories:

\begin{itemize}
    \item \textbf{Discrete Quantitative Data}: These are countable numbers, representing finite possible values. They accurately reflect counts, like household size or number of kittens in a litter. Differences between values have a clear quantitative interpretation (e.g., the difference between 9 and 7 households is the same as between 5 and 3 households).
    \item \textbf{Continuous Quantitative Data}: Representing measurements like length, volume, time, mass, etc., these are ideally continuous and gapless. While theoretically continuous, practical limitations often lead to less detailed measurements (e.g., measuring time in days instead of seconds). Despite not being measured with infinite precision, treating these variables as continuous is generally appropriate.
\end{itemize}

Categorical data are summarized by frequencies or proportions in each category, whereas quantitative data are typically summarized using averages or means.

In the Danscher et al. (2009) study on acute laminitis in cattle, different data types were used:

\begin{itemize}
    \item \textbf{Location (Nominal Data)}: Non-ordered categories (I or II).
    \item \textbf{Weight (Continuous Quantitative Data)}: Reported in whole kilograms, with meaningful differences.
    \item \textbf{Lameness Score (Ordinal Data)}: Ranked (normal to severely lame).
    \item \textbf{Number of Swelled Joints (Discrete Quantitative Data)}: Countable numeric values.
\end{itemize}
This study exemplifies the integration of various data types in research.

\subsection{Visualizing categorical data}
\paragraph{}
Categorical data can be represented in many ways, these are listed below:

\begin{itemize}
    \item \textbf{Frequency tables}: for listing category occurrences; ideal for fewer categories.
    \item \textbf{Bar charts/graphs}: for larger categories or cross-population comparison.
    \item \textbf{Segmented bar charts}: Display relative frequencies as parts of a whole. Useful for comparing category distributions across populations.
\end{itemize}

\paragraph{}
There are also some important definitions regarding categorical data:

\begin{itemize}
    \item \textbf{Frequency}: Count of each category's occurrence.
    \item \textbf {Relative frequency}: Frequency divided by total observations; enables comparison across different-sized datasets. \text{Relative Frequency} = $\frac{\text{frequency}}{n}$
\end{itemize}






\subsection{Visualizing quantitative data}
\paragraph{}
Data visualization is crucial for understanding and interpreting categorical and quantitative data. For categorical variables or discrete quantitative data with limited values, frequency or relative frequency plots are effective. However, for continuous quantitative data, these plots become less informative due to the vast number of unique values. Instead, we use histograms, grouping data into bins, and counting observations per bin. This approach effectively represents the data's distribution, showing the center, spread, and modes.

\subsubsection*{Histograms and Relative Frequency Histograms}
\paragraph{}
Histograms, akin to bar charts for categorical data, display the count of observations in each bin. Relative frequency histograms, on the other hand, show the proportion of observations per bin, making it easier to compare different populations. The shape of a relative frequency histogram mirrors that of a standard histogram, differing only in scale. It's important to note that the histogram's accuracy depends on equal bin widths; unequal widths can distort the representation of frequencies.

\subsubsection*{Scatter Plots for Quantitative Variables}
\paragraph{}
For illustrating relationships between two quantitative variables, scatter plots are employed. These plots reveal the strength, shape (linear, curved, etc.), and direction (positive or negative) of the relationship between variables. They also help identify outliers or extreme observations. When one variable is under experimental control, it's designated as the explanatory variable and typically plotted on the x-axis, with the response variable on the y-axis. If there's no clear explanatory variable, the choice of axes is flexible, with the scatter plot highlighting correlation rather than causation.

\subsubsection*{Case Study: Tenderness of Pork}
\paragraph{}
An experiment compared two cooling methods (tunnel and rapid cooling) for pork from two groups (low and high pH). The tenderness of the pork was measured post-cooling. Data analysis included histograms and relative frequency histograms for each pH group, allowing comparison of tenderness distributions between the low- and high-pH groups. Additionally, scatter plots depicted the relationship between the tenderness scores from both cooling methods, showcasing the practical application of these visualization techniques in interpreting interactions between two quantitative variables.


\subsection{Statistical summaries}

Categorical data are effectively summarized using tables. For quantitative data, which don't fit fixed categories, binning the data like in histograms is an option, but this can lose detail and depends heavily on bin choices. Instead, summary statistics are preferred. The measure of central tendency, like an average, represents the data's "middle" value, indicating a typical observation. However, as different datasets can have the same central tendency, it's also crucial to assess the data's variability or dispersion. This shows how much data points deviate from the central value, giving a fuller understanding of the data's spread and overall distribution.

\subsubsection{Median and inter-quartile range}

In a sample with $n$ independent, quantitative observations $(y_1,...,y_n)$, these can be ordered from smallest to largest, denoted as $y_1,...,y_n$ where $y_1$ is the smallest, $y_2$ is the second smallest, and so on.

The median, a central tendency measure, is the middle value in this ordered set. For an odd number of observations $(n)$, it's $y_{(\frac{n+1}{2})}$; for an even $n$, it's $\frac{1}{2}[y_{(n/2)}+y_{(n/2+1)}]$. The median applies to both quantitative and ordinal categorical data.

The range, a basic dispersion measure, is the difference between the highest and lowest values $(y_n-y_1)$. However, the range only considers two values and can be misleading. For example, three datasets with the same range can have very different dispersions.

\begin{itemize}
    \item Dataset 1: 14, 14, 14, 14, 14, 14, 34
    \item Dataset 2: 14, 16, 19, 22, 26, 30, 34
    \item Dataset 3: 14, 14, 14, 34, 34, 34, 34
\end{itemize}
All have a range of 20, but their distributions vary significantly.

The interquartile range (IQR) is a more nuanced dispersion measure. It's calculated by removing the top and bottom 25\% of observations and then finding the range of the remaining 50\%. Denoted as $IQR=Q3-Q1$, where $Q1$ and $Q3$ are the first and third quartiles, respectively, IQR is less affected by extreme values.

Quartiles are specific examples of quantiles, which divide ordered data into equal-sized subsets. The $x$th quantile is the value below which $x\%$ of the data falls.  The 25th and 75th quantiles are the first and third quartiles, respectively, and the 50th quantile is the median, dividing the data into four equal parts. Though exact quantile calculation can vary slightly in finite datasets, the interpretation remains consistent.

\subsubsection{Boxplot}
\textbf{Boxplot Overview}
\begin{itemize}
    \item Summarizes data with five key statistics: minimum, $Q1$, median, $Q3$, and maximum.
    \item Box represents the IQR, median is shown as a central line, and whiskers extend to the minimum and maximum values.
    \item Useful for assessing distribution, including symmetry and skewness.
\end{itemize}

\textbf{Outlier Detection Using IQR}
\begin{itemize}
    \item Outliers are defined as observations beyond $Q1 - 1.5 \cdot IQR$ or $Q3 + 1.5 \cdot IQR$.
    \item Crucial for recognizing anomalous data points that may affect the analysis.
\end{itemize}

\textbf{Modified Boxplot Representation}
\begin{itemize}
    \item Shows outliers as individual points, with minimum and maximum defined within the interval $[Q1 - 1.5 \cdot IQR, Q3 + 1.5 \cdot IQR]$.
    \item Allows for clearer visualization of extreme values, aiding in comparative distribution analysis.
\end{itemize}

\subsubsection{The mean and standard deviation}
The mean and standard deviation are key measures for quantitative data:

\begin{itemize}
    \item \textbf{Mean}: The mean (denoted as $\bar{y}$) is the average of a sample's observations. It's calculated by $\bar{y}=\frac{\sum_{i=1}^{n} {y_i}}{n}$ 
    \item \textbf{Standard Deviation}:  This measures the dispersion or how much the observations typically deviate from the mean.
    It's defined as: $s=\sqrt{\frac{\sum_{i=1}^{n} ({y_i-\bar{y}})^2}{n-1}}$
    \item \textbf{Variance}: Represented as $s^2$, it's the square of the standard deviation: $s^2=\frac{\sum_{i=1}^{n} ({y_i-\bar{y}})^2}{n-1}$
\end{itemize}

Both the mean and standard deviation incorporate information from all observations, providing a more comprehensive view than the median and inter-quartile range. They are expressed in the same units as the original data, enabling direct interpretation in the context of the observed values.

\textbf{Why do we divide by n-1?}

Using the $n-1$ denominator compensates for the loss of a degree of freedom. When we know the mean $\bar x$, we loose a degree of freedom of information, because once we know $n-1$ deviations, we know the $n$'th deviation (as all the deviations should sum to 0), thus the $n$'th deviation is dependent. If we were using $n$ as a denominator, we'd be assuming that all the deviations are independent, which is not the case, causing a biased estimation.

\pagebreak

\textbf{Sample mean and standard deviation of linearly transformed data}

Let $\bar{y}$ and $s$ be the sample mean and sample standard deviation from observations $y_1,...,y_n$ and let $y_{i}'=c \cdot y_{i} + b$ be a linear transformation of the $y$'s with constraints $b$ and $c$. Then $\bar{y'} = c \cdot \bar{y}+b$ and $s'=|c|\cdot s$.

\subsubsection{Mean or median?}
The median divides the data into two parts with an equal number of observations or equal areas under the histogram, disregarding the actual distances from the center. The mean also partitions the data into two halves, but it considers the values' distance from the center, making it sensitive to extreme values. 

\textbf{Sensitivity to Extreme Values}
\begin{itemize}
    \item The median is robust against extreme values, relying only on the two middle observations.
    \item The mean is influenced by all observations, making it susceptible to extreme values.
\end{itemize} 

\textbf{Mathematical Properties and Usage}
\begin{itemize}
    \item The mean has desirable mathematical properties, aiding in proving theorems and inferential statistics.
    \item The median is more robust but mathematically more challenging to work with.
    \item The mean is preferred for symmetric data except in the presence of extreme values, where the median is more suitable.
    \item The central limit theorem supports the use of sample means as symmetric estimates, regardless of the original distribution, given a large sample size.
\end{itemize}

\subsection{What is a probability?}

Probability is the likelihood of a random event occurring and is based on the concept of relative frequency in large numbers of experiments.

When conducting random experiments, such as rolling a die or measuring daily milk production from cows, the outcomes can vary each time. This variability is intrinsic to random events. For example, the occurrence of an even number on a die roll is a basic type of random event. If we denote this event as A and perform a large number of die rolls (denoted as $n$), he relative frequency of A $(n_A/n)$, which is the number of times A occurs divided by the total number of rolls, provides an empirical estimate of the probability of A.

As the number of trials $n$ increases, the relative frequency tends to stabilize. This stabilized value, approached as $n$ becomes very large, is considered the probability of the event.

For instance, in an experiment involving throwing a thumbtack 100 times and observing whether the pin points up or down, the relative frequency of the pin pointing down stabilizes around a certain value as the number of throws increases. This stabilization point, observed empirically, is interpreted as the probability of the thumbtack landing pin-down. In the thumbtack example, the probability was found to be approximately 0.6 or 60%.

This example illustrates how probability is derived from the relative frequency of an event in a large number of trials, reflecting the likelihood of that event occurring in a given random experiment.
\pagebreak

\subsection{R Code}

\subsubsection*{Boxplots and Modified Boxplots}

\begin{lstlisting}
# Example data: A sample of quantitative observations
observations <- c(22, 26, 24, 19, 23, 27, 28, 18, 30, 40, 15)  # Replace with actual data

# Standard Boxplot
boxplot(observations, 
        main = "Standard Boxplot", 
        ylab = "Values",
        xlab = "Data",
        range=0) # range=0 extends whiskers to minimum and maximum values

# Modified Boxplot
boxplot(observations, 
        main = "Modified Boxplot", 
        ylab = "Values",
        xlab = "Data")
\end{lstlisting}

\subsubsection*{Mean, Median, Standard Deviation, and Variance}

\begin{lstlisting}
# Example data: A sample of quantitative observations
observations <- c(5, 10, 15, 20, 25)  # Replace with actual data

# Calculate the mean
mean_value <- mean(observations)
# The 'mean' function calculates the average of the observations

# Calculate the median
median_value <- median(observations)
# The 'median' function calculates the median of the observations

# Calculate the standard deviation
std_dev <- sd(observations)
# The 'sd' function calculates the standard deviation, 
# which measures the average deviation from the mean

# Calculate the variance
variance_value <- var(observations)
# The 'var' function calculates the variance (standard deviation squared)
# Variance measures how spread out the numbers are from the mean

# Print the results
print(paste("Mean:", mean_value))
print(paste("Median:", median_value))
print(paste("Standard Deviation:", std_dev))
print(paste("Variance:", variance_value))
\end{lstlisting}

\pagebreak

\subsection{Proofs}
\subsubsection*{Proof that the Sample Variance is an Unbiased Estimator of the Population Variance}
Let $X_1, X_2, \ldots, X_n$ be a random sample from a population with variance $\sigma^2$. The sample variance $S^2$ is defined as:

\[
E(S^2) = \frac{1}{n-1} E(\sum_{i=1}^{n} (X_i - \bar{X})^2)
\]

Where $\bar{X}$ is the sample mean:

\[
\bar{X} = \frac{1}{n} \sum_{i=1}^{n} X_i
\]

To prove that the sample variance is an unbiased estimator of the population variance, we need to show that:

\[
E(S^2) = \sigma^2
\]

\begin{proof}
\hfill \break
\hfill \break
Let's start by expanding $E(\sum_{i=1}^{n} (X_i - \bar{X})^2)$:
\[
E(\sum_{i=1}^{n} (X_i - \bar{X})^2) = E((\sum_{i=1}^{n} (X_i^2)) - 2\bar{X}(\sum_{i=1}^{n} (X_i)) + (\sum_{i=1}^{n} (\bar{X^2}))
\]

This is equivalent to
\[
E((\sum_{i=1}^{n} (X_i)) - 2\bar{X}(\sum_{i=1}^{n} (X_i)) + n\bar{X^2}
\]

If we consider $\bar{X} = \frac{(\sum_{i=1}^{n} (X_i)}{n}$ then,
\[
\sum_{i=1}^{n} (X_i) = n\bar{X}
E((\sum_{i=1}^{n} (X_i)) - 2\bar{X}(\sum_{i=1}^{n} (X_i)) + n\bar{X^2} = E((\sum_{i=1}^{n} (X_i)) - 2\bar{X}\bar{X}n + n\bar{X^2}
\]

\[
= E((\sum_{i=1}^{n} (X_i)) - n\bar{X^2}))
\]

From here we can rewrite this equation as
\[
\sum_{i=1}^{n} (E(X_i) - nE(\bar{X^2}))
\]

We know that
\[
E(X_i^2) = \sigma^2 + \mu^2
\]

\[
E(\bar{X^2}) = \frac{\sigma^2}{n} + \mu^2
\]

This can be subbed into the equation above to find
\[
\sum_{i=1}^{n} (\sigma^2 + \mu^2 - n(\frac{\sigma^2}{n} + \mu^2)) = n\sigma^2 + n\mu^2 - \sigma^2 - n\mu^2
\]

\[
= (n-1)\sigma^2
\]

If we sub this back into the equation for $E(S^2)$, in place of $E(\sum_{i=1}^{n} (X_i - \bar{X})^2)$ then
\[
E(S^2) = \frac{1}{n-1}(n - 1)\sigma^2 = \sigma^2
\]
\end{proof}

\section{Linear Regression}
\paragraph{}
Data analysis often seeks to express one variable as a function of another, either based on theoretical hypotheses or empirical discovery. Simple linear regression models this relationship between two quantitative variables, \( x \) and \( y \), through a linear equation \( y = \alpha + \beta \cdot x \), where \( \alpha \) is the intercept and \( \beta \) the slope. Here, \( y \) is the dependent variable, influenced by the explanatory variable, \( x \).
\paragraph{}
An example is modeling the relationship between stearic acid levels and fat digestibility, where data suggests a linear trend, enabling predictions outside the observed range. However, real-life data often deviates from the model, indicating the linear model is an approximation, capturing the general trend rather than exact values.

\subsection{Fitting a regression line}
\paragraph{}
Fitting a regression line involves determining the optimal parameters (\( \hat{\alpha}, \hat{\beta} \)) that best represent observed data pairs (\(x_i, y_i\)). This is done by minimizing residuals (\(r_i = y_i - \hat{y_i}\)), the differences between observed values and those predicted by the model \(y = \hat{\alpha} + \hat{\beta} \cdot x\). The method of least squares addresses this by squaring and summing the residuals, ensuring that deviations are treated uniformly, irrespective of their direction. It involves solving for the parameters that minimize the sum of squared residuals, leading to unique parameter estimates.
\paragraph{}
For the stearic acid and digestibility example, the best-fit line was found to be \(y = -0.9337 \cdot x + 96.5334\), allowing for predictions and insights into the relationship between stearic acid levels and digestibility. The least squares method not only ensures the minimization of squared residuals but also that the regression line passes through the mean points (\(\bar{x}, \bar{y}\)), providing a robust model for understanding and predicting the dependent variable based on the independent variable.

\subsubsection{Least squares estimation}
The least squares method is employed to optimize model parameters by minimizing the sum of squared deviations between observed data and model predictions. In the context of a linear regression, the objective is to identify the optimal parameters, \( \alpha \) (intercept) and \( \beta \) (slope), such that the model \( y = \alpha + \beta \cdot x \) best fits the observed data points \( (x_i, y_i) \).

Given \( n \) observations, the least squares criterion seeks to minimize the sum of squared differences between the observed values \( y_i \) and the values predicted by the model \( \hat{y_i} = \alpha + \beta \cdot x_i \), formally expressed as:

\[
Q(\alpha, \beta; x, y) = \sum_{i=1}^{n} (y_i - \alpha - \beta \cdot x_i)^2
\]

To find the values of \( \alpha \) and \( \beta \) that minimize \( Q \), we take partial derivatives with respect to \( \alpha \) and \( \beta \), set them to zero, and solve the resulting equations:

\[
\frac{\partial Q}{\partial \alpha} = -2 \sum_{i=1}^{n} (y_i - \alpha - \beta \cdot x_i) = 0
\]

\[
\frac{\partial Q}{\partial \beta} = -2 \sum_{i=1}^{n} x_i (y_i - \alpha - \beta \cdot x_i) = 0
\]

Solving these equations provides the least squares estimates \( \hat{\alpha} \) and \( \hat{\beta} \) for the intercept and slope, respectively:

\[
\hat{\beta} = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}
\]

\[
\hat{\alpha} = \bar{y} - \hat{\beta} \cdot \bar{x}
\]

where \( \bar{x} \) and \( \bar{y} \) are the mean values of \( x \) and \( y \) respectively. The resulting line \( y = \hat{\alpha} + \hat{\beta} \cdot x \) represents the best fit to the data in the least squares sense, ensuring the minimized sum of squared residuals and passing through the point \( (\bar{x}, \bar{y}) \).

This method is not confined to linear regression but is a generalized approach for parameter estimation in various models, underpinning the robustness of predictions and understanding of relationships between variables.

\subsection{When is linear regression appropriate?}

This sub-section discusses fitting a linear relationship between two variables, $x$ and $y$, and highlights key considerations before applying linear regression:

\begin{itemize}
    \item \textbf{Quantitative Variables}: Linear regression is suitable only for quantitative variables. Both $x$ and $y$ must be quantitative to validly model their relationship through linear regression.
    \item \textbf{Linearity of Relationship}: It's crucial to assess if the relationship between $x$ and $y$ is linear. This is typically done by graphing the data and observing the relationship. If the relationship appears curvilinear, linear regression might not be appropriate. In such cases, data transformation or alternative models should be considered.
    \item \textbf{Influential Points}: Influential points in linear regression are those data points that have a significant impact on the regression line's slope. These points are often outliers in the $x$-direction. The influence of each data point on the slope is determined by its distance from the mean value of $x$ $(\bar{x})$. If the $x$-value of a point, $x_i$, is close to $\bar{x}$, it has minimal impact on the slope since the term $(x_i - \bar{x})$ in the slope formula's numerator and denominator will be small. If $x_i$ is far from $\bar{x}$, then both numerator and denominator will be large, and the difference $y_i - \bar{y}$ can have a large impact on the slope estimate.
    \item \textbf{$x$ on $y$ or $y$ on $x$}: In regression analysis, the choice of which variable to designate as the explanatory variable $x$ and which as the response variable $y$ significantly affects the model and its interpretation. When regressing $x$ on $y$, a different model is required than when regressing $y$ on $x$.

    In the regression equation $y=\alpha+\beta x$, solving for $x$ gives $x=-\alpha/\beta+y/\beta$, implying a line with intercept $-\alpha/\beta$ and slope $1/\beta$. However, this regression does not yield the same results as a direct regression of $x$ on $y$. The reason lies in the minimization of different types of residuals: the original regression minimizes vertical errors ($y$-direction) for predicting $y$ from $x$, whereas regressing $x$ on $y$ involves minimizing horizontal errors ($x$-direction) for predicting $x$ from $y$.
    
    Deciding which variable to treat as explanatory or response is simple in experiments where one variable is controlled and the other is observed. However, in cases where this distinction is not clear, it might be more appropriate to compute the correlation coefficient, as this approach avoids the inherent bias of choosing one variable over the other as the explanatory variable.
    
    \item \textbf{Interpolation}: Interpolation is the prediction of unobserved $y$ values within the observed data range (range of $x$ values). It is generally reliable except in certain cases, such as when there are limited $x$ values with multiple responses. In such cases, it's challenging to establish a linear relationship between $x$ and $y$.

    \item \textbf{Extrapolation}: Extrapolation is predicting outside the observed data range, becoming less certain with increased distance from this range. This uncertainty arises because the linear relationship cannot be confirmed beyond the observed values. While linear regression may fit well within specific intervals, it may not be appropriate for the entire range of possible values.
    
\end{itemize}

\subsubsection{Transformation}
\paragraph{}
The text discusses how to use linear regression, a statistical method, in situations where the relationship between two variables isn't straight-line (linear). Sometimes, even if the relationship isn't linear, we can make it linear by transforming one of the variables. This process is explained through the growth example of duckweed, a plant.
Below is the mathematical explanation of the transformation process. \hfill \break

\textbf{Original Situation} \hfill \break

We have data points $(x_i, y_i)$ where $x_i$ represents the days and $y_i$ the number of leaves. The relationship between days ($x$) and leaves ($y$) isn't linear, so a straight line doesn't fit the data well. \hfill \break

\textbf{Transformation} \hfill \break

We transform the response variable (number of leaves) using a logarithmic transformation. Recall that explanatory variable is what changes, and reponse variable gets changed in response to it. Let's call the transformed variable $z$. The transformation is: 
\begin{equation}
z_i = \log(y_i)
\end{equation}
\paragraph{}
After transformation, the relationship between days ($x$) and transformed leaves ($z$) is more linear. \hfill \break

\textbf{Linear Regression Model} \hfill \break

Now, we can model $z$ as a linear function of $x$:
\begin{equation}
z = \alpha + \beta \cdot x
\end{equation}
where $\alpha$ and $\beta$ are parameters to be estimated from the data. \hfill \break

\textbf{Exponential Growth Model} \hfill \break

The exponential growth model is given by:
\begin{equation}
f(t) = c \cdot \exp(b \cdot t)
\end{equation}
where:
\begin{itemize}
    \item $f(t)$ is the population size at time $t$,
    \item $c$ is the population size at time zero,
    \item $b$ is the average population increase per time unit.
\end{itemize} \hfill \break

\textbf{Applying Logarithm to the Model} \hfill \break

Taking the natural logarithm on both sides of the exponential growth model gives:
\begin{equation}
\log(f(t)) = \log(c) + b \cdot t
\end{equation}
\paragraph{}
This is a linear relationship with $\log(f(t))$ as the response and $t$ as the explanatory variable. We can compare this with the linear model $z = \alpha + \beta \cdot x$ and identify $\log(c)$ with $\alpha$ and $b$ with $\beta$. \hfill \break

\textbf{Estimation and Back-Transformation} \hfill \break

After fitting the linear model to the transformed data, we get estimates $\hat{\alpha}$ and $\hat{\beta}$. We back-transform these estimates to the original scale:
\begin{equation}
\hat{c} = \exp(\hat{\alpha}), \quad \hat{b} = \hat{\beta}
\end{equation}
\paragraph{}
These estimates can be plugged into the exponential growth model to predict the number of leaves on any given day:
\begin{equation}
\hat{f}(t) = \hat{c} \cdot \exp(\hat{b} \cdot t)
\end{equation}
\paragraph{}
The growth rate interpretation is that for every day, the number of leaves multiplies by a factor of $\exp(\hat{b})$. \hfill \break

By transforming the response variable and applying linear regression to the transformed data, we can effectively model and understand relationships that are inherently non-linear, as demonstrated in the duckweed growth example. \hfill \break
 \hfill \break
 \hfill \break

\subsection{Correlation Coefficient}

In linear regression, we model $y$ as a function of $x$, often assuming a causal relationship where $x$ influences $y$. This assumption is valid in controlled experiments, where $x$ is determined by the investigator, as in the example of stearic acid levels. However, in other cases, while there may be an association between $x$ and $y$, it's not necessarily correct to infer causality. Examples include the relationship between systolic and diastolic blood pressure, human height and weight, steak tenderness and meat dice size, or the growth of two plants in the same pot. These instances show an association but not necessarily a direct causal link. \hfill \break

The\textbf{ sample correlation coefficient} is: 

\begin{equation*}
\hat \rho = \frac{\sum_{i=1}^{n} {(x_i-\hat{x})(y_i-\hat{y})}}
{\sqrt{(\sum_{i=1}^{n} {(x_i-\bar{x})^2})(\sum_{i=1}^{n} {(y_i-\bar{y})^2})}}
\end{equation*}

It measures the strength of the linear relationship between $x$ and $y$ and always lies between -1 and 1. A value of 1 indicates a perfect positive linear relationship, -1 a perfect negative linear relationship, and 0 means there's no linear relationship. The correlation coefficient is dimensionless, reflecting the tightness of the linear relationship, not the slope. It's important to note that a strong correlation does not imply causation and a zero correlation doesn't rule out a strong non-linear relationship.

The correlation coefficient is analogous to the regression slope when regressing $y$ on $x$ after scaling both variables to have a standard deviation of 1. For this, $x_i$ and $y_i$ are transformed into $x_i'$ and $y_i'$, respectively, by dividing by their standard deviations, $s_x$ and $s_y$: $x_i'=x_i/s_x$ and $y_i'=y_i/s_y, i=1,...,n$. The regression slope of $y'$ on $x'$ is the correlation coefficient, $\hat\rho$.

\begin{itemize}
    \item \textbf{Numerator}: The numerator of the formula involves multiplying the residual of $x$ by the residual of $y$ for each pair. A positive contribution results when $x$ and $y$ deviate in the same direction from their means, and a negative one when they deviate in opposite directions.

    \item \textbf{Denominator}: The denominator of the formula is positive, except when all $x$ or $y$ values are identical, making it undefined. 

\end{itemize}

The sign of the correlation coefficient matches the sign of the regression slope, as their numerators are the same. Notably, $x$ and $y$ are symmetric in the correlation formula, making the correlation of $x$ and $y$ identical to that of $y$ and $x$.

\subsubsection{When is the correlation coefficient relevant?}

\begin{itemize}
    \item \textbf{Quantitative Variables}: Both variables $x$ and $y$ must be quantitative for the sample correlation coefficient to apply.

    \item \textbf{Linear association}: The association between $x$ and $y$ must be linear. It is wise to graph it, to see if there is perhaps another non-linear association.

\end{itemize}

\subsubsection{Coefficient of Determination}

The coefficient of determination, $R^2$, equal to $\rho ^2$, can take on values between 0 and 1. This range indicates the \% to which the variance in the dependent variable ($y$) is explained by the independent variable ($x$). For example an $R^2$ value of 0.42 indicates that 42\% of the variance in $y$ is explained by $x$ in the model. It does not indicate if the relationship is positive or negative though, and it is symmertric with respects to $x$ and $y$, as the correlation coefficient $\rho$ is.

\subsection{Perspective}

In linear regression, x is considered the explanatory variable and is assumed to be quantitative. It can be controlled by the investigator and does not necessarily need to be continuous. The key advantage of regression analysis is its ability to model the relationship between y and the observed x values, facilitating predictions of y for unobserved x values within the sample dataset.

On the other hand, the correlation coefficient is used to measure the strength or "tightness" of the linear relationship. It is appropriate only for assessing the linear association between two continuous variables, x and y. Unlike regression analysis, it does not provide a functional relationship for prediction purposes.

\subsubsection{Modeling the residuals}

In linear regression, the sample standard deviation measures the average distance of observations from their mean. 
Residuals, measure the distance between observed and predicted values. The standard deviation of the residuals can be calculated using a similar method. 

The sum of the residuals of a linear regression equal zero:

\begin{equation*}
\sum\limits_{i} {r_i} = \sum_{i=1}^n {y_i-(\hat \alpha + \hat \beta x_i)}=0\end{equation*}

The residual standard deviation is:

\begin{equation*}
s_{y|x}=\sqrt{\frac {\sum\limits_{i} {r_i^2}} {n-2}}
\end{equation*}

The subscript $y|x$ refers to a regression model where $y$ is the dependent variable and $x$ is the independent variable. This measures the typical distance between the observed values of $y$ and the values predicted by the model, given $x$.

Note how the denominator is $n-2$, analogous to $n-1$ in the sample standard deviation formula.

\pagebreak

\subsection{R Code}

\begin{lstlisting}
# Load the dataset
data(mtcars)

# View the first few rows of the dataset, just like in jupyter
head(mtcars)

# Plotting mpg against wt
# pch is the point shape, 19 is a filled round dot
plot(mtcars$wt, mtcars$mpg, main="MPG vs Weight", xlab="Weight (1000 lbs)", ylab="Miles per Gallon", pch=19)

# Fitting a linear model
# lm is a R function for fitting linear models
model <- lm(mpg ~ wt, data=mtcars)

# Plot the linear regression line for mpg against wt using the model
abline(model, col="blue")

# Transforming hp to a logarithmic scale
mtcars$log_hp = log(mtcars$hp)

# Plotting log(hp) against mpg with a regression line
plot(mtcars$log_hp, mtcars$mpg, main="MPG vs Log(Horsepower)", xlab="Log(Horsepower)", ylab="Miles per Gallon", pch=19)
abline(lm(mpg ~ log_hp, data=mtcars), col="green")

# Calculating residuals for the mpg against wt model
residuals <- resid(model)

# Plotting residuals
plot(mtcars$wt, residuals, main="Residuals of MPG vs Weight Model", xlab="Weight (1000 lbs)", ylab="Residuals", pch=19)
abline(h=0, col="blue")

# Calculating and printing the residual standard deviation
residual_sd <- sd(residuals)
cat("Residual Standard Deviation:", residual_sd, "\n")
\end{lstlisting}

\pagebreak

\section{Comparison of groups}

One-way analysis of variance (ANOVA) is used for comparing two or more groups. 

For instance, a meat scientist investigating how three varied storage conditions affect meat tenderness would randomly allocate 24 meat pieces into three groups, with eight pieces per group. Each group's meat is stored under identical conditions, and their tenderness is measured after a period. The inquiry is whether storage conditions significantly influence tenderness, distinguishing between actual effects and random variations, and determining the magnitude of any differences. This analysis is an example of ANOVA, as the scientist will compare 3 different categorical groups.

\subsection{Graphical and simple numerical comparison}

Analyzing data typically begins with graphical inspection, such as using parallel boxplots or strip charts, to highlight key differences, data variation, and consistency across groups. Statistical summaries of each group are also compared for detailed analysis.

An example involves comparing parasite counts in two salmon stocks to determine if infection rates differ. Statistical analysis, including mean and standard deviation calculations for each stock, showed higher parasite counts in the Ätran stock compared to the Conon stock:

\begin{itemize}
    \item \textbf{Ätran}: mean = 32.23, SD = 7.28

    \item \textbf{Conon}: mean = 21.54, SD = 5.81 

\end{itemize}

This suggests higher susceptibility to parasites in Ätran salmon. However, results could vary with new samples, highlighting the need to distinguish between genuine differences and random variation. The analysis focuses on the difference between sample means, considering the natural variability of sample means to determine if observed differences are significant or due to chance.

\subsection{Between-group variation and within-group variation}

When comparing only two groups, the difference in means ($\bar{y_1}-\bar{y_2}$) serves as a practical measure of group differences. However, it's crucial to consider the natural variation within samples to determine if the observed difference might be due to chance.

With three or more groups, the analysis involves multiple pairwise differences that need to be collectively assessed. This introduces the concept of between-group and within-group variations, essential for understanding the dynamics of data analysis:

\begin{itemize}
    \item \textbf{Between-group variation} refers to the differences between the groups. Graphically, it's represented by the discrepancy between the means of each group and the overall mean. The formula is: ${SS}_{grp}=\sum_{j=1}^{k} n_j(\bar{y_i}-\bar{y})^2$

    \item \textbf{Within-group variation} refers to the variability within each group - the spread of observations around their group mean. The formula is: ${SS}_{e}=\sum_{i=1}^{n} (y_i-\bar{y}_{g(i)})^2$

\end{itemize}

For the two formulas:

\begin{itemize}
    \item \textbf{$k$} = the number of groups
    \item \textbf{$n_j$} = the number of observations in group $j$
    \item \textbf{$g(i)$} = the group for observation i
\end{itemize}


A significant between-group variation suggests differences between group means. However, if within-group variation is also high, the differences could be attributed to random variation, indicating that repeating the experiment might yield varying results. Thus, analysis must account for both types of variation.

This differentiation between variation sources is foundational to the analysis of variance (ANOVA), a method named for its focus on dissecting and analyzing these variations to discern significant differences between groups.

\subsection{Populations, samples, and expected values}

In comparing different populations, such as men's and women's average blood pressure, we aim to analyze the population means ($\alpha_m$ for men and $\alpha_f$ for women), which represent the expected values of blood pressure in the absence of additional information. To compare these averages, samples from each population are collected and their blood pressures measured to infer about the population averages.

The concept applies similarly to comparing two different salmon stocks from the earlier exammple, Ätran and Conon, where samples from each population are used to study differences in parasite counts. In experiments like the antibiotics study involving heifers, even though the groups come from a single population and are differentiated by treatment, the objective remains the same: to compare expected values ($\alpha$'s) across groups, where $\alpha_{spiramycin}$ and $\alpha_{control}$ might represent the average amounts of organic material in treated vs. untreated heifers, respectively.

Regardless of whether groups are formed from distinct populations or as a result of experimental intervention, the goal is to compare these expected values across groups. Essential to this comparison is accounting for the variation within groups, quantified by the standard deviations of each group. This within-group variation is a critical factor in assessing the significance of differences between the expected values of different groups or treatments.

\subsection{Least squares estimation and residuals}
Consider a scenario with $n$ observations distributed across $k$ groups, where each group is labeled from 1 to $k$. The objective is to estimate the expected values $\mu_1, \ldots, \mu_k$ for each group. To achieve this, we employ the least squares criterion. For any observation $i$, let $g(i)$ represent its group, implying $\mu_{g(i)}$ is the expected value for observation $i$. The sample mean $\bar{y}_j$ and sample standard deviation $s_j$ for group $j$ are defined as:
\[
\bar{y}_j = \frac{1}{n_j} \sum_{i:g(i)=j} y_i, \quad s_j = \sqrt{\frac{1}{n_j - 1} \sum_{i:g(i)=j} (y_i - \bar{y}_j)^2},
\]
for $j = 1, \ldots, k$. The least squares estimates of $\mu_1, \ldots, \mu_k$ are the values that minimize the sum of squared deviations $Q(\mu_1, \ldots, \mu_k) = \sum_{i=1}^{n} (y_i - \mu_{g(i)})^2$. These estimates are given by $\hat{\mu}_j = \bar{y}_j$ for $j = 1, \ldots, k$.

The residual for observation $i$ is $r_i = y_i - \hat{\mu}_{g(i)}$, and the residual variance $s^2$ and standard deviation $s$ are computed as:
\[
s^2 = \frac{1}{n - k} \sum_{i=1}^{n} r_i^2, \quad s = \sqrt{s^2},
\]
where $n - k$ adjusts for the estimation of $k$ parameters. The residual variance can also be expressed as a weighted average of the group variance estimates $s_j^2$, which is known as the pooled sample variance. This approach accounts for the within-group variability and assumes equal population standard deviations across groups.

\subsection{Paired and unpaired samples}
We revisit the scenario of comparing two groups, but with a distinct data structure characterized by pairs of observations naturally linked to each other, known as paired samples. This arrangement often arises when two measurements are taken under varying conditions for each subject, such as in dietary studies where subjects switch diets between periods, or in experiments involving pairs of related units like twins. The aim is to assess differences between conditions, reducing the substantial variation among subjects by focusing on paired differences.

An example involves a study on equine lameness with measurements of symmetry scores for eight horses, both in healthy and induced-lameness states. The analysis relies on the differences in symmetry scores between these states for each horse, aiming to determine if lameness significantly alters the symmetry score. Positive differences for all horses suggest a change, but statistical analysis is needed to confirm if this is beyond random variation.

It's crucial to differentiate between paired and unpaired (independent) samples since they require distinct analytical methods. In paired samples, observations within a pair share underlying characteristics, making the assumption of independence between observations inappropriate. This shared information, such as a horse's general gait pattern, can influence the results. By analyzing the differences within pairs, we eliminate issues related to independence and potentially neutralize factors like physical proportions or general patterns that could affect both measurements similarly.

Paired designs can offer practical and economical advantages by requiring fewer subjects to achieve a certain precision in estimating differences. However, it's not always feasible to conduct paired studies, especially when measurements involve irreversible processes or when pairing is impractical, as in the case with different salmon stocks. Paired studies aim to minimize random variation unrelated to the investigation's focus, essentially using the experimental units as their own controls to highlight the effect of interest.

\subsection{Perspective}

This section discusses three types of data structures relevant to comparing groups or treatments:

\begin{itemize}
    \item \textbf{Two Independent Samples}: This structure involves samples from two distinct groups or treatments, presumed independent. It's highlighted separately for its importance in distinguishing from paired samples due to the necessity of different analysis methods and because it introduces foundational concepts for understanding more complex arrangements.

    \item \textbf{Independent Samples Across Multiple Groups}: This structure extends the first to encompass samples from $k$ different groups or treatments, maintaining independence among them. This setup is suited for one-way analysis of variance (ANOVA), which compares variation sources within and across groups to assess significant differences.

    \item \textbf{Paired Samples}: This involves paired measurements, where each pair corresponds to observations from two different groups or treatments. The paired nature of the data requires distinct analytical approaches due to the inherent linkage between observations within each pair.
\end{itemize}

The first case serves as a basis for the second, emphasizing the shift in complexity when analyzing more than two groups. One-way ANOVA, used in the independent samples scenario, focuses on comparing expected values or average response levels across groups, underlining the importance of understanding the precision of these estimates to discern real differences from those occurring by chance.

A critical assumption for ANOVA is the similarity of standard deviations across groups, with a rule-of-thumb being that the ratio between the largest and smallest group standard deviations should not exceed 2. However, this guideline's applicability depends on sample sizes, with larger and equally sized samples offering more robust results.

The analysis must also account for data symmetry (since standard deviation is meaningful primarily for symmetric data) and sample size, as small samples can lead to imprecise standard deviation estimates. In cases of heterogeneous variation, data transformation (like logarithmic transformation) might be necessary to achieve homogeneity of variances across groups.

Furthermore, scientific experiments often involve more than one explanatory factor, necessitating multi-way ANOVA for simultaneous consideration of multiple factors. Two-way ANOVA, for example, analyzes the effects of two factors and their interactions, providing a more comprehensive understanding of the factors influencing the response variable.

\subsection{R Code}

\begin{lstlisting}
# Load the mtcars dataset
data(mtcars)

# Graphical comparison of mpg across different cylinder groups
boxplot(mpg ~ cyl, data = mtcars,
        main = "MPG by Number of Cylinders",
        xlab = "Number of Cylinders", ylab = "Miles Per Gallon")

# One-Way ANOVA for mpg across cylinder groups
anova_result <- aov(mpg ~ as.factor(cyl), data = mtcars)
summary(anova_result)

# Checking assumptions

# Normality check for each cylinder group for 'mpg'
shapiro.test(mtcars$mpg[mtcars$cyl == 4])
shapiro.test(mtcars$mpg[mtcars$cyl == 6])
shapiro.test(mtcars$mpg[mtcars$cyl == 8])

# Homogeneity of variances (Levene's Test is not available in base R, using Bartlett's test as an alternative)
bartlett.test(mpg ~ as.factor(cyl), data = mtcars)

# Since mtcars does not have paired data, let's simulate a scenario for demonstration purposes
# Simulating before and after data for a paired sample scenario
set.seed(123) # For reproducibility
before <- rnorm(10, 20, 5)
after <- before + rnorm(10, -2, 2) # Assume some decrease

# Paired t-test
paired_test_result <- t.test(before, after, paired = TRUE)
print(paired_test_result)
\end{lstlisting}

\section{Normal Distribution}

Statistical models capture systematic behavior and random variation. The section introduces the normal (Gaussian) distribution for random variation, extending beyond the average behavior explored in linear regression and one-way ANOVA. The normal distribution, vital for continuous variables, is effective due to the central limit theorem (CLT), which suggests averages tend toward a normal distribution regardless of the original data's distribution. This distribution is not suitable for categorical data, addressed with the binomial distribution in later chapters. Named after Carl Friedrich Gauss, the Gaussian distribution is preferred for its accurate data representation, especially in biological data, and its simple mathematical properties, enhancing statistical analysis for continuous data.

\subsection{Properties}

Continuous quantitative data, exemplified by a study on crab weights at the Royal Veterinary and Agricultural University in Denmark, show the utility of the normal distribution. Recording weights of 162 crabs, the study found a mean of 12.76 grams and a standard deviation of 2.25 grams. A histogram of crab weights aligns well with a normal distribution curve defined by these parameters, illustrating the distribution's effectiveness in modeling weight variation. The corresponding density function for this normal distribution highlights its applicability in analyzing such continuous data.

$f(y)= \frac {1} {\sqrt{2 \pi \cdot 2.25^2}} \exp (-\frac {1} {2 \cdot 2.25^2} (y-12.76)^2)$

\subsubsection{Density, mean, and standard deviation}

Histograms for continuous data divide observations into subintervals, representing frequency with rectangle heights. When normalized, these rectangles' areas sum to 1, indicating relative frequencies or probabilities for large samples. Smooth histograms can approximate smooth curves, like the normal distribution density:

$f(y) = \frac {1} {\sqrt{2 \pi \sigma ^2}} \exp (- \frac {1} {2 \sigma ^2} (y-\mu)^2 ), -\infty < y < \infty$

where $\mu$ and $\sigma$ represent the mean and standard deviation. This density suggests that the probability of an observation falling within any interval (a, b) is the area under the curve from a to b, calculated as $P(a<Y<B)= \int_a^b f(y) dy$, where $Y$ is a random observation.

For a sample drawn according to this density, the sample mean and standard deviation approach $\mu$ and $\sigma$ s sample size increases, defining the mean and standard deviation of the distribution. A variable $Y$ is normally distributed, $Y \sim N(\mu, \sigma ^2)$, if its values follow this density.

Continuing the crab weight example, using $\mu = 12.76$ and $\sigma=2.25$, the normal density closely matches the histogram, suggesting a $N(12.76, 2.25^2)$ distribution for crab weights. The probability of a crab weighing between 16 and 18 grams is computed through the integral of the density function over that range, showing a close match between calculated probabilities and observed relative frequencies when the sample well represents the normal distribution.

Normal densities are bell-shaped and symmetric around $\mu$, with properties of symmetry, centering (maximum value at $\mu$), and dispersion (width varies with $\sigma$). The interpretations of $\mu$ and $\sigma$ align with their roles in determining the distribution's center and spread. Probabilities for any interval are non-zero due to the density's positive value across its range, but single points have zero probability. The total area under the density curve equals 1, reflecting the total probability mass.

\subsubsection{Transformations of normally distributed variables}

The normal distribution's key properties include the normality of sums of normally distributed variables and the retention of normality through linear transformations. Specifically:

\begin{itemize}
    \item \textbf{Sum of Normally Distributed Variables}: If $Y_1 \sim N(\mu_1 \sigma_1^2)$ and $Y_2 \sim N(\mu_2 \sigma_2^2)$ are independent, their sum $Y_1 + Y_2$ follows a normal distribution $N(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)$, with mean $\mu_1 + \mu_2$ and standard deviation $\sqrt{\sigma_1^2 + \sigma_2^2}$
    \item \textbf{Linear Transformations}: A normally distributed variable $Y \sim N(\mu \sigma^2)$ transformed by $V=a+bY$ also follows a normal distribution, $N(a+b\mu, b^2\sigma^2)$, where $a$ and $b$ are real numbers. The mean and standard deviation of $V$ are $a+b\mu$ and $|b|\sigma$, respectively
    \item \textbf{Standardization}: Any normally distributed variable $Y$ can be standardized to $Z= \frac {Y-\mu} {\sigma} \sim N(0,1)$, a special case of linear transformation, with $a = -\mu / \sigma$ and $b = 1/ \sigma$
\end{itemize}

Independence is crucial for these properties, especially for the variance formula in sums. Even without independence, $Y_1 + Y_2$ remains normally distributed with mean $\mu_1 + \mu_2$, but variance calculations require independence. These properties extend to any number of terms, affecting sample means and standard deviations.

For crab weights modeled by $N(12.76, 2.25^2)$ applications include:

\begin{itemize}
    \item The total weight of two crabs, summing to a mean of 25.52 and a standard deviation of 3.18.
    \item Converting weights to kilograms with a specific adjustment results in a normal distribution with adjusted mean and standard deviation.
    \item Standardizing crab weights yields a $N(0,1)$ distribution
\end{itemize}

Furthermore, if $y_1,...,y_n$ are independent and identically distributed as $N(\mu, \sigma^2)$, the sum of these variables is normally distributed with mean $n \mu$ and variance $n \sigma ^2$. The sample mean, $\bar{y} = \frac {1} {n} \sum_{i=1}^{n} y_i$, follows $N(\mu, \sigma ^2 / n)$, implying an average of normally distributed variables remains normally distributed, with the original mean but standard deviation reduced by $\sqrt{n}$. This principle is vital for understanding distributions of sample means and their implications in statistical analysis.

\subsubsection{Probability calculations}

\subsubsection{Central part of distribution}

\subsection{One sample}

The expected value of observations $y_1, ..., y_n$ reflects the average value in a population. These observations help infer population characteristics. For example, studying crab weights aims to determine the population's average weight and what deviations are considered unusual. The sample mean, $\bar y$ estimates the population mean. However, its reliability varies with sample size—the larger the sample, the more accurate the estimate. Assuming the population follows a normal distribution and the sample is randomly drawn, it's possible to precisely estimate population characteristics like average weights and typical variations.

\subsubsection{Independence}

Observations in a sample are assumed to be independent, meaning the probability of multiple events happening simultaneously is the product of their individual probabilities:

$P(Y_1 \leq a_1, ..., Y_n \leq a_n) = P(Y_1 \leq a_1) \cdot \cdot \cdot P(Y_n \leq a_n)$

Independence implies each observation adds unique information to the dataset. For example, in studying apple tree yields, if 20 trees are randomly selected from 1000, their yields are considered independent. However, if for each of 10 randomly selected trees, a "twin" tree is also selected for its similarity, the yields are not independent since each pair of trees holds related information. Similarly, selecting 10 trees and their "opposites" (based on size or other criteria) also results in dependent observations. Furthermore, sampling two trees from each of 10 orchards introduces dependency because trees within the same orchard are influenced by common environmental factors, making it unreasonable to assume independence among these samples.

\subsubsection{Estimation}

When $y_1, ..., y_n$ are independent observations from a normal distribution with mean $\mu$ and standard deviation $\sigma$, they are considered iid $N(\mu, \sigma^2)$. The sample mean $\bar y$ and sample standard deviation $s$ serve as natural estimates for $\mu$ and $\sigma$ respectively, with $\hat \mu = \bar y$ and $\hat \sigma = s$.

The sample mean $\bar y$ itself follows a normal distribution with its expected value $E(\hat \mu) = \mu$ and standard deviation $sd(\hat \mu) = \frac {1} {\sqrt{n}} \sigma$, illustrating statistical properties:

\begin{itemize}
    \item \textbf{Unbiased Estimate}: The sample mean accurately reflects the population mean on average, indicating that across many samples, the average of the sample means equals $\mu$
    \item \textbf{Consistent Estimate}: Precision of the sample mean improves with sample size, as its standard deviation decreases, allowing for an increasingly accurate estimate of $\mu$
\end{itemize}

These properties underscore that the sample mean is both a correct and precise estimator of the population mean, with precision that can be enhanced by increasing the sample size.

\subsection{Are the data (approximately) normally distributed?}

Model validation often requires a variable's distribution to closely resemble a normal distribution. Perfect normality is rare, but statistical methods remain effective if the normality assumption is reasonably satisfied. This is usually checked using graphical methods.

\subsubsection{Histograms and QQ-plots}

For validating a variable's approximate normal distribution, comparing its histogram to a normal density curve with matching sample mean and standard deviation is useful. This method was applied to crab weight data, showing a good fit, indicating an approximate normal distribution.

Additionally, the QQ-plot (quantile-quantile plot) is crucial for this validation, comparing sample quantiles against the normal distribution's quantiles. A QQ-plot for crab weights aligned closely with a straight line defined by the sample's mean ($\bar y = 12.76$) and standard deviation ($s =2.25$), indicating no significant deviations from normality.

The QQ-plot operates on the principle that if a sample ($z_1,...,z_n$) possibly comes from a $N(0,1)$ distribution, plotting ordered observations ($z_{j}$) against corresponding $N(0,1)$ quantiles ($u_j$) should reveal a linear pattern with intercept zero and slope one for a perfectly normal distribution. For a general sample ($y_1,...,y_n$) from $N(\mu, \sigma^2)$, this plot helps estimate $\mu$ and $\sigma$ by the linearity of plotted points around a straight line, with the line's intercept and slope serving as estimates for $\mu$ and $\sigma$.

To make a QQ plot for checking if your data follows a normal distribution:

\begin{itemize}
    \item \textbf{Sort the data}: Arrange the data points in ascending order
    \item \textbf{Calculate theoretical quantiles}: Determine the expected positions of the data points if they were perfectly normally distributed. This involves figuring out where each data point would lie on a normal distribution curve
    \item \textbf{Plot the data}: On a graph, plot each of the sorted data points against the corresponding theoretical quantile. The x-axis will represent the theoretical (expected) quantiles from a normal distribution, and the y-axis will show the actual data's quantiles
\end{itemize}

If the data is normally distributed, the points on the QQ plot will roughly follow a straight line.

\subsubsection{Transformations}

Histograms or QQ-plots sometimes reveal that data do not fit a normal distribution, often due to subgroup differences (e.g., men vs. women) resulting in bimodal distributions or skewness in data with only positive values. For bimodal distributions, modeling each group with its own normal distribution may be more appropriate. Skewed data may benefit from log-transformation, making the log-transformed data more normally distributed, as shown with vitamin A intake among Danish men. This transformation aligns histograms and QQ-plots closer to normality. However, any analysis or probability computations should then be conducted on the transformed scale. The study on Danish food intake, including vitamin A and basal metabolic rate (BMR), demonstrates these points. BMR data, when analyzed separately for men and women, fit normal distributions reasonably well but combined data show bimodality, indicating the importance of considering subgroup differences for normal approximation.

To log-transform data:

\begin{outline}
    \1 \textbf{Choose the Log Base}: Select the base for the logarithm, typically $e$ (natural logarithm) or 10.
    \1 \textbf{Decide on Log Transformation}: Determine whether you need to log-transform $y$, $x$, or both, based on the distribution of your data or the specific analysis requirements.
        \2 If transforming $y$: Apply the log function to each $y_i$ to get $log(y_1), ..., log(y_n)$
        \2 If transforming $x$: Apply the log function to each $x_i$ to get $log(x_1), ..., log(x_n)$
        \2 If transforming both: Apply the log function to each pair of $x_i$ and $y_i$ to get pairs of $log(x_1),log(y_1);...;log(x_n),log(y_n)$
    \1 \textbf{Transform}: Transform the data, then plot/store the resulting values
\end{outline}

\subsubsection{The exponential distribution}

\subsection{The central limit theorem}

\subsection{R Code}

\section{Law of Large Numbers and Multivariate Gaussian Distribution}

\subsection{Law of Large Numbers}

The Law of Large Numbers (LLN) is a fundamental theorem in probability and statistics that describes the result of performing the same experiment a large number of times. According to the LLN, the average of the results obtained from a large number of trials will converge to the expected value as the number of trials approaches infinity. This theorem provides a solid foundation for the concept of long-term stability in probabilities and is crucial for understanding the behavior of averages in large samples.

The LLN is divided into two versions: the Weak Law of Large Numbers (WLLN) and the Strong Law of Large Numbers (SLLN). Both versions assert that with a sufficient number of observations, the sample mean will be close to the population mean, but they differ in their mathematical rigor and conditions for convergence.

\subsubsection{Weak Law of Large Numbers (WLLN)}

The Weak Law of Large Numbers states that for a sequence of independent and identically distributed (iid) random variables with a finite expected value, the sample average converges in probability towards the expected value as the sample size increases. Mathematically, if $X_1,X_2,...,X_n$ are iid random variables with expected value $E[X]=\mu$ and variance $Var(X)=\sigma^2$, then for any $\epsilon > 0$,

$P(|\frac {1} {n} \sum_{i=1}^{n} X_i- \mu| \geq \epsilon) \rightarrow 0 \text{ as } n \rightarrow \infty$

This means that the probability of the sample mean deviating from the population mean by more than $\epsilon$ tends to zero as the sample size $n$ becomes very large.

\subsubsection{Strong Law of Large Numbers (SLLN)}

The Strong Law of Large Numbers strengthens the WLLN by stating that the sample average almost surely converges to the expected value. In other words, the probability that the sample mean converges to the population mean as the sample size approaches infinity is equal to one. For the same sequence of iid random variables as above, the SLLN asserts that:

$P(\lim\limits_{n\rightarrow \infty} (\frac {1} {n} \sum_{i=1}^{n} X_i)=\mu) =1$

The SLLN implies not just that large deviations become improbable as the sample size increases, but that they essentially do not occur.

\subsubsection{Implications and Applications}

The Law of Large Numbers has profound implications in both theoretical and applied statistics:

\begin{itemize}
    \item \textbf{Estimation of Probabilities}: It underlies the principle that probabilities can be estimated by the relative frequency of events in large numbers of trials.
    \item \textbf{Statistical Stability}: It justifies the expectation that empirical averages of large samples are stable and reliable estimates of theoretical averages.
    \item \textbf{Insurance and Gambling}: In insurance, premiums are priced on the expectation that, over a large number of policyholders, actual losses will average out to the expected losses. In gambling, the LLN explains why casinos always win in the long run.
\end{itemize}

\subsubsection{Conditions and Limitations}

\begin{itemize}
    \item \textbf{Independence and Identical Distribution}: The classical form of the LLN assumes that the random variables are independent and identically distributed. This condition can be relaxed in some versions of the theorem.
    \item \textbf{Finite Expected Value}: The random variables must have a finite expected value. If the expected value is infinite, the LLN does not apply.
    \item \textbf{Convergence}: The LLN speaks about convergence as the sample size goes to infinity. In practice, "large enough" sample sizes are often sufficient for the LLN to hold, but what constitutes "large enough" can vary depending on the distribution of the data.
\end{itemize}

\subsection{Multivariate Gaussian Distribution}

The Multivariate Gaussian Distribution, also known as the Multivariate Normal Distribution, extends the concept of the normal distribution to multiple dimensions, looking at several things at once (for example: height and weight).

\subsubsection{Mean Vector and Covariance Matrix}

\paragraph{Mean Vector} Vector of means for each variable; in our example, if we denote height by $X_1$ and weight by $X_2$, the mean vector $\boldsymbol \mu$ would look like: $\boldsymbol \mu = \begin{bmatrix} \mu_1 \\ \mu_2 \end{bmatrix}$ where $\mu_1$ and $\mu_2$ are the means for $X_1$ and $X_2$ respectively.

\paragraph{Covariance Matrix} The covariance matrix indicates how much each variable varies on its own and how they vary together.

\begin{itemize}
    \item \textbf{On its own (Diagonal)}: The diagonal tells us about the variance of each variable; in our example, how spread out the heights and weights are.
    \item \textbf{Together (Off-diagonal)}: The off-diagonal part tells us about the covariance; in our example, whether taller people tend to be heavier (a positive relationships) or lighter (a negative relationship). 
\end{itemize}

In our example, for height ($X_1$) and weight $X_2$, the covariance matrix $\boldsymbol \Sigma$ is: 

\begin{align*}
\boldsymbol{\Sigma} = \begin{bmatrix} {\sigma_{X_1}^2} & {\sigma_{X_1X_2}} \\  {\sigma_{X_1X_2}} & {\sigma_{X_2}^2} \end{bmatrix}
\end{align*}

Where:

\begin{itemize}
    \item $\sigma_{X_1}^2$ is the variance in height
    \item $\sigma_{X_2}^2$ is the variance in weight
    \item $\sigma_{X_1 X_2}$ is the covariance between height and weight
    
\end{itemize}

\subsubsection{The Probability Density Function (PDF)}

The PDF of the Multivariate Gaussian tells us how likely we are to see a particular combination of variables; in our example, height and weight.

The PDF for a $D$-dimensional vector $\boldsymbol{x} = (x_1, x_2), \boldsymbol{x} \in \mathbb{R}^D$, given mean vector $\boldsymbol{\mu}$ and covariance matrix $\boldsymbol{\Sigma}$, is:

\begin{align*}
p(\boldsymbol{x})= \frac {1} {(2\pi)^{D/2} |\boldsymbol{\Sigma}|^{1/2}} \exp{[-\frac 1 2 (\boldsymbol{x} - \boldsymbol{u})^T \boldsymbol{\Sigma}^{-1} (\boldsymbol{x} - \boldsymbol{u})]}
\end{align*}

When we plot the distribution, the tilt indicates the relationship (covariance) between the variables, while the shape indicates the variance of the variables. Circular plots indicate that the variables have the same variance.

\subsection{R Code}

\subsubsection*{Law of Large Numbers}

\begin{lstlisting}
# Set the true parameters for the normal distribution
true_mean <- 0
true_sd <- 1

# Set the number of simulations
n_sim <- 25000

# Generate random samples from the normal distribution
samples <- rnorm(n_sim, mean = true_mean, sd = true_sd)

# Calculate the cumulative mean
cumulative_means <- cumsum(samples) / (1:n_sim)

# Plot the cumulative mean
plot(cumulative_means, type = "l", col = "blue", ylim = c(true_mean - 0.25, true_mean + 0.25),
     xlab = "Number of Samples", ylab = "Cumulative Average",
     main = "Demonstration of the Law of Large Numbers")

# Add a line for the true mean
abline(h = true_mean, col = "red", lwd = 2)

# Annotate the true mean
legend("bottomright", legend = c("Cumulative Mean", "True Mean"), col = c("blue", "red"), lty = 1)
\end{lstlisting}

\subsubsection*{Multivariate Gaussian Distribution}

\begin{lstlisting}
# Parameters
mean_vector <- c(170, 65)
mu_height <- 170
mu_weight <- 65
var_height <- 25
var_weight <- 36
cov_hw <- 18

# Generating grid points for height and weight
heights <- seq(mu_height - 3*sqrt(var_height), mu_height + 3*sqrt(var_height), length.out = 100)
weights <- seq(mu_weight - 3*sqrt(var_weight), mu_weight + 3*sqrt(var_weight), length.out = 100)

# Function to calculate the density of the Multivariate Gaussian
mvn_density <- function(x, y, mu, Sigma) {
  exp(-0.5 * t(c(x - mu[1], y - mu[2])) %*% solve(Sigma) %*% c(x - mu[1], y - mu[2])) /
    (2 * pi * sqrt(det(Sigma)))
}

# Calculate density values
Sigma <- matrix(c(var_height, cov_hw, cov_hw, var_weight), nrow = 2)
density_values <- outer(heights, weights, Vectorize(function(x, y) mvn_density(x, y, mu = c(mu_height, mu_weight), Sigma = Sigma)))

# Plot with filled contours
filled.contour(heights, weights, density_values,
               xlab = "Height (cm)", ylab = "Weight (kg)",
               main = "Height vs Weight Distribution with Color Shading",
               color.palette = colorRampPalette(c("blue", "green", "yellow", "red")),
               key.title = title(main = "Density Levels", cex.main = 1),
               key.axes = axis(4, las = 1))
\end{lstlisting}

\end{document}